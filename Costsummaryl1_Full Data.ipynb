{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (1,12,13,14,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ys_tenant_uuid</th>\n",
       "      <th>ys_billingfilename</th>\n",
       "      <th>ys_billingmonth</th>\n",
       "      <th>timedim</th>\n",
       "      <th>payeraccountid</th>\n",
       "      <th>linkedaccountid</th>\n",
       "      <th>productname</th>\n",
       "      <th>usagetype</th>\n",
       "      <th>ys_usagetype</th>\n",
       "      <th>ys_usagetypegroup</th>\n",
       "      <th>...</th>\n",
       "      <th>ys_resourceid</th>\n",
       "      <th>ys_all_tags</th>\n",
       "      <th>ys_attributes</th>\n",
       "      <th>reservedinstance</th>\n",
       "      <th>ys_tenancytype</th>\n",
       "      <th>ys_reservedusagetype</th>\n",
       "      <th>ys_usagequantity</th>\n",
       "      <th>ys_reservedusagequantity</th>\n",
       "      <th>run_timestamp</th>\n",
       "      <th>ys_amortizedcost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9ce3e72e31b44e188d92f1d2e4758458</td>\n",
       "      <td>819983868943-aws-billing-detailed-line-items-w...</td>\n",
       "      <td>201804</td>\n",
       "      <td>2018-04-23</td>\n",
       "      <td>819983868943</td>\n",
       "      <td>819983868943</td>\n",
       "      <td>Amazon Elastic Compute Cloud</td>\n",
       "      <td>BoxUsage:c5.large</td>\n",
       "      <td>c5.large</td>\n",
       "      <td>Instance</td>\n",
       "      <td>...</td>\n",
       "      <td>879b263592416c392df4eb3f750a302b3aac2fcf</td>\n",
       "      <td>{\"aws:autoscaling:groupName\":\"beta-s3config-AS...</td>\n",
       "      <td>{}</td>\n",
       "      <td>N</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.191944</td>\n",
       "      <td>0.191944</td>\n",
       "      <td>2018-10-09 11:26:39</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9ce3e72e31b44e188d92f1d2e4758458</td>\n",
       "      <td>819983868943-aws-billing-detailed-line-items-w...</td>\n",
       "      <td>201804</td>\n",
       "      <td>2018-04-23</td>\n",
       "      <td>819983868943</td>\n",
       "      <td>819983868943</td>\n",
       "      <td>Amazon Elastic Compute Cloud</td>\n",
       "      <td>BoxUsage:t2.small</td>\n",
       "      <td>t2.small</td>\n",
       "      <td>Instance</td>\n",
       "      <td>...</td>\n",
       "      <td>d81578e13ef96fe336f4ec9e631e1be7ef1e6141</td>\n",
       "      <td>{\"aws:autoscaling:groupName\":\"utilization-summ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>N</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>2018-10-09 11:26:39</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9ce3e72e31b44e188d92f1d2e4758458</td>\n",
       "      <td>819983868943-aws-billing-detailed-line-items-w...</td>\n",
       "      <td>201804</td>\n",
       "      <td>2018-04-23</td>\n",
       "      <td>819983868943</td>\n",
       "      <td>819983868943</td>\n",
       "      <td>Amazon Elastic Compute Cloud</td>\n",
       "      <td>USE1-CAN1-AWS-In-Bytes</td>\n",
       "      <td>ys-data-in</td>\n",
       "      <td>Network</td>\n",
       "      <td>...</td>\n",
       "      <td>ed9a3eeabb946a28b79194756c84e100a41b39f2</td>\n",
       "      <td>{\"aws:autoscaling:groupName\":\"inventory-proces...</td>\n",
       "      <td>{}</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>2018-10-09 11:26:39</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9ce3e72e31b44e188d92f1d2e4758458</td>\n",
       "      <td>819983868943-aws-billing-detailed-line-items-w...</td>\n",
       "      <td>201804</td>\n",
       "      <td>2018-04-23</td>\n",
       "      <td>819983868943</td>\n",
       "      <td>819983868943</td>\n",
       "      <td>Amazon Elastic Compute Cloud</td>\n",
       "      <td>USE1-EU-AWS-In-Bytes</td>\n",
       "      <td>ys-data-in</td>\n",
       "      <td>Network</td>\n",
       "      <td>...</td>\n",
       "      <td>ed9a3eeabb946a28b79194756c84e100a41b39f2</td>\n",
       "      <td>{\"aws:autoscaling:groupName\":\"inventory-proces...</td>\n",
       "      <td>{}</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001676</td>\n",
       "      <td>0.001676</td>\n",
       "      <td>2018-10-09 11:26:39</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9ce3e72e31b44e188d92f1d2e4758458</td>\n",
       "      <td>819983868943-aws-billing-detailed-line-items-w...</td>\n",
       "      <td>201804</td>\n",
       "      <td>2018-04-23</td>\n",
       "      <td>819983868943</td>\n",
       "      <td>819983868943</td>\n",
       "      <td>Amazon Elastic Compute Cloud</td>\n",
       "      <td>USE1-USW1-AWS-In-Bytes</td>\n",
       "      <td>ys-data-in</td>\n",
       "      <td>Network</td>\n",
       "      <td>...</td>\n",
       "      <td>2f619de01b3e71a4a4c08af75e3d91e3fbe51ac6</td>\n",
       "      <td>{\"aws:autoscaling:groupName\":\"inventory-proces...</td>\n",
       "      <td>{}</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>2018-10-09 11:26:39</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ys_tenant_uuid  \\\n",
       "0  9ce3e72e31b44e188d92f1d2e4758458   \n",
       "1  9ce3e72e31b44e188d92f1d2e4758458   \n",
       "2  9ce3e72e31b44e188d92f1d2e4758458   \n",
       "3  9ce3e72e31b44e188d92f1d2e4758458   \n",
       "4  9ce3e72e31b44e188d92f1d2e4758458   \n",
       "\n",
       "                                  ys_billingfilename  ys_billingmonth  \\\n",
       "0  819983868943-aws-billing-detailed-line-items-w...           201804   \n",
       "1  819983868943-aws-billing-detailed-line-items-w...           201804   \n",
       "2  819983868943-aws-billing-detailed-line-items-w...           201804   \n",
       "3  819983868943-aws-billing-detailed-line-items-w...           201804   \n",
       "4  819983868943-aws-billing-detailed-line-items-w...           201804   \n",
       "\n",
       "      timedim  payeraccountid  linkedaccountid                   productname  \\\n",
       "0  2018-04-23    819983868943     819983868943  Amazon Elastic Compute Cloud   \n",
       "1  2018-04-23    819983868943     819983868943  Amazon Elastic Compute Cloud   \n",
       "2  2018-04-23    819983868943     819983868943  Amazon Elastic Compute Cloud   \n",
       "3  2018-04-23    819983868943     819983868943  Amazon Elastic Compute Cloud   \n",
       "4  2018-04-23    819983868943     819983868943  Amazon Elastic Compute Cloud   \n",
       "\n",
       "                usagetype ys_usagetype ys_usagetypegroup       ...         \\\n",
       "0       BoxUsage:c5.large     c5.large          Instance       ...          \n",
       "1       BoxUsage:t2.small     t2.small          Instance       ...          \n",
       "2  USE1-CAN1-AWS-In-Bytes   ys-data-in           Network       ...          \n",
       "3    USE1-EU-AWS-In-Bytes   ys-data-in           Network       ...          \n",
       "4  USE1-USW1-AWS-In-Bytes   ys-data-in           Network       ...          \n",
       "\n",
       "                              ys_resourceid  \\\n",
       "0  879b263592416c392df4eb3f750a302b3aac2fcf   \n",
       "1  d81578e13ef96fe336f4ec9e631e1be7ef1e6141   \n",
       "2  ed9a3eeabb946a28b79194756c84e100a41b39f2   \n",
       "3  ed9a3eeabb946a28b79194756c84e100a41b39f2   \n",
       "4  2f619de01b3e71a4a4c08af75e3d91e3fbe51ac6   \n",
       "\n",
       "                                         ys_all_tags ys_attributes  \\\n",
       "0  {\"aws:autoscaling:groupName\":\"beta-s3config-AS...            {}   \n",
       "1  {\"aws:autoscaling:groupName\":\"utilization-summ...            {}   \n",
       "2  {\"aws:autoscaling:groupName\":\"inventory-proces...            {}   \n",
       "3  {\"aws:autoscaling:groupName\":\"inventory-proces...            {}   \n",
       "4  {\"aws:autoscaling:groupName\":\"inventory-proces...            {}   \n",
       "\n",
       "  reservedinstance ys_tenancytype ys_reservedusagetype ys_usagequantity  \\\n",
       "0                N        default                  NaN         0.191944   \n",
       "1                N        default                  NaN         0.610000   \n",
       "2                N            NaN                  NaN         0.000890   \n",
       "3                N            NaN                  NaN         0.001676   \n",
       "4                N            NaN                  NaN         0.001531   \n",
       "\n",
       "  ys_reservedusagequantity        run_timestamp ys_amortizedcost  \n",
       "0                 0.191944  2018-10-09 11:26:39              0.0  \n",
       "1                 0.610000  2018-10-09 11:26:39              0.0  \n",
       "2                 0.000890  2018-10-09 11:26:39              0.0  \n",
       "3                 0.001676  2018-10-09 11:26:39              0.0  \n",
       "4                 0.001531  2018-10-09 11:26:39              0.0  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('costsummarydayl1_201908301107.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11618068, 45)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ys_tenant_uuid', 'ys_billingfilename', 'ys_billingmonth',\n",
       "       'timedim', 'payeraccountid', 'linkedaccountid', 'productname',\n",
       "       'usagetype', 'ys_usagetype', 'ys_usagetypegroup', 'operation',\n",
       "       'ys_operationgroup', 'availabilityzone', 'ys_instancetype',\n",
       "       'ys_os', 'ys_region', 'ys_application', 'ys_owner', 'ys_role',\n",
       "       'ys_cluster', 'ys_environment', 'ys_operatinghours', 'ys_project',\n",
       "       'ys_customer', 'ys_costcenter', 'ys_compliance', 'ys_type',\n",
       "       'usagequantity', 'unblendedcost', 'blendedcost', 'ys_updatedon',\n",
       "       'itemdescription', 'blendedrate', 'unblendedrate', 'resourceid',\n",
       "       'ys_resourceid', 'ys_all_tags', 'ys_attributes',\n",
       "       'reservedinstance', 'ys_tenancytype', 'ys_reservedusagetype',\n",
       "       'ys_usagequantity', 'ys_reservedusagequantity', 'run_timestamp',\n",
       "       'ys_amortizedcost'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (1,12,13,14,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9832, 14)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import data\n",
    "pd.options.display.float_format = \"{:.5f}\".format\n",
    "df = pd.read_csv('costsummarydayl1_201908301107.csv')\n",
    "df.timedim = pd.to_datetime(df.timedim)\n",
    "\n",
    "#drop useless data\n",
    "df = df.drop([ \"ys_tenant_uuid\",\"ys_billingmonth\", \"payeraccountid\",\"run_timestamp\", \"ys_updatedon\"], axis=1)\n",
    "df = df.drop([\"blendedcost\", \"ys_usagequantity\", \"ys_reservedusagequantity\", \"linkedaccountid\",\"ys_role\", \n",
    "              \"ys_customer\",\"ys_costcenter\",\"ys_compliance\", \"availabilityzone\", \"ys_instancetype\", \"ys_os\", \n",
    "              \"ys_tenancytype\", \"ys_reservedusagetype\",\"usagetype\",\"ys_usagetype\",\"ys_operationgroup\"], axis=1)\n",
    "\n",
    "#mapping perspective to the data\n",
    "df['perspective'] = np.where(df['ys_operatinghours']=='ANALYTICS-PLATFORM','Analytics Platform',\n",
    "                    np.where(df['ys_operatinghours']=='FORECASTING-AND-BUDGETING','Forecasting and Budgeting',\n",
    "                    np.where(df['ys_operatinghours']=='INVENTORY-AND-TAG-MANAGEMENT','Inventory and Tag Management',\n",
    "                    np.where(df['ys_operatinghours']=='INFRASTRUCTURE','Infrastructure',\n",
    "                    np.where(df['ys_operatinghours']=='PLATFORM','Infrastructure',     \n",
    "                    np.where(df['ys_operatinghours']=='WEB-APP','Webapp',\n",
    "                    np.where(df['ys_operatinghours']=='COST-PROESSING','Cost Processing',\n",
    "                    np.where(df['ys_operatinghours']=='UTILIZATION-ANALYTICS','Utilization Processing',\n",
    "                    np.where(df['ys_operatinghours']=='DEVOPS','Devops',\n",
    "                    np.where(df['ys_operatinghours']=='ANOMALY-DETECTION','Anomaly Detection',\n",
    "                    np.where(df['ys_operatinghours']=='RECOMMENDATION','Recommendations',\n",
    "                    np.where(df['ys_operatinghours']=='RECOMMENDATION-PROCESSING','Recommendations','Untagged'))))))))))))\n",
    "\n",
    "df=df[df.unblendedcost>=0]\n",
    "df=df[df.unblendedcost<df.unblendedcost.max()]\n",
    "df=df[df.unblendedcost<=5*df.unblendedcost.std() + df.unblendedcost.mean()]\n",
    "        \n",
    "data = df.drop(['usagequantity'],axis=1)\n",
    "data['ys_region'].fillna('Untagged', inplace=True)\n",
    "data['operation'].fillna('Untagged', inplace=True)\n",
    "#data aggregation to per product per perspective level: taking the mode of each feature\n",
    "df = data.groupby(['timedim','productname','perspective']).agg({'unblendedcost':'sum','ys_usagetypegroup':lambda x: x.value_counts().index[0],'ys_application':lambda x: x.value_counts().index[0],'ys_owner':lambda x: x.value_counts().index[0],\n",
    "                                                          'ys_cluster':lambda x: x.value_counts().index[0],'operation':lambda x: x.value_counts().index[0],'ys_region':lambda x: x.value_counts().index[0],'ys_environment':lambda x: x.value_counts().index[0],\n",
    "                                                          'ys_operatinghours':lambda x: x.value_counts().index[0],'ys_project':lambda x: x.value_counts().index[0],'ys_type':lambda x: x.value_counts().index[0],'reservedinstance':lambda x: x.value_counts().index[0],\n",
    "                                                               'productname':lambda x: x.value_counts().index[0],'perspective':lambda x: x.value_counts().index[0]})\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188 total features after one-hot encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>unblendedcost</th>\n",
       "      <th>ys_usagetypegroup_Access</th>\n",
       "      <th>ys_usagetypegroup_Instance</th>\n",
       "      <th>ys_usagetypegroup_Network</th>\n",
       "      <th>ys_usagetypegroup_Others</th>\n",
       "      <th>ys_usagetypegroup_Storage</th>\n",
       "      <th>ys_application_Jenkins,Nexus</th>\n",
       "      <th>ys_application_OpenVPN</th>\n",
       "      <th>ys_application_Postgres</th>\n",
       "      <th>ys_application_Redis</th>\n",
       "      <th>...</th>\n",
       "      <th>productname_Amazon SimpleDB</th>\n",
       "      <th>productname_AmazonCloudWatch</th>\n",
       "      <th>productname_OpenVPN Access Server (10 Connected Devices)</th>\n",
       "      <th>perspective_Anomaly Detection</th>\n",
       "      <th>perspective_Devops</th>\n",
       "      <th>perspective_Infrastructure</th>\n",
       "      <th>perspective_Recommendations</th>\n",
       "      <th>perspective_Untagged</th>\n",
       "      <th>perspective_Utilization Processing</th>\n",
       "      <th>perspective_Webapp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timedim</th>\n",
       "      <th>productname</th>\n",
       "      <th>perspective</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2018-01-01</th>\n",
       "      <th>AWS CloudTrail</th>\n",
       "      <th>Untagged</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AWS Key Management Service</th>\n",
       "      <th>Untagged</th>\n",
       "      <td>0.06452</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AWS Lambda</th>\n",
       "      <th>Untagged</th>\n",
       "      <td>0.75242</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AWS Support (Business)</th>\n",
       "      <th>Untagged</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AWS WAF</th>\n",
       "      <th>Untagged</th>\n",
       "      <td>0.16129</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   unblendedcost  \\\n",
       "timedim    productname                perspective                  \n",
       "2018-01-01 AWS CloudTrail             Untagged           0.00000   \n",
       "           AWS Key Management Service Untagged           0.06452   \n",
       "           AWS Lambda                 Untagged           0.75242   \n",
       "           AWS Support (Business)     Untagged           0.00000   \n",
       "           AWS WAF                    Untagged           0.16129   \n",
       "\n",
       "                                                   ys_usagetypegroup_Access  \\\n",
       "timedim    productname                perspective                             \n",
       "2018-01-01 AWS CloudTrail             Untagged                            0   \n",
       "           AWS Key Management Service Untagged                            0   \n",
       "           AWS Lambda                 Untagged                            0   \n",
       "           AWS Support (Business)     Untagged                            0   \n",
       "           AWS WAF                    Untagged                            0   \n",
       "\n",
       "                                                   ys_usagetypegroup_Instance  \\\n",
       "timedim    productname                perspective                               \n",
       "2018-01-01 AWS CloudTrail             Untagged                              0   \n",
       "           AWS Key Management Service Untagged                              0   \n",
       "           AWS Lambda                 Untagged                              0   \n",
       "           AWS Support (Business)     Untagged                              0   \n",
       "           AWS WAF                    Untagged                              0   \n",
       "\n",
       "                                                   ys_usagetypegroup_Network  \\\n",
       "timedim    productname                perspective                              \n",
       "2018-01-01 AWS CloudTrail             Untagged                             0   \n",
       "           AWS Key Management Service Untagged                             0   \n",
       "           AWS Lambda                 Untagged                             0   \n",
       "           AWS Support (Business)     Untagged                             0   \n",
       "           AWS WAF                    Untagged                             0   \n",
       "\n",
       "                                                   ys_usagetypegroup_Others  \\\n",
       "timedim    productname                perspective                             \n",
       "2018-01-01 AWS CloudTrail             Untagged                            1   \n",
       "           AWS Key Management Service Untagged                            1   \n",
       "           AWS Lambda                 Untagged                            1   \n",
       "           AWS Support (Business)     Untagged                            1   \n",
       "           AWS WAF                    Untagged                            1   \n",
       "\n",
       "                                                   ys_usagetypegroup_Storage  \\\n",
       "timedim    productname                perspective                              \n",
       "2018-01-01 AWS CloudTrail             Untagged                             0   \n",
       "           AWS Key Management Service Untagged                             0   \n",
       "           AWS Lambda                 Untagged                             0   \n",
       "           AWS Support (Business)     Untagged                             0   \n",
       "           AWS WAF                    Untagged                             0   \n",
       "\n",
       "                                                   ys_application_Jenkins,Nexus  \\\n",
       "timedim    productname                perspective                                 \n",
       "2018-01-01 AWS CloudTrail             Untagged                                0   \n",
       "           AWS Key Management Service Untagged                                0   \n",
       "           AWS Lambda                 Untagged                                0   \n",
       "           AWS Support (Business)     Untagged                                0   \n",
       "           AWS WAF                    Untagged                                0   \n",
       "\n",
       "                                                   ys_application_OpenVPN  \\\n",
       "timedim    productname                perspective                           \n",
       "2018-01-01 AWS CloudTrail             Untagged                          0   \n",
       "           AWS Key Management Service Untagged                          0   \n",
       "           AWS Lambda                 Untagged                          0   \n",
       "           AWS Support (Business)     Untagged                          0   \n",
       "           AWS WAF                    Untagged                          0   \n",
       "\n",
       "                                                   ys_application_Postgres  \\\n",
       "timedim    productname                perspective                            \n",
       "2018-01-01 AWS CloudTrail             Untagged                           0   \n",
       "           AWS Key Management Service Untagged                           0   \n",
       "           AWS Lambda                 Untagged                           0   \n",
       "           AWS Support (Business)     Untagged                           0   \n",
       "           AWS WAF                    Untagged                           0   \n",
       "\n",
       "                                                   ys_application_Redis  \\\n",
       "timedim    productname                perspective                         \n",
       "2018-01-01 AWS CloudTrail             Untagged                        0   \n",
       "           AWS Key Management Service Untagged                        0   \n",
       "           AWS Lambda                 Untagged                        0   \n",
       "           AWS Support (Business)     Untagged                        0   \n",
       "           AWS WAF                    Untagged                        0   \n",
       "\n",
       "                                                          ...          \\\n",
       "timedim    productname                perspective         ...           \n",
       "2018-01-01 AWS CloudTrail             Untagged            ...           \n",
       "           AWS Key Management Service Untagged            ...           \n",
       "           AWS Lambda                 Untagged            ...           \n",
       "           AWS Support (Business)     Untagged            ...           \n",
       "           AWS WAF                    Untagged            ...           \n",
       "\n",
       "                                                   productname_Amazon SimpleDB  \\\n",
       "timedim    productname                perspective                                \n",
       "2018-01-01 AWS CloudTrail             Untagged                               0   \n",
       "           AWS Key Management Service Untagged                               0   \n",
       "           AWS Lambda                 Untagged                               0   \n",
       "           AWS Support (Business)     Untagged                               0   \n",
       "           AWS WAF                    Untagged                               0   \n",
       "\n",
       "                                                   productname_AmazonCloudWatch  \\\n",
       "timedim    productname                perspective                                 \n",
       "2018-01-01 AWS CloudTrail             Untagged                                0   \n",
       "           AWS Key Management Service Untagged                                0   \n",
       "           AWS Lambda                 Untagged                                0   \n",
       "           AWS Support (Business)     Untagged                                0   \n",
       "           AWS WAF                    Untagged                                0   \n",
       "\n",
       "                                                   productname_OpenVPN Access Server (10 Connected Devices)  \\\n",
       "timedim    productname                perspective                                                             \n",
       "2018-01-01 AWS CloudTrail             Untagged                                                     0          \n",
       "           AWS Key Management Service Untagged                                                     0          \n",
       "           AWS Lambda                 Untagged                                                     0          \n",
       "           AWS Support (Business)     Untagged                                                     0          \n",
       "           AWS WAF                    Untagged                                                     0          \n",
       "\n",
       "                                                   perspective_Anomaly Detection  \\\n",
       "timedim    productname                perspective                                  \n",
       "2018-01-01 AWS CloudTrail             Untagged                                 0   \n",
       "           AWS Key Management Service Untagged                                 0   \n",
       "           AWS Lambda                 Untagged                                 0   \n",
       "           AWS Support (Business)     Untagged                                 0   \n",
       "           AWS WAF                    Untagged                                 0   \n",
       "\n",
       "                                                   perspective_Devops  \\\n",
       "timedim    productname                perspective                       \n",
       "2018-01-01 AWS CloudTrail             Untagged                      0   \n",
       "           AWS Key Management Service Untagged                      0   \n",
       "           AWS Lambda                 Untagged                      0   \n",
       "           AWS Support (Business)     Untagged                      0   \n",
       "           AWS WAF                    Untagged                      0   \n",
       "\n",
       "                                                   perspective_Infrastructure  \\\n",
       "timedim    productname                perspective                               \n",
       "2018-01-01 AWS CloudTrail             Untagged                              0   \n",
       "           AWS Key Management Service Untagged                              0   \n",
       "           AWS Lambda                 Untagged                              0   \n",
       "           AWS Support (Business)     Untagged                              0   \n",
       "           AWS WAF                    Untagged                              0   \n",
       "\n",
       "                                                   perspective_Recommendations  \\\n",
       "timedim    productname                perspective                                \n",
       "2018-01-01 AWS CloudTrail             Untagged                               0   \n",
       "           AWS Key Management Service Untagged                               0   \n",
       "           AWS Lambda                 Untagged                               0   \n",
       "           AWS Support (Business)     Untagged                               0   \n",
       "           AWS WAF                    Untagged                               0   \n",
       "\n",
       "                                                   perspective_Untagged  \\\n",
       "timedim    productname                perspective                         \n",
       "2018-01-01 AWS CloudTrail             Untagged                        1   \n",
       "           AWS Key Management Service Untagged                        1   \n",
       "           AWS Lambda                 Untagged                        1   \n",
       "           AWS Support (Business)     Untagged                        1   \n",
       "           AWS WAF                    Untagged                        1   \n",
       "\n",
       "                                                   perspective_Utilization Processing  \\\n",
       "timedim    productname                perspective                                       \n",
       "2018-01-01 AWS CloudTrail             Untagged                                      0   \n",
       "           AWS Key Management Service Untagged                                      0   \n",
       "           AWS Lambda                 Untagged                                      0   \n",
       "           AWS Support (Business)     Untagged                                      0   \n",
       "           AWS WAF                    Untagged                                      0   \n",
       "\n",
       "                                                   perspective_Webapp  \n",
       "timedim    productname                perspective                      \n",
       "2018-01-01 AWS CloudTrail             Untagged                      0  \n",
       "           AWS Key Management Service Untagged                      0  \n",
       "           AWS Lambda                 Untagged                      0  \n",
       "           AWS Support (Business)     Untagged                      0  \n",
       "           AWS WAF                    Untagged                      0  \n",
       "\n",
       "[5 rows x 188 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one-hot encoding\n",
    "data = pd.get_dummies(df)\n",
    "#data['productname']=df.productname\n",
    "#data['perspective']=df.perspective\n",
    "encoded = list(data.columns)\n",
    "print(\"{} total features after one-hot encoding.\".format(len(encoded)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.reset_index(inplace=True)\n",
    "data = data.set_index(['productname','perspective'])\n",
    "ori_column_num=data.shape[1]-1\n",
    "ori_column_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:1472: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  return self._getitem_tuple(key)\n"
     ]
    }
   ],
   "source": [
    "#divide the whole dataframe into dictionary, key is the perspective and product, value is the dataframe under this type\n",
    "prob_pers_index=pd.DataFrame(data.index).drop_duplicates()\n",
    "data_dict={}\n",
    "for i in range(len(prob_pers_index)):\n",
    "    temp_df=data.loc[prob_pers_index.iloc[i,0],:]\n",
    "    temp_df.set_index('timedim',inplace=True)\n",
    "    data_dict[prob_pers_index.iloc[i,0]]=temp_df\n",
    "    #print(prob_pers_index.iloc[i,0],len(data_dict[prob_pers_index.iloc[i,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of timesteps to go back for feature engineering\n",
    "def shift_series(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    dff = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    for i in range(n_in, 0, -1):\n",
    "        if i>=len(dff):\n",
    "            # if dataframe is not long enough, fill in with the last time period\n",
    "            cols.append(dff.shift(len(dff)-1))\n",
    "        else:\n",
    "            cols.append(dff.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    for i in range(0, n_out):\n",
    "        if i>=len(dff):\n",
    "            cols.append(dff.shift(1-len(dff)))\n",
    "        else:\n",
    "            cols.append(dff.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_period=7\n",
    "multi_time_data_dict={}\n",
    "\n",
    "# for every type (perspective x product), shift data to prepare features and labels\n",
    "for key in data_dict.keys():\n",
    "    temp=shift_series(data_dict[key],t_period,1)\n",
    "    label_index=['var1(t-'+str(t_period-i-1)+')' for i in range(t_period-1)]+['var1(t)']\n",
    "    label_data=temp.loc[:,label_index]\n",
    "    label_data.columns=['y(t-'+str(t_period-i-1)+')' for i in range(t_period-1)]+['y(t)']\n",
    "    temp.drop(temp.iloc[:,-ori_column_num:], axis=1, inplace=True)\n",
    "    temp=pd.concat([temp,label_data],axis=1)\n",
    "    #print(reframed_data.columns)\n",
    "    multi_time_data_dict[key]=temp\n",
    "    #print(key,len(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(215, 7, 188) (215, 7) (143, 7, 188) (143, 7)\n",
      "(430, 7, 188) (430, 7) (286, 7, 188) (286, 7)\n",
      "(645, 7, 188) (645, 7) (429, 7, 188) (429, 7)\n",
      "(684, 7, 188) (684, 7) (454, 7, 188) (454, 7)\n",
      "(899, 7, 188) (899, 7) (597, 7, 188) (597, 7)\n",
      "(1114, 7, 188) (1114, 7) (740, 7, 188) (740, 7)\n",
      "(1293, 7, 188) (1293, 7) (858, 7, 188) (858, 7)\n",
      "(1508, 7, 188) (1508, 7) (1001, 7, 188) (1001, 7)\n",
      "(1686, 7, 188) (1686, 7) (1118, 7, 188) (1118, 7)\n",
      "(1901, 7, 188) (1901, 7) (1261, 7, 188) (1261, 7)\n",
      "(2116, 7, 188) (2116, 7) (1404, 7, 188) (1404, 7)\n",
      "(2241, 7, 188) (2241, 7) (1487, 7, 188) (1487, 7)\n",
      "(2456, 7, 188) (2456, 7) (1630, 7, 188) (1630, 7)\n",
      "(2671, 7, 188) (2671, 7) (1773, 7, 188) (1773, 7)\n",
      "(2784, 7, 188) (2784, 7) (1847, 7, 188) (1847, 7)\n",
      "(2999, 7, 188) (2999, 7) (1990, 7, 188) (1990, 7)\n",
      "(3202, 7, 188) (3202, 7) (2125, 7, 188) (2125, 7)\n",
      "(3417, 7, 188) (3417, 7) (2268, 7, 188) (2268, 7)\n",
      "(3632, 7, 188) (3632, 7) (2411, 7, 188) (2411, 7)\n",
      "(3847, 7, 188) (3847, 7) (2554, 7, 188) (2554, 7)\n",
      "(4062, 7, 188) (4062, 7) (2697, 7, 188) (2697, 7)\n",
      "(4277, 7, 188) (4277, 7) (2840, 7, 188) (2840, 7)\n",
      "(4317, 7, 188) (4317, 7) (2866, 7, 188) (2866, 7)\n",
      "(4477, 7, 188) (4477, 7) (2972, 7, 188) (2972, 7)\n",
      "(4560, 7, 188) (4560, 7) (3026, 7, 188) (3026, 7)\n",
      "(4561, 7, 188) (4561, 7) (3026, 7, 188) (3026, 7)\n",
      "(4562, 7, 188) (4562, 7) (3026, 7, 188) (3026, 7)\n",
      "(4590, 7, 188) (4590, 7) (3044, 7, 188) (3044, 7)\n",
      "(4591, 7, 188) (4591, 7) (3044, 7, 188) (3044, 7)\n",
      "(4642, 7, 188) (4642, 7) (3077, 7, 188) (3077, 7)\n",
      "(4694, 7, 188) (4694, 7) (3110, 7, 188) (3110, 7)\n",
      "(4743, 7, 188) (4743, 7) (3141, 7, 188) (3141, 7)\n",
      "(4784, 7, 188) (4784, 7) (3168, 7, 188) (3168, 7)\n",
      "(4819, 7, 188) (4819, 7) (3191, 7, 188) (3191, 7)\n",
      "(4854, 7, 188) (4854, 7) (3213, 7, 188) (3213, 7)\n",
      "(4889, 7, 188) (4889, 7) (3235, 7, 188) (3235, 7)\n",
      "(4924, 7, 188) (4924, 7) (3257, 7, 188) (3257, 7)\n",
      "(4959, 7, 188) (4959, 7) (3279, 7, 188) (3279, 7)\n",
      "(4994, 7, 188) (4994, 7) (3301, 7, 188) (3301, 7)\n",
      "(5029, 7, 188) (5029, 7) (3323, 7, 188) (3323, 7)\n",
      "(5063, 7, 188) (5063, 7) (3345, 7, 188) (3345, 7)\n",
      "(5097, 7, 188) (5097, 7) (3367, 7, 188) (3367, 7)\n",
      "(5131, 7, 188) (5131, 7) (3389, 7, 188) (3389, 7)\n",
      "(5165, 7, 188) (5165, 7) (3411, 7, 188) (3411, 7)\n",
      "(5199, 7, 188) (5199, 7) (3433, 7, 188) (3433, 7)\n",
      "(5233, 7, 188) (5233, 7) (3455, 7, 188) (3455, 7)\n",
      "(5266, 7, 188) (5266, 7) (3476, 7, 188) (3476, 7)\n",
      "(5299, 7, 188) (5299, 7) (3497, 7, 188) (3497, 7)\n",
      "(5332, 7, 188) (5332, 7) (3518, 7, 188) (3518, 7)\n",
      "(5365, 7, 188) (5365, 7) (3539, 7, 188) (3539, 7)\n",
      "(5398, 7, 188) (5398, 7) (3560, 7, 188) (3560, 7)\n",
      "(5431, 7, 188) (5431, 7) (3581, 7, 188) (3581, 7)\n",
      "(5464, 7, 188) (5464, 7) (3602, 7, 188) (3602, 7)\n",
      "(5497, 7, 188) (5497, 7) (3623, 7, 188) (3623, 7)\n",
      "(5530, 7, 188) (5530, 7) (3644, 7, 188) (3644, 7)\n",
      "(5563, 7, 188) (5563, 7) (3665, 7, 188) (3665, 7)\n",
      "(5596, 7, 188) (5596, 7) (3686, 7, 188) (3686, 7)\n",
      "(5624, 7, 188) (5624, 7) (3704, 7, 188) (3704, 7)\n",
      "(5657, 7, 188) (5657, 7) (3725, 7, 188) (3725, 7)\n",
      "(5686, 7, 188) (5686, 7) (3744, 7, 188) (3744, 7)\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for key in multi_time_data_dict.keys():\n",
    "    values=multi_time_data_dict[key].values\n",
    "    train_index = int(values.shape[0]*0.6)+1\n",
    "    train = values[:train_index, :]\n",
    "    test = values[train_index:, :]\n",
    "    sub_train_X, sub_train_y = train[:, :-t_period], train[:, -t_period:]\n",
    "    sub_test_X, sub_test_y = test[:, :-t_period], test[:, -t_period:]\n",
    "    sub_train_X = sub_train_X.reshape((sub_train_X.shape[0], t_period, ori_column_num))\n",
    "    sub_test_X = sub_test_X.reshape((sub_test_X.shape[0], t_period, ori_column_num))\n",
    "    if i==0:\n",
    "        train_X,train_y,test_X,test_y=sub_train_X,sub_train_y,sub_test_X,sub_test_y\n",
    "    else:\n",
    "        train_X=np.concatenate([train_X,sub_train_X])\n",
    "        train_y=np.concatenate([train_y,sub_train_y])\n",
    "        test_X=np.concatenate([test_X,sub_test_X])\n",
    "        test_y=np.concatenate([test_y,sub_test_y])\n",
    "    i+=1\n",
    "    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import numpy as np \n",
    "from scipy.stats import randint\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns  \n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD \n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "import itertools\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1004 12:43:43.655645 4572501440 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5686 samples, validate on 3744 samples\n",
      "Epoch 1/1000\n",
      "5686/5686 [==============================] - 1s 221us/step - loss: 380.3345 - val_loss: 1032.5774\n",
      "Epoch 2/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 357.5740 - val_loss: 1005.2345\n",
      "Epoch 3/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 344.2625 - val_loss: 988.1675\n",
      "Epoch 4/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 334.5235 - val_loss: 974.5192\n",
      "Epoch 5/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 326.6577 - val_loss: 961.3993\n",
      "Epoch 6/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 318.8164 - val_loss: 948.3490\n",
      "Epoch 7/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 311.3909 - val_loss: 937.5790\n",
      "Epoch 8/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 304.6131 - val_loss: 926.6242\n",
      "Epoch 9/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 297.9110 - val_loss: 916.2048\n",
      "Epoch 10/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 291.4163 - val_loss: 906.8651\n",
      "Epoch 11/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 285.6292 - val_loss: 895.6372\n",
      "Epoch 12/1000\n",
      "5686/5686 [==============================] - 1s 120us/step - loss: 282.1250 - val_loss: 887.1318\n",
      "Epoch 13/1000\n",
      "5686/5686 [==============================] - 1s 125us/step - loss: 274.9409 - val_loss: 877.1413\n",
      "Epoch 14/1000\n",
      "5686/5686 [==============================] - 1s 121us/step - loss: 269.6934 - val_loss: 868.4464\n",
      "Epoch 15/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 263.7615 - val_loss: 859.3648\n",
      "Epoch 16/1000\n",
      "5686/5686 [==============================] - 1s 105us/step - loss: 258.9523 - val_loss: 851.0949\n",
      "Epoch 17/1000\n",
      "5686/5686 [==============================] - 1s 105us/step - loss: 254.0233 - val_loss: 843.4744\n",
      "Epoch 18/1000\n",
      "5686/5686 [==============================] - 1s 106us/step - loss: 249.5356 - val_loss: 834.7252\n",
      "Epoch 19/1000\n",
      "5686/5686 [==============================] - 1s 106us/step - loss: 244.6569 - val_loss: 826.0178\n",
      "Epoch 20/1000\n",
      "5686/5686 [==============================] - 1s 106us/step - loss: 239.9403 - val_loss: 818.0645\n",
      "Epoch 21/1000\n",
      "5686/5686 [==============================] - 1s 106us/step - loss: 235.6274 - val_loss: 810.6290\n",
      "Epoch 22/1000\n",
      "5686/5686 [==============================] - 1s 107us/step - loss: 231.9728 - val_loss: 804.3251\n",
      "Epoch 23/1000\n",
      "5686/5686 [==============================] - 1s 107us/step - loss: 227.9866 - val_loss: 796.3234\n",
      "Epoch 24/1000\n",
      "5686/5686 [==============================] - 1s 107us/step - loss: 223.9163 - val_loss: 789.1398\n",
      "Epoch 25/1000\n",
      "5686/5686 [==============================] - 1s 107us/step - loss: 219.6944 - val_loss: 780.9947\n",
      "Epoch 26/1000\n",
      "5686/5686 [==============================] - 1s 107us/step - loss: 215.9255 - val_loss: 774.1303\n",
      "Epoch 27/1000\n",
      "5686/5686 [==============================] - 1s 107us/step - loss: 211.5545 - val_loss: 766.9445\n",
      "Epoch 28/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 208.0750 - val_loss: 760.0743\n",
      "Epoch 29/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 204.4956 - val_loss: 753.3213\n",
      "Epoch 30/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 201.1577 - val_loss: 746.6397\n",
      "Epoch 31/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 198.4337 - val_loss: 740.3890\n",
      "Epoch 32/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 194.0948 - val_loss: 733.6864\n",
      "Epoch 33/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 190.4931 - val_loss: 727.0681\n",
      "Epoch 34/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 187.2370 - val_loss: 719.4271\n",
      "Epoch 35/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 183.9629 - val_loss: 713.4237\n",
      "Epoch 36/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 179.9981 - val_loss: 706.8627\n",
      "Epoch 37/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 177.0351 - val_loss: 700.6030\n",
      "Epoch 38/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 174.0731 - val_loss: 695.4463\n",
      "Epoch 39/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 170.7912 - val_loss: 689.1090\n",
      "Epoch 40/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 168.1678 - val_loss: 683.1184\n",
      "Epoch 41/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 165.0927 - val_loss: 678.9778\n",
      "Epoch 42/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 162.1703 - val_loss: 671.2383\n",
      "Epoch 43/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 159.0058 - val_loss: 665.5874\n",
      "Epoch 44/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 157.1362 - val_loss: 660.3883\n",
      "Epoch 45/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 154.4582 - val_loss: 654.5041\n",
      "Epoch 46/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 151.1727 - val_loss: 649.2871\n",
      "Epoch 47/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 148.2633 - val_loss: 644.1884\n",
      "Epoch 48/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 145.9452 - val_loss: 638.8881\n",
      "Epoch 49/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 143.9117 - val_loss: 633.7592\n",
      "Epoch 50/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 140.7986 - val_loss: 628.0988\n",
      "Epoch 51/1000\n",
      "5686/5686 [==============================] - 1s 121us/step - loss: 137.4119 - val_loss: 622.7927\n",
      "Epoch 52/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 136.4656 - val_loss: 618.4339\n",
      "Epoch 53/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 133.8258 - val_loss: 613.2086\n",
      "Epoch 54/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 130.9658 - val_loss: 608.0056\n",
      "Epoch 55/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 128.6806 - val_loss: 604.9282\n",
      "Epoch 56/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 127.6981 - val_loss: 598.6729\n",
      "Epoch 57/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 124.4073 - val_loss: 593.3278\n",
      "Epoch 58/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 122.3231 - val_loss: 588.7681\n",
      "Epoch 59/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 120.1938 - val_loss: 584.7471\n",
      "Epoch 60/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 117.9604 - val_loss: 580.2589\n",
      "Epoch 61/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 116.4409 - val_loss: 575.1605\n",
      "Epoch 62/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 114.2208 - val_loss: 571.0134\n",
      "Epoch 63/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 112.4539 - val_loss: 566.7014\n",
      "Epoch 64/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 110.4617 - val_loss: 562.0960\n",
      "Epoch 65/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 108.9820 - val_loss: 558.2723\n",
      "Epoch 66/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 107.6817 - val_loss: 554.0217\n",
      "Epoch 67/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 105.6972 - val_loss: 550.2530\n",
      "Epoch 68/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 104.9436 - val_loss: 546.3058\n",
      "Epoch 69/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 102.1318 - val_loss: 542.6423\n",
      "Epoch 70/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 100.0646 - val_loss: 537.2657\n",
      "Epoch 71/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 98.8893 - val_loss: 533.2320\n",
      "Epoch 72/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 96.1916 - val_loss: 529.9808\n",
      "Epoch 73/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 94.8944 - val_loss: 525.4014\n",
      "Epoch 74/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 93.8920 - val_loss: 521.6200\n",
      "Epoch 75/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 91.9833 - val_loss: 517.9101\n",
      "Epoch 76/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 91.2183 - val_loss: 514.5564\n",
      "Epoch 77/1000\n",
      "5686/5686 [==============================] - 1s 121us/step - loss: 90.0504 - val_loss: 510.7947\n",
      "Epoch 78/1000\n",
      "5686/5686 [==============================] - 1s 123us/step - loss: 88.4500 - val_loss: 506.9083\n",
      "Epoch 79/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 86.7131 - val_loss: 503.5013\n",
      "Epoch 80/1000\n",
      "5686/5686 [==============================] - 1s 121us/step - loss: 85.7569 - val_loss: 500.5985\n",
      "Epoch 81/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 84.8986 - val_loss: 496.6320\n",
      "Epoch 82/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 83.0102 - val_loss: 493.8055\n",
      "Epoch 83/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 82.2517 - val_loss: 491.0308\n",
      "Epoch 84/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 86.9883 - val_loss: 488.1059\n",
      "Epoch 85/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 79.3001 - val_loss: 485.5890\n",
      "Epoch 86/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 79.4355 - val_loss: 482.1186\n",
      "Epoch 87/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 78.7340 - val_loss: 480.4095\n",
      "Epoch 88/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 78.7062 - val_loss: 508.9247\n",
      "Epoch 89/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 87.5190 - val_loss: 486.7598\n",
      "Epoch 90/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 78.6373 - val_loss: 482.1880\n",
      "Epoch 91/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 78.0002 - val_loss: 472.1247\n",
      "Epoch 92/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 75.2551 - val_loss: 468.1312\n",
      "Epoch 93/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 75.0187 - val_loss: 464.8175\n",
      "Epoch 94/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 73.1338 - val_loss: 463.2249\n",
      "Epoch 95/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 71.4752 - val_loss: 477.8007\n",
      "Epoch 96/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 75.9767 - val_loss: 477.1329\n",
      "Epoch 97/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 76.7285 - val_loss: 461.4747\n",
      "Epoch 98/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 70.2594 - val_loss: 453.1993\n",
      "Epoch 99/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 68.3647 - val_loss: 451.6708\n",
      "Epoch 100/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 67.2193 - val_loss: 442.3837\n",
      "Epoch 101/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 65.3287 - val_loss: 462.9948\n",
      "Epoch 102/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 70.9223 - val_loss: 450.2822\n",
      "Epoch 103/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 66.6221 - val_loss: 545.9504\n",
      "Epoch 104/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 93.0889 - val_loss: 495.8091\n",
      "Epoch 105/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 82.1476 - val_loss: 482.7628\n",
      "Epoch 106/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 78.1534 - val_loss: 474.7404\n",
      "Epoch 107/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 75.7489 - val_loss: 466.8103\n",
      "Epoch 108/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 73.3090 - val_loss: 464.0631\n",
      "Epoch 109/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 71.8702 - val_loss: 458.1542\n",
      "Epoch 110/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 69.8213 - val_loss: 450.0811\n",
      "Epoch 111/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 67.2525 - val_loss: 446.0941\n",
      "Epoch 112/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 67.0683 - val_loss: 443.6130\n",
      "Epoch 113/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 65.1517 - val_loss: 440.2805\n",
      "Epoch 114/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 64.3520 - val_loss: 437.2344\n",
      "Epoch 115/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 63.2510 - val_loss: 434.0832\n",
      "Epoch 116/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 62.3127 - val_loss: 431.1387\n",
      "Epoch 117/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 61.9384 - val_loss: 428.1453\n",
      "Epoch 118/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 59.5464 - val_loss: 425.5546\n",
      "Epoch 119/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 59.8707 - val_loss: 422.5330\n",
      "Epoch 120/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 59.0266 - val_loss: 421.0092\n",
      "Epoch 121/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 57.7264 - val_loss: 416.8964\n",
      "Epoch 122/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 57.0209 - val_loss: 414.4168\n",
      "Epoch 123/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 56.6813 - val_loss: 411.4844\n",
      "Epoch 124/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 55.6016 - val_loss: 409.6791\n",
      "Epoch 125/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 56.1604 - val_loss: 405.8465\n",
      "Epoch 126/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 53.8482 - val_loss: 396.5564\n",
      "Epoch 127/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 52.1583 - val_loss: 388.6346\n",
      "Epoch 128/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 50.8636 - val_loss: 385.9712\n",
      "Epoch 129/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 48.6282 - val_loss: 383.2029\n",
      "Epoch 130/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 49.0363 - val_loss: 380.7633\n",
      "Epoch 131/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 47.7616 - val_loss: 376.3744\n",
      "Epoch 132/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 47.1522 - val_loss: 374.7354\n",
      "Epoch 133/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 45.3497 - val_loss: 372.2739\n",
      "Epoch 134/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 46.7825 - val_loss: 369.5162\n",
      "Epoch 135/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 44.4606 - val_loss: 366.7772\n",
      "Epoch 136/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 43.7795 - val_loss: 364.6926\n",
      "Epoch 137/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 44.1977 - val_loss: 362.2766\n",
      "Epoch 138/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 42.9898 - val_loss: 360.3226\n",
      "Epoch 139/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 42.8273 - val_loss: 357.7956\n",
      "Epoch 140/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 41.6907 - val_loss: 355.6765\n",
      "Epoch 141/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 41.7751 - val_loss: 353.6445\n",
      "Epoch 142/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 40.5745 - val_loss: 351.1629\n",
      "Epoch 143/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 41.0786 - val_loss: 348.8993\n",
      "Epoch 144/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 39.5491 - val_loss: 346.3074\n",
      "Epoch 145/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 39.5138 - val_loss: 340.2232\n",
      "Epoch 146/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 37.4198 - val_loss: 336.0150\n",
      "Epoch 147/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 37.1398 - val_loss: 333.3723\n",
      "Epoch 148/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5686/5686 [==============================] - 1s 110us/step - loss: 36.7067 - val_loss: 331.6503\n",
      "Epoch 149/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 36.6294 - val_loss: 329.0444\n",
      "Epoch 150/1000\n",
      "5686/5686 [==============================] - 1s 127us/step - loss: 36.2274 - val_loss: 326.4971\n",
      "Epoch 151/1000\n",
      "5686/5686 [==============================] - 1s 121us/step - loss: 35.0739 - val_loss: 325.2646\n",
      "Epoch 152/1000\n",
      "5686/5686 [==============================] - 1s 122us/step - loss: 35.0867 - val_loss: 322.4739\n",
      "Epoch 153/1000\n",
      "5686/5686 [==============================] - 1s 118us/step - loss: 33.9263 - val_loss: 320.3348\n",
      "Epoch 154/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 33.4627 - val_loss: 319.0804\n",
      "Epoch 155/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 33.6187 - val_loss: 317.0952\n",
      "Epoch 156/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 34.1516 - val_loss: 314.5655\n",
      "Epoch 157/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 32.2474 - val_loss: 313.8319\n",
      "Epoch 158/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 32.2987 - val_loss: 311.0209\n",
      "Epoch 159/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 31.3273 - val_loss: 308.4162\n",
      "Epoch 160/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 31.8651 - val_loss: 307.5127\n",
      "Epoch 161/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 30.9002 - val_loss: 306.0078\n",
      "Epoch 162/1000\n",
      "5686/5686 [==============================] - 1s 119us/step - loss: 31.3070 - val_loss: 303.2660\n",
      "Epoch 163/1000\n",
      "5686/5686 [==============================] - 1s 118us/step - loss: 30.6242 - val_loss: 301.2873\n",
      "Epoch 164/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 29.8957 - val_loss: 299.3832\n",
      "Epoch 165/1000\n",
      "5686/5686 [==============================] - 1s 119us/step - loss: 29.2754 - val_loss: 297.3977\n",
      "Epoch 166/1000\n",
      "5686/5686 [==============================] - 1s 123us/step - loss: 29.0196 - val_loss: 295.6072\n",
      "Epoch 167/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 28.9162 - val_loss: 294.4470\n",
      "Epoch 168/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 28.5222 - val_loss: 292.2847\n",
      "Epoch 169/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 28.3911 - val_loss: 290.4903\n",
      "Epoch 170/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 28.4191 - val_loss: 289.1649\n",
      "Epoch 171/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 28.3722 - val_loss: 287.2261\n",
      "Epoch 172/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 27.8362 - val_loss: 286.1312\n",
      "Epoch 173/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 27.3517 - val_loss: 283.4972\n",
      "Epoch 174/1000\n",
      "5686/5686 [==============================] - 1s 121us/step - loss: 26.0954 - val_loss: 281.9513\n",
      "Epoch 175/1000\n",
      "5686/5686 [==============================] - 1s 125us/step - loss: 27.6159 - val_loss: 280.9049\n",
      "Epoch 176/1000\n",
      "5686/5686 [==============================] - 1s 118us/step - loss: 29.5002 - val_loss: 286.4375\n",
      "Epoch 177/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 32.0157 - val_loss: 412.0443\n",
      "Epoch 178/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 51.9358 - val_loss: 395.1524\n",
      "Epoch 179/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 51.0976 - val_loss: 338.8361\n",
      "Epoch 180/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 35.2502 - val_loss: 292.4639\n",
      "Epoch 181/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 27.6787 - val_loss: 285.4158\n",
      "Epoch 182/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 26.7163 - val_loss: 279.4350\n",
      "Epoch 183/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 25.3327 - val_loss: 276.7614\n",
      "Epoch 184/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 25.3617 - val_loss: 274.7374\n",
      "Epoch 185/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 25.9203 - val_loss: 273.2966\n",
      "Epoch 186/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 25.0708 - val_loss: 271.4139\n",
      "Epoch 187/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 25.1465 - val_loss: 269.6119\n",
      "Epoch 188/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 24.6931 - val_loss: 268.0253\n",
      "Epoch 189/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 24.5552 - val_loss: 266.4123\n",
      "Epoch 190/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 24.4489 - val_loss: 265.3656\n",
      "Epoch 191/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 24.1322 - val_loss: 259.8339\n",
      "Epoch 192/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 23.3420 - val_loss: 253.4240\n",
      "Epoch 193/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 22.2877 - val_loss: 251.2155\n",
      "Epoch 194/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 20.7671 - val_loss: 245.2955\n",
      "Epoch 195/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 21.0546 - val_loss: 241.8389\n",
      "Epoch 196/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 23.7678 - val_loss: 240.7977\n",
      "Epoch 197/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 25.2395 - val_loss: 240.3663\n",
      "Epoch 198/1000\n",
      "5686/5686 [==============================] - 1s 126us/step - loss: 23.3327 - val_loss: 238.5312\n",
      "Epoch 199/1000\n",
      "5686/5686 [==============================] - 1s 123us/step - loss: 22.4130 - val_loss: 237.8452\n",
      "Epoch 200/1000\n",
      "5686/5686 [==============================] - 1s 120us/step - loss: 20.5840 - val_loss: 236.2194\n",
      "Epoch 201/1000\n",
      "5686/5686 [==============================] - 1s 124us/step - loss: 20.0287 - val_loss: 234.9729\n",
      "Epoch 202/1000\n",
      "5686/5686 [==============================] - 1s 126us/step - loss: 19.2894 - val_loss: 233.5504\n",
      "Epoch 203/1000\n",
      "5686/5686 [==============================] - 1s 120us/step - loss: 19.3382 - val_loss: 232.6617\n",
      "Epoch 204/1000\n",
      "5686/5686 [==============================] - 1s 121us/step - loss: 20.5910 - val_loss: 231.4662\n",
      "Epoch 205/1000\n",
      "5686/5686 [==============================] - 1s 125us/step - loss: 19.9348 - val_loss: 230.3027\n",
      "Epoch 206/1000\n",
      "5686/5686 [==============================] - 1s 119us/step - loss: 20.1749 - val_loss: 229.0803\n",
      "Epoch 207/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 19.8293 - val_loss: 227.6981\n",
      "Epoch 208/1000\n",
      "5686/5686 [==============================] - 1s 138us/step - loss: 18.5849 - val_loss: 226.9044\n",
      "Epoch 209/1000\n",
      "5686/5686 [==============================] - 1s 126us/step - loss: 18.7402 - val_loss: 225.3892\n",
      "Epoch 210/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 20.0470 - val_loss: 224.7782\n",
      "Epoch 211/1000\n",
      "5686/5686 [==============================] - 1s 122us/step - loss: 18.5893 - val_loss: 223.3108\n",
      "Epoch 212/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 19.4924 - val_loss: 223.2932\n",
      "Epoch 213/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 19.1052 - val_loss: 221.6808\n",
      "Epoch 214/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 17.5906 - val_loss: 220.1078\n",
      "Epoch 215/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 17.8174 - val_loss: 219.0743\n",
      "Epoch 216/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 17.6773 - val_loss: 217.4711\n",
      "Epoch 217/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 17.8066 - val_loss: 216.6137\n",
      "Epoch 218/1000\n",
      "5686/5686 [==============================] - 1s 123us/step - loss: 18.6171 - val_loss: 215.1449\n",
      "Epoch 219/1000\n",
      "5686/5686 [==============================] - 1s 121us/step - loss: 19.1359 - val_loss: 215.1550\n",
      "Epoch 220/1000\n",
      "5686/5686 [==============================] - 1s 125us/step - loss: 18.7901 - val_loss: 214.0319\n",
      "Epoch 221/1000\n",
      "5686/5686 [==============================] - 1s 120us/step - loss: 17.6906 - val_loss: 212.1873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 222/1000\n",
      "5686/5686 [==============================] - 1s 127us/step - loss: 18.4105 - val_loss: 211.9577\n",
      "Epoch 223/1000\n",
      "5686/5686 [==============================] - 1s 126us/step - loss: 18.4666 - val_loss: 211.1332\n",
      "Epoch 224/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 19.2194 - val_loss: 213.4271\n",
      "Epoch 225/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 18.3418 - val_loss: 208.9613\n",
      "Epoch 226/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 19.4388 - val_loss: 208.3605\n",
      "Epoch 227/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 17.8256 - val_loss: 207.1472\n",
      "Epoch 228/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 18.8914 - val_loss: 206.5856\n",
      "Epoch 229/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 17.4838 - val_loss: 205.6802\n",
      "Epoch 230/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 17.1314 - val_loss: 204.5423\n",
      "Epoch 231/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 16.6240 - val_loss: 203.2487\n",
      "Epoch 232/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 17.8019 - val_loss: 202.7250\n",
      "Epoch 233/1000\n",
      "5686/5686 [==============================] - 1s 118us/step - loss: 16.5825 - val_loss: 201.1970\n",
      "Epoch 234/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 16.5430 - val_loss: 200.3257\n",
      "Epoch 235/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 16.3727 - val_loss: 198.7936\n",
      "Epoch 236/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 18.3534 - val_loss: 198.7456\n",
      "Epoch 237/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 17.5081 - val_loss: 197.2378\n",
      "Epoch 238/1000\n",
      "5686/5686 [==============================] - 1s 119us/step - loss: 17.1176 - val_loss: 196.7197\n",
      "Epoch 239/1000\n",
      "5686/5686 [==============================] - 1s 118us/step - loss: 16.5420 - val_loss: 195.1257\n",
      "Epoch 240/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 16.2099 - val_loss: 194.7391\n",
      "Epoch 241/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 15.3562 - val_loss: 193.3363\n",
      "Epoch 242/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 15.1974 - val_loss: 192.2282\n",
      "Epoch 243/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 15.2747 - val_loss: 191.0751\n",
      "Epoch 244/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 15.3629 - val_loss: 190.1733\n",
      "Epoch 245/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 14.6671 - val_loss: 188.8817\n",
      "Epoch 246/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 15.6771 - val_loss: 188.1094\n",
      "Epoch 247/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 16.2308 - val_loss: 186.3739\n",
      "Epoch 248/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 16.5172 - val_loss: 187.3259\n",
      "Epoch 249/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 16.9165 - val_loss: 185.0087\n",
      "Epoch 250/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 15.7142 - val_loss: 184.5417\n",
      "Epoch 251/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 15.0202 - val_loss: 183.0835\n",
      "Epoch 252/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 14.2627 - val_loss: 182.4269\n",
      "Epoch 253/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 14.7429 - val_loss: 180.7576\n",
      "Epoch 254/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 16.1397 - val_loss: 182.6959\n",
      "Epoch 255/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 16.2190 - val_loss: 179.5159\n",
      "Epoch 256/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 15.1909 - val_loss: 179.1270\n",
      "Epoch 257/1000\n",
      "5686/5686 [==============================] - 1s 119us/step - loss: 14.4014 - val_loss: 177.4453\n",
      "Epoch 258/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 13.9995 - val_loss: 177.2896\n",
      "Epoch 259/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 14.4674 - val_loss: 176.0554\n",
      "Epoch 260/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 14.1097 - val_loss: 174.9242\n",
      "Epoch 261/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 13.7755 - val_loss: 173.6681\n",
      "Epoch 262/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 13.5679 - val_loss: 173.2708\n",
      "Epoch 263/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 14.4640 - val_loss: 171.8173\n",
      "Epoch 264/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 15.4525 - val_loss: 173.4936\n",
      "Epoch 265/1000\n",
      "5686/5686 [==============================] - 1s 118us/step - loss: 15.7480 - val_loss: 170.7544\n",
      "Epoch 266/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 14.7429 - val_loss: 170.5717\n",
      "Epoch 267/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 13.1987 - val_loss: 168.9554\n",
      "Epoch 268/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 12.9688 - val_loss: 167.6219\n",
      "Epoch 269/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 13.9246 - val_loss: 167.8257\n",
      "Epoch 270/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 14.4264 - val_loss: 165.5864\n",
      "Epoch 271/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 15.4430 - val_loss: 168.0293\n",
      "Epoch 272/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 15.4694 - val_loss: 164.3374\n",
      "Epoch 273/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 14.1356 - val_loss: 163.8750\n",
      "Epoch 274/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 13.7628 - val_loss: 162.5638\n",
      "Epoch 275/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 12.8322 - val_loss: 161.7534\n",
      "Epoch 276/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 12.6985 - val_loss: 160.5398\n",
      "Epoch 277/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 12.5267 - val_loss: 161.0695\n",
      "Epoch 278/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 14.4904 - val_loss: 158.7082\n",
      "Epoch 279/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 13.5543 - val_loss: 162.6853\n",
      "Epoch 280/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 14.3093 - val_loss: 156.9301\n",
      "Epoch 281/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 12.4776 - val_loss: 156.3233\n",
      "Epoch 282/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 12.8552 - val_loss: 155.6537\n",
      "Epoch 283/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 12.0267 - val_loss: 154.6811\n",
      "Epoch 284/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 12.0408 - val_loss: 153.6407\n",
      "Epoch 285/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 11.9055 - val_loss: 152.5195\n",
      "Epoch 286/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 12.3679 - val_loss: 151.8991\n",
      "Epoch 287/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 11.7368 - val_loss: 150.7865\n",
      "Epoch 288/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 11.3313 - val_loss: 150.2645\n",
      "Epoch 289/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 11.1944 - val_loss: 149.5861\n",
      "Epoch 290/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 12.0275 - val_loss: 148.1438\n",
      "Epoch 291/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 11.9837 - val_loss: 148.8502\n",
      "Epoch 292/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 13.0249 - val_loss: 147.4393\n",
      "Epoch 293/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 14.3563 - val_loss: 154.0209\n",
      "Epoch 294/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 16.4617 - val_loss: 145.8623\n",
      "Epoch 295/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 13.5907 - val_loss: 145.8318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 11.7043 - val_loss: 144.6994\n",
      "Epoch 297/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 11.0707 - val_loss: 144.0505\n",
      "Epoch 298/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 11.3111 - val_loss: 142.9998\n",
      "Epoch 299/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 11.9788 - val_loss: 144.6539\n",
      "Epoch 300/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 12.0156 - val_loss: 141.1999\n",
      "Epoch 301/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 11.0978 - val_loss: 140.8486\n",
      "Epoch 302/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 10.7046 - val_loss: 139.9736\n",
      "Epoch 303/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 10.1575 - val_loss: 139.2565\n",
      "Epoch 304/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 10.8252 - val_loss: 138.2836\n",
      "Epoch 305/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 11.1896 - val_loss: 138.0328\n",
      "Epoch 306/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 11.8120 - val_loss: 136.6910\n",
      "Epoch 307/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 12.2768 - val_loss: 142.8011\n",
      "Epoch 308/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 13.0003 - val_loss: 135.0960\n",
      "Epoch 309/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 12.1565 - val_loss: 135.9822\n",
      "Epoch 310/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 12.2180 - val_loss: 134.1232\n",
      "Epoch 311/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 11.2062 - val_loss: 134.2859\n",
      "Epoch 312/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 10.8190 - val_loss: 132.7109\n",
      "Epoch 313/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 10.7970 - val_loss: 132.6173\n",
      "Epoch 314/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 10.3990 - val_loss: 131.6351\n",
      "Epoch 315/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 10.2008 - val_loss: 131.4030\n",
      "Epoch 316/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 9.7329 - val_loss: 130.3996\n",
      "Epoch 317/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 10.4232 - val_loss: 129.7197\n",
      "Epoch 318/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 9.8478 - val_loss: 128.7488\n",
      "Epoch 319/1000\n",
      "5686/5686 [==============================] - 1s 125us/step - loss: 10.6325 - val_loss: 132.7519\n",
      "Epoch 320/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 11.9771 - val_loss: 127.5095\n",
      "Epoch 321/1000\n",
      "5686/5686 [==============================] - 1s 128us/step - loss: 12.6297 - val_loss: 129.0436\n",
      "Epoch 322/1000\n",
      "5686/5686 [==============================] - 1s 122us/step - loss: 11.0626 - val_loss: 126.6917\n",
      "Epoch 323/1000\n",
      "5686/5686 [==============================] - 1s 118us/step - loss: 11.1414 - val_loss: 127.7845\n",
      "Epoch 324/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 10.2250 - val_loss: 125.5986\n",
      "Epoch 325/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 10.3403 - val_loss: 127.4347\n",
      "Epoch 326/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 11.0556 - val_loss: 124.5170\n",
      "Epoch 327/1000\n",
      "5686/5686 [==============================] - 1s 119us/step - loss: 11.6891 - val_loss: 127.1744\n",
      "Epoch 328/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 10.6552 - val_loss: 123.1492\n",
      "Epoch 329/1000\n",
      "5686/5686 [==============================] - 1s 131us/step - loss: 9.4960 - val_loss: 123.1745\n",
      "Epoch 330/1000\n",
      "5686/5686 [==============================] - 1s 125us/step - loss: 9.7051 - val_loss: 122.0361\n",
      "Epoch 331/1000\n",
      "5686/5686 [==============================] - 1s 120us/step - loss: 9.4869 - val_loss: 121.4187\n",
      "Epoch 332/1000\n",
      "5686/5686 [==============================] - 1s 129us/step - loss: 9.4832 - val_loss: 120.8057\n",
      "Epoch 333/1000\n",
      "5686/5686 [==============================] - 1s 118us/step - loss: 9.5837 - val_loss: 120.0650\n",
      "Epoch 334/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 8.8140 - val_loss: 119.3815\n",
      "Epoch 335/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 9.2308 - val_loss: 118.8823\n",
      "Epoch 336/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 9.0360 - val_loss: 118.2173\n",
      "Epoch 337/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 11.4942 - val_loss: 131.8778\n",
      "Epoch 338/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 13.1473 - val_loss: 117.6883\n",
      "Epoch 339/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 12.0273 - val_loss: 120.2512\n",
      "Epoch 340/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 10.6398 - val_loss: 116.9328\n",
      "Epoch 341/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 10.3269 - val_loss: 117.9370\n",
      "Epoch 342/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 9.6484 - val_loss: 115.8517\n",
      "Epoch 343/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 9.0330 - val_loss: 116.3203\n",
      "Epoch 344/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 9.2536 - val_loss: 114.6943\n",
      "Epoch 345/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 8.9704 - val_loss: 114.4629\n",
      "Epoch 346/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 9.1022 - val_loss: 113.7180\n",
      "Epoch 347/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 8.7905 - val_loss: 113.4072\n",
      "Epoch 348/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 8.3661 - val_loss: 112.5517\n",
      "Epoch 349/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 8.8038 - val_loss: 114.2571\n",
      "Epoch 350/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 10.0160 - val_loss: 112.2046\n",
      "Epoch 351/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 11.1964 - val_loss: 123.0803\n",
      "Epoch 352/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 11.3819 - val_loss: 111.0740\n",
      "Epoch 353/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 10.3630 - val_loss: 111.7491\n",
      "Epoch 354/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 8.6038 - val_loss: 110.5418\n",
      "Epoch 355/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 8.9538 - val_loss: 110.2973\n",
      "Epoch 356/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 9.4966 - val_loss: 109.4458\n",
      "Epoch 357/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 8.9736 - val_loss: 109.3055\n",
      "Epoch 358/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 8.9584 - val_loss: 108.6769\n",
      "Epoch 359/1000\n",
      "5686/5686 [==============================] - 1s 120us/step - loss: 9.5114 - val_loss: 111.8183\n",
      "Epoch 360/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 10.0632 - val_loss: 108.0829\n",
      "Epoch 361/1000\n",
      "5686/5686 [==============================] - 1s 127us/step - loss: 9.7683 - val_loss: 108.9750\n",
      "Epoch 362/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 9.7652 - val_loss: 106.7848\n",
      "Epoch 363/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 8.9694 - val_loss: 107.7583\n",
      "Epoch 364/1000\n",
      "5686/5686 [==============================] - 1s 128us/step - loss: 8.5558 - val_loss: 106.0390\n",
      "Epoch 365/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 9.0153 - val_loss: 106.6768\n",
      "Epoch 366/1000\n",
      "5686/5686 [==============================] - 1s 119us/step - loss: 8.9158 - val_loss: 105.4436\n",
      "Epoch 367/1000\n",
      "5686/5686 [==============================] - 1s 119us/step - loss: 8.6101 - val_loss: 105.4512\n",
      "Epoch 368/1000\n",
      "5686/5686 [==============================] - 1s 118us/step - loss: 8.4291 - val_loss: 104.5114\n",
      "Epoch 369/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 7.9968 - val_loss: 104.0178\n",
      "Epoch 370/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5686/5686 [==============================] - 1s 115us/step - loss: 7.9046 - val_loss: 103.5159\n",
      "Epoch 371/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 7.7432 - val_loss: 103.1473\n",
      "Epoch 372/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 8.0138 - val_loss: 102.7447\n",
      "Epoch 373/1000\n",
      "5686/5686 [==============================] - 1s 120us/step - loss: 7.7538 - val_loss: 102.3101\n",
      "Epoch 374/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 8.2772 - val_loss: 101.6212\n",
      "Epoch 375/1000\n",
      "5686/5686 [==============================] - 1s 118us/step - loss: 8.1472 - val_loss: 102.9113\n",
      "Epoch 376/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 9.4835 - val_loss: 101.8125\n",
      "Epoch 377/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 10.7938 - val_loss: 120.6992\n",
      "Epoch 378/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 13.5336 - val_loss: 102.7206\n",
      "Epoch 379/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 9.9668 - val_loss: 103.0465\n",
      "Epoch 380/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 8.4107 - val_loss: 100.4925\n",
      "Epoch 381/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 7.9432 - val_loss: 99.9549\n",
      "Epoch 382/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 8.3348 - val_loss: 99.6124\n",
      "Epoch 383/1000\n",
      "5686/5686 [==============================] - 1s 118us/step - loss: 8.7069 - val_loss: 99.2654\n",
      "Epoch 384/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 8.2520 - val_loss: 100.1712\n",
      "Epoch 385/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 8.9723 - val_loss: 99.5034\n",
      "Epoch 386/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 9.8558 - val_loss: 102.8652\n",
      "Epoch 387/1000\n",
      "5686/5686 [==============================] - 1s 121us/step - loss: 11.3481 - val_loss: 99.2720\n",
      "Epoch 388/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 10.7103 - val_loss: 115.8169\n",
      "Epoch 389/1000\n",
      "5686/5686 [==============================] - 1s 119us/step - loss: 10.0225 - val_loss: 97.9294\n",
      "Epoch 390/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 8.1084 - val_loss: 97.9260\n",
      "Epoch 391/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 7.5295 - val_loss: 97.4192\n",
      "Epoch 392/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 7.6751 - val_loss: 97.0916\n",
      "Epoch 393/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 7.4939 - val_loss: 96.7040\n",
      "Epoch 394/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 7.1767 - val_loss: 96.6233\n",
      "Epoch 395/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.2821 - val_loss: 96.7285\n",
      "Epoch 396/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.3549 - val_loss: 96.3714\n",
      "Epoch 397/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 7.6014 - val_loss: 95.3096\n",
      "Epoch 398/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.2991 - val_loss: 95.1513\n",
      "Epoch 399/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 7.3777 - val_loss: 94.9917\n",
      "Epoch 400/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 8.2067 - val_loss: 98.0100\n",
      "Epoch 401/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 10.0834 - val_loss: 95.0976\n",
      "Epoch 402/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 10.1833 - val_loss: 95.4322\n",
      "Epoch 403/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 8.4988 - val_loss: 93.3207\n",
      "Epoch 404/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.8058 - val_loss: 92.9712\n",
      "Epoch 405/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.5315 - val_loss: 92.9092\n",
      "Epoch 406/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.5105 - val_loss: 92.5969\n",
      "Epoch 407/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.4460 - val_loss: 92.2996\n",
      "Epoch 408/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 7.3066 - val_loss: 91.7440\n",
      "Epoch 409/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.2939 - val_loss: 91.8086\n",
      "Epoch 410/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.3795 - val_loss: 91.2848\n",
      "Epoch 411/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.0915 - val_loss: 91.1516\n",
      "Epoch 412/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 7.5803 - val_loss: 91.1115\n",
      "Epoch 413/1000\n",
      "5686/5686 [==============================] - 1s 118us/step - loss: 7.8103 - val_loss: 90.7034\n",
      "Epoch 414/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.5478 - val_loss: 91.8481\n",
      "Epoch 415/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 7.8657 - val_loss: 90.6021\n",
      "Epoch 416/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 8.5570 - val_loss: 93.5229\n",
      "Epoch 417/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 10.0973 - val_loss: 90.8415\n",
      "Epoch 418/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 8.9529 - val_loss: 92.7230\n",
      "Epoch 419/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 9.0309 - val_loss: 90.1441\n",
      "Epoch 420/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.7142 - val_loss: 89.3860\n",
      "Epoch 421/1000\n",
      "5686/5686 [==============================] - 1s 119us/step - loss: 7.8984 - val_loss: 90.4017\n",
      "Epoch 422/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 7.4553 - val_loss: 90.2151\n",
      "Epoch 423/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 8.3687 - val_loss: 89.5097\n",
      "Epoch 424/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 9.0517 - val_loss: 90.4453\n",
      "Epoch 425/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.8352 - val_loss: 88.3449\n",
      "Epoch 426/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.5756 - val_loss: 87.8259\n",
      "Epoch 427/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.3585 - val_loss: 87.9529\n",
      "Epoch 428/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 6.8962 - val_loss: 88.0673\n",
      "Epoch 429/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 7.5359 - val_loss: 88.4975\n",
      "Epoch 430/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 8.0716 - val_loss: 88.0437\n",
      "Epoch 431/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 7.9088 - val_loss: 86.1161\n",
      "Epoch 432/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 6.9755 - val_loss: 86.1367\n",
      "Epoch 433/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 6.8775 - val_loss: 86.4647\n",
      "Epoch 434/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.1366 - val_loss: 86.0505\n",
      "Epoch 435/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 7.3222 - val_loss: 86.1582\n",
      "Epoch 436/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.7887 - val_loss: 87.2396\n",
      "Epoch 437/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 7.5485 - val_loss: 85.5503\n",
      "Epoch 438/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.0613 - val_loss: 86.0378\n",
      "Epoch 439/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 7.4028 - val_loss: 85.8995\n",
      "Epoch 440/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 7.8161 - val_loss: 85.1658\n",
      "Epoch 441/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 8.0425 - val_loss: 84.4012\n",
      "Epoch 442/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.5580 - val_loss: 84.2595\n",
      "Epoch 443/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.8049 - val_loss: 84.0159\n",
      "Epoch 444/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 6.7163 - val_loss: 83.2979\n",
      "Epoch 445/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.4653 - val_loss: 83.1074\n",
      "Epoch 446/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.8022 - val_loss: 83.1736\n",
      "Epoch 447/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 6.9447 - val_loss: 84.1852\n",
      "Epoch 448/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 7.4571 - val_loss: 84.8947\n",
      "Epoch 449/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 7.7651 - val_loss: 84.1836\n",
      "Epoch 450/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 7.6465 - val_loss: 83.3938\n",
      "Epoch 451/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 7.5218 - val_loss: 84.3525\n",
      "Epoch 452/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.9707 - val_loss: 87.6356\n",
      "Epoch 453/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 8.4406 - val_loss: 85.9879\n",
      "Epoch 454/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 9.0814 - val_loss: 88.4108\n",
      "Epoch 455/1000\n",
      "5686/5686 [==============================] - 1s 120us/step - loss: 10.9658 - val_loss: 85.5879\n",
      "Epoch 456/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 9.9364 - val_loss: 100.6013\n",
      "Epoch 457/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 8.6836 - val_loss: 92.7381\n",
      "Epoch 458/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 7.5678 - val_loss: 84.3480\n",
      "Epoch 459/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 6.9460 - val_loss: 82.8978\n",
      "Epoch 460/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.1073 - val_loss: 82.2034\n",
      "Epoch 461/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.7782 - val_loss: 83.1435\n",
      "Epoch 462/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 7.5290 - val_loss: 84.8929\n",
      "Epoch 463/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 8.3373 - val_loss: 82.5188\n",
      "Epoch 464/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 7.9165 - val_loss: 81.8194\n",
      "Epoch 465/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.6533 - val_loss: 81.1697\n",
      "Epoch 466/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.9144 - val_loss: 82.0167\n",
      "Epoch 467/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 6.3259 - val_loss: 80.4096\n",
      "Epoch 468/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 6.3629 - val_loss: 81.8888\n",
      "Epoch 469/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.3721 - val_loss: 80.2958\n",
      "Epoch 470/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 6.9944 - val_loss: 81.8314\n",
      "Epoch 471/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 6.6639 - val_loss: 78.6694\n",
      "Epoch 472/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.4842 - val_loss: 79.6001\n",
      "Epoch 473/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.1357 - val_loss: 80.5322\n",
      "Epoch 474/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.8559 - val_loss: 79.5328\n",
      "Epoch 475/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.0975 - val_loss: 79.9259\n",
      "Epoch 476/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.0343 - val_loss: 81.3163\n",
      "Epoch 477/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 7.6912 - val_loss: 80.1423\n",
      "Epoch 478/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.6257 - val_loss: 79.3273\n",
      "Epoch 479/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 6.8781 - val_loss: 78.1258\n",
      "Epoch 480/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.2978 - val_loss: 79.5968\n",
      "Epoch 481/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.7348 - val_loss: 78.2449\n",
      "Epoch 482/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 6.5539 - val_loss: 79.3226\n",
      "Epoch 483/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.1676 - val_loss: 80.5825\n",
      "Epoch 484/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.0862 - val_loss: 79.6049\n",
      "Epoch 485/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 7.0351 - val_loss: 77.5010\n",
      "Epoch 486/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.8306 - val_loss: 79.2101\n",
      "Epoch 487/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 6.7366 - val_loss: 78.7187\n",
      "Epoch 488/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 6.8201 - val_loss: 77.4055\n",
      "Epoch 489/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.6891 - val_loss: 77.1495\n",
      "Epoch 490/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.7346 - val_loss: 77.6348\n",
      "Epoch 491/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.6537 - val_loss: 78.1341\n",
      "Epoch 492/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.9574 - val_loss: 78.0230\n",
      "Epoch 493/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.2464 - val_loss: 77.4625\n",
      "Epoch 494/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 7.1587 - val_loss: 77.4627\n",
      "Epoch 495/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 6.8084 - val_loss: 77.2606\n",
      "Epoch 496/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 6.5365 - val_loss: 76.7406\n",
      "Epoch 497/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 6.5215 - val_loss: 77.4053\n",
      "Epoch 498/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 6.5963 - val_loss: 76.0817\n",
      "Epoch 499/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 6.2390 - val_loss: 76.0034\n",
      "Epoch 500/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 6.2985 - val_loss: 75.7255\n",
      "Epoch 501/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 6.1511 - val_loss: 75.6796\n",
      "Epoch 502/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 6.3927 - val_loss: 75.9789\n",
      "Epoch 503/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 6.2593 - val_loss: 75.9612\n",
      "Epoch 504/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 6.6928 - val_loss: 77.1613\n",
      "Epoch 505/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.1000 - val_loss: 76.8406\n",
      "Epoch 506/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 7.5579 - val_loss: 77.6255\n",
      "Epoch 507/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 7.8802 - val_loss: 77.5942\n",
      "Epoch 508/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 7.0683 - val_loss: 75.8649\n",
      "Epoch 509/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.2565 - val_loss: 76.0779\n",
      "Epoch 510/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 6.7504 - val_loss: 75.5866\n",
      "Epoch 511/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 6.7638 - val_loss: 75.3444\n",
      "Epoch 512/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.9724 - val_loss: 77.8230\n",
      "Epoch 513/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 7.3204 - val_loss: 78.6274\n",
      "Epoch 514/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 7.6528 - val_loss: 77.2881\n",
      "Epoch 515/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 8.0173 - val_loss: 76.0769\n",
      "Epoch 516/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 7.5286 - val_loss: 76.8898\n",
      "Epoch 517/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 7.1271 - val_loss: 77.3229\n",
      "Epoch 518/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.9740 - val_loss: 82.6044\n",
      "Epoch 519/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 7.1250 - val_loss: 74.8455\n",
      "Epoch 520/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.9049 - val_loss: 75.0351\n",
      "Epoch 521/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.3787 - val_loss: 73.5088\n",
      "Epoch 522/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.9471 - val_loss: 74.2077\n",
      "Epoch 523/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.9517 - val_loss: 73.6791\n",
      "Epoch 524/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.2660 - val_loss: 74.5703\n",
      "Epoch 525/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 6.0605 - val_loss: 75.1599\n",
      "Epoch 526/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 6.2009 - val_loss: 74.3788\n",
      "Epoch 527/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 6.7315 - val_loss: 73.1143\n",
      "Epoch 528/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 6.6594 - val_loss: 73.8005\n",
      "Epoch 529/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.5395 - val_loss: 72.8772\n",
      "Epoch 530/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.5272 - val_loss: 73.4524\n",
      "Epoch 531/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.0974 - val_loss: 73.3290\n",
      "Epoch 532/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.2824 - val_loss: 74.4489\n",
      "Epoch 533/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 7.0209 - val_loss: 73.7252\n",
      "Epoch 534/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.7461 - val_loss: 74.0411\n",
      "Epoch 535/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.4123 - val_loss: 73.5487\n",
      "Epoch 536/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.1657 - val_loss: 72.4734\n",
      "Epoch 537/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.9620 - val_loss: 72.2819\n",
      "Epoch 538/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.1096 - val_loss: 72.4491\n",
      "Epoch 539/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 5.9335 - val_loss: 72.2233\n",
      "Epoch 540/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.7963 - val_loss: 73.1078\n",
      "Epoch 541/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.7397 - val_loss: 73.5039\n",
      "Epoch 542/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.8174 - val_loss: 73.0342\n",
      "Epoch 543/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 5.9704 - val_loss: 73.2703\n",
      "Epoch 544/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.7314 - val_loss: 72.9616\n",
      "Epoch 545/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.6324 - val_loss: 73.9615\n",
      "Epoch 546/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 5.8564 - val_loss: 76.5532\n",
      "Epoch 547/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.7513 - val_loss: 81.5698\n",
      "Epoch 548/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 6.4161 - val_loss: 73.1700\n",
      "Epoch 549/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.6661 - val_loss: 73.7093\n",
      "Epoch 550/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.9633 - val_loss: 78.0829\n",
      "Epoch 551/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 8.2187 - val_loss: 104.5339\n",
      "Epoch 552/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 14.6326 - val_loss: 86.0210\n",
      "Epoch 553/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 10.8404 - val_loss: 100.0409\n",
      "Epoch 554/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 12.7700 - val_loss: 87.1214\n",
      "Epoch 555/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 9.1675 - val_loss: 80.1483\n",
      "Epoch 556/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 6.6418 - val_loss: 79.3145\n",
      "Epoch 557/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 6.8042 - val_loss: 79.1847\n",
      "Epoch 558/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.5089 - val_loss: 78.3841\n",
      "Epoch 559/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 6.1907 - val_loss: 78.5724\n",
      "Epoch 560/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.7040 - val_loss: 78.6038\n",
      "Epoch 561/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.8511 - val_loss: 78.0921\n",
      "Epoch 562/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.1704 - val_loss: 78.0328\n",
      "Epoch 563/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 6.3408 - val_loss: 77.3050\n",
      "Epoch 564/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 5.9513 - val_loss: 77.2852\n",
      "Epoch 565/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.0462 - val_loss: 76.8162\n",
      "Epoch 566/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.1563 - val_loss: 76.5511\n",
      "Epoch 567/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.8912 - val_loss: 76.8007\n",
      "Epoch 568/1000\n",
      "5686/5686 [==============================] - 1s 135us/step - loss: 5.8506 - val_loss: 76.5229\n",
      "Epoch 569/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 5.6401 - val_loss: 76.3542\n",
      "Epoch 570/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 5.9220 - val_loss: 75.9135\n",
      "Epoch 571/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 5.8333 - val_loss: 75.9093\n",
      "Epoch 572/1000\n",
      "5686/5686 [==============================] - 1s 121us/step - loss: 6.2433 - val_loss: 76.2712\n",
      "Epoch 573/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 6.3610 - val_loss: 76.5570\n",
      "Epoch 574/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 6.7876 - val_loss: 75.8465\n",
      "Epoch 575/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.7889 - val_loss: 82.4738\n",
      "Epoch 576/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 6.5318 - val_loss: 74.7416\n",
      "Epoch 577/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 6.2548 - val_loss: 75.3951\n",
      "Epoch 578/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 5.9844 - val_loss: 74.5210\n",
      "Epoch 579/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 5.6331 - val_loss: 74.0490\n",
      "Epoch 580/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.9387 - val_loss: 74.1793\n",
      "Epoch 581/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.7435 - val_loss: 73.2597\n",
      "Epoch 582/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.0870 - val_loss: 72.9716\n",
      "Epoch 583/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.9843 - val_loss: 72.1634\n",
      "Epoch 584/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.9060 - val_loss: 72.0377\n",
      "Epoch 585/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.7513 - val_loss: 71.4183\n",
      "Epoch 586/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.3844 - val_loss: 72.2137\n",
      "Epoch 587/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 6.5126 - val_loss: 71.6543\n",
      "Epoch 588/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.6733 - val_loss: 73.3534\n",
      "Epoch 589/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.6710 - val_loss: 70.9468\n",
      "Epoch 590/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.0835 - val_loss: 71.1139\n",
      "Epoch 591/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 6.1807 - val_loss: 71.3579\n",
      "Epoch 592/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 6.6025 - val_loss: 70.4626\n",
      "Epoch 593/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.9435 - val_loss: 70.2628\n",
      "Epoch 594/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.6518 - val_loss: 70.0175\n",
      "Epoch 595/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.5863 - val_loss: 70.2396\n",
      "Epoch 596/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.7670 - val_loss: 70.8355\n",
      "Epoch 597/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 6.6938 - val_loss: 72.3146\n",
      "Epoch 598/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.7599 - val_loss: 71.8529\n",
      "Epoch 599/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 6.5437 - val_loss: 73.6563\n",
      "Epoch 600/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 6.5124 - val_loss: 72.3801\n",
      "Epoch 601/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 6.9533 - val_loss: 72.3573\n",
      "Epoch 602/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.6930 - val_loss: 74.5669\n",
      "Epoch 603/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 7.2084 - val_loss: 72.7343\n",
      "Epoch 604/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.2703 - val_loss: 74.5375\n",
      "Epoch 605/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 6.8571 - val_loss: 72.4027\n",
      "Epoch 606/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.7781 - val_loss: 79.4282\n",
      "Epoch 607/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 8.4464 - val_loss: 75.2144\n",
      "Epoch 608/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 7.3187 - val_loss: 75.8278\n",
      "Epoch 609/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.3481 - val_loss: 70.2455\n",
      "Epoch 610/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.6251 - val_loss: 70.1575\n",
      "Epoch 611/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 5.6329 - val_loss: 69.8560\n",
      "Epoch 612/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.6259 - val_loss: 69.7367\n",
      "Epoch 613/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.6744 - val_loss: 70.2111\n",
      "Epoch 614/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.4888 - val_loss: 69.9919\n",
      "Epoch 615/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.4028 - val_loss: 69.8169\n",
      "Epoch 616/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.6465 - val_loss: 69.8485\n",
      "Epoch 617/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.4357 - val_loss: 70.1066\n",
      "Epoch 618/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 5.4337 - val_loss: 69.6644\n",
      "Epoch 619/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 5.3999 - val_loss: 70.8611\n",
      "Epoch 620/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.6589 - val_loss: 73.5256\n",
      "Epoch 621/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.4074 - val_loss: 75.2583\n",
      "Epoch 622/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 5.5952 - val_loss: 73.3030\n",
      "Epoch 623/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.5834 - val_loss: 71.9293\n",
      "Epoch 624/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.9073 - val_loss: 74.0110\n",
      "Epoch 625/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.3624 - val_loss: 74.3468\n",
      "Epoch 626/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 6.1933 - val_loss: 69.8220\n",
      "Epoch 627/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 6.1361 - val_loss: 70.5130\n",
      "Epoch 628/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 6.0986 - val_loss: 69.0601\n",
      "Epoch 629/1000\n",
      "5686/5686 [==============================] - 1s 118us/step - loss: 6.0499 - val_loss: 69.4145\n",
      "Epoch 630/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 5.8953 - val_loss: 68.9039\n",
      "Epoch 631/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 5.6539 - val_loss: 71.2886\n",
      "Epoch 632/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 5.5356 - val_loss: 68.7797\n",
      "Epoch 633/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.5460 - val_loss: 70.7309\n",
      "Epoch 634/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.4688 - val_loss: 69.1037\n",
      "Epoch 635/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.6882 - val_loss: 68.7352\n",
      "Epoch 636/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.7812 - val_loss: 69.9577\n",
      "Epoch 637/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 7.0043 - val_loss: 70.1017\n",
      "Epoch 638/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.8134 - val_loss: 71.9474\n",
      "Epoch 639/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.4084 - val_loss: 73.7869\n",
      "Epoch 640/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.4474 - val_loss: 81.0464\n",
      "Epoch 641/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 7.7361 - val_loss: 74.8520\n",
      "Epoch 642/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.2777 - val_loss: 74.2788\n",
      "Epoch 643/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.9855 - val_loss: 72.7379\n",
      "Epoch 644/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.1957 - val_loss: 69.8420\n",
      "Epoch 645/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.4919 - val_loss: 69.0469\n",
      "Epoch 646/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.0404 - val_loss: 68.9231\n",
      "Epoch 647/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.6052 - val_loss: 69.5164\n",
      "Epoch 648/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.7227 - val_loss: 68.8641\n",
      "Epoch 649/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.5449 - val_loss: 68.9023\n",
      "Epoch 650/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 5.4591 - val_loss: 68.5310\n",
      "Epoch 651/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 5.5333 - val_loss: 68.7189\n",
      "Epoch 652/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 5.5810 - val_loss: 68.8304\n",
      "Epoch 653/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.4027 - val_loss: 69.4582\n",
      "Epoch 654/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 5.6213 - val_loss: 68.1722\n",
      "Epoch 655/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 5.3257 - val_loss: 68.4604\n",
      "Epoch 656/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.5721 - val_loss: 68.3509\n",
      "Epoch 657/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.3998 - val_loss: 68.3464\n",
      "Epoch 658/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.3725 - val_loss: 68.2848\n",
      "Epoch 659/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 5.5689 - val_loss: 70.8162\n",
      "Epoch 660/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.4859 - val_loss: 68.8028\n",
      "Epoch 661/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.2785 - val_loss: 70.5364\n",
      "Epoch 662/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.1629 - val_loss: 72.2336\n",
      "Epoch 663/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 7.1951 - val_loss: 69.4473\n",
      "Epoch 664/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 6.0627 - val_loss: 69.2297\n",
      "Epoch 665/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 5.6405 - val_loss: 69.0222\n",
      "Epoch 666/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 5.4632 - val_loss: 68.3617\n",
      "Epoch 667/1000\n",
      "5686/5686 [==============================] - 1s 123us/step - loss: 5.4205 - val_loss: 68.2730\n",
      "Epoch 668/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.3460 - val_loss: 68.5627\n",
      "Epoch 669/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.5254 - val_loss: 68.4556\n",
      "Epoch 670/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.2073 - val_loss: 69.9755\n",
      "Epoch 671/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.4031 - val_loss: 68.3180\n",
      "Epoch 672/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 5.5627 - val_loss: 70.3482\n",
      "Epoch 673/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.9684 - val_loss: 69.8072\n",
      "Epoch 674/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.6677 - val_loss: 68.7892\n",
      "Epoch 675/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.8308 - val_loss: 74.0894\n",
      "Epoch 676/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 6.3029 - val_loss: 72.9795\n",
      "Epoch 677/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.0110 - val_loss: 68.9967\n",
      "Epoch 678/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 5.4120 - val_loss: 69.8153\n",
      "Epoch 679/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.6481 - val_loss: 79.4699\n",
      "Epoch 680/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.7054 - val_loss: 70.5542\n",
      "Epoch 681/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 5.6861 - val_loss: 75.6681\n",
      "Epoch 682/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.5983 - val_loss: 68.0727\n",
      "Epoch 683/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.4540 - val_loss: 67.4722\n",
      "Epoch 684/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.0947 - val_loss: 67.5250\n",
      "Epoch 685/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.0840 - val_loss: 67.4663\n",
      "Epoch 686/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.2182 - val_loss: 68.1283\n",
      "Epoch 687/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.1677 - val_loss: 67.7467\n",
      "Epoch 688/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.4055 - val_loss: 68.3529\n",
      "Epoch 689/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.6518 - val_loss: 67.9715\n",
      "Epoch 690/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.6296 - val_loss: 67.7904\n",
      "Epoch 691/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.5301 - val_loss: 68.3314\n",
      "Epoch 692/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.8596 - val_loss: 67.7721\n",
      "Epoch 693/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.6208 - val_loss: 70.9019\n",
      "Epoch 694/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 6.1053 - val_loss: 71.4377\n",
      "Epoch 695/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 6.1183 - val_loss: 71.4674\n",
      "Epoch 696/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 6.8126 - val_loss: 79.2662\n",
      "Epoch 697/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 8.0859 - val_loss: 80.9872\n",
      "Epoch 698/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 10.4337 - val_loss: 73.4012\n",
      "Epoch 699/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 8.6991 - val_loss: 72.0289\n",
      "Epoch 700/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 7.4794 - val_loss: 68.9899\n",
      "Epoch 701/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 6.0134 - val_loss: 69.6450\n",
      "Epoch 702/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 5.7480 - val_loss: 69.6775\n",
      "Epoch 703/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 5.5352 - val_loss: 68.2170\n",
      "Epoch 704/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.2906 - val_loss: 68.2276\n",
      "Epoch 705/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 5.2796 - val_loss: 68.5841\n",
      "Epoch 706/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.5380 - val_loss: 68.4314\n",
      "Epoch 707/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.4034 - val_loss: 68.6861\n",
      "Epoch 708/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.4046 - val_loss: 68.9539\n",
      "Epoch 709/1000\n",
      "5686/5686 [==============================] - 1s 107us/step - loss: 5.1165 - val_loss: 68.0949\n",
      "Epoch 710/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.1499 - val_loss: 68.6612\n",
      "Epoch 711/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.5889 - val_loss: 68.2196\n",
      "Epoch 712/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.0739 - val_loss: 68.0752\n",
      "Epoch 713/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 5.0640 - val_loss: 67.7158\n",
      "Epoch 714/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.0525 - val_loss: 68.2507\n",
      "Epoch 715/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.3168 - val_loss: 68.1272\n",
      "Epoch 716/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 5.2561 - val_loss: 69.0513\n",
      "Epoch 717/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.3193 - val_loss: 69.4294\n",
      "Epoch 718/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.2659 - val_loss: 67.6477\n",
      "Epoch 719/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.2015 - val_loss: 68.2309\n",
      "Epoch 720/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.6421 - val_loss: 67.1884\n",
      "Epoch 721/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.6189 - val_loss: 70.5296\n",
      "Epoch 722/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.7615 - val_loss: 70.6664\n",
      "Epoch 723/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 6.2004 - val_loss: 68.2605\n",
      "Epoch 724/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 6.2965 - val_loss: 69.1522\n",
      "Epoch 725/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.8072 - val_loss: 73.0460\n",
      "Epoch 726/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 5.7964 - val_loss: 76.9380\n",
      "Epoch 727/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.9968 - val_loss: 75.7119\n",
      "Epoch 728/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.6655 - val_loss: 68.1795\n",
      "Epoch 729/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.5553 - val_loss: 69.5437\n",
      "Epoch 730/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.5848 - val_loss: 70.1860\n",
      "Epoch 731/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.2986 - val_loss: 69.4208\n",
      "Epoch 732/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 5.3124 - val_loss: 70.2123\n",
      "Epoch 733/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.2226 - val_loss: 68.9394\n",
      "Epoch 734/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 4.9962 - val_loss: 69.4948\n",
      "Epoch 735/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 5.0685 - val_loss: 69.4854\n",
      "Epoch 736/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.2970 - val_loss: 69.8181\n",
      "Epoch 737/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.7951 - val_loss: 69.1268\n",
      "Epoch 738/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.7067 - val_loss: 69.4467\n",
      "Epoch 739/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.8123 - val_loss: 71.7267\n",
      "Epoch 740/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 6.8884 - val_loss: 76.9697\n",
      "Epoch 741/1000\n",
      "5686/5686 [==============================] - 1s 137us/step - loss: 8.9339 - val_loss: 76.0222\n",
      "Epoch 742/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 8.5772 - val_loss: 76.7450\n",
      "Epoch 743/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 7.5727 - val_loss: 74.1367\n",
      "Epoch 744/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 6.4370 - val_loss: 69.8589\n",
      "Epoch 745/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5686/5686 [==============================] - 1s 107us/step - loss: 5.3486 - val_loss: 69.4244\n",
      "Epoch 746/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.3396 - val_loss: 68.2776\n",
      "Epoch 747/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.5248 - val_loss: 69.2765\n",
      "Epoch 748/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.6281 - val_loss: 69.3126\n",
      "Epoch 749/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.2316 - val_loss: 69.2260\n",
      "Epoch 750/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 5.3100 - val_loss: 67.8131\n",
      "Epoch 751/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.2204 - val_loss: 67.9641\n",
      "Epoch 752/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.2193 - val_loss: 68.2060\n",
      "Epoch 753/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.2283 - val_loss: 68.2858\n",
      "Epoch 754/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.0179 - val_loss: 68.7556\n",
      "Epoch 755/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 5.0617 - val_loss: 68.9944\n",
      "Epoch 756/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 4.8987 - val_loss: 69.9291\n",
      "Epoch 757/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.1461 - val_loss: 70.0953\n",
      "Epoch 758/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.3473 - val_loss: 69.3681\n",
      "Epoch 759/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 5.5862 - val_loss: 71.1274\n",
      "Epoch 760/1000\n",
      "5686/5686 [==============================] - 1s 107us/step - loss: 5.6689 - val_loss: 71.6855\n",
      "Epoch 761/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 6.4804 - val_loss: 72.9206\n",
      "Epoch 762/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 7.1367 - val_loss: 77.5519\n",
      "Epoch 763/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 7.1218 - val_loss: 71.8113\n",
      "Epoch 764/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 6.3952 - val_loss: 72.5431\n",
      "Epoch 765/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 6.0893 - val_loss: 71.0163\n",
      "Epoch 766/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.6803 - val_loss: 68.9324\n",
      "Epoch 767/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.3069 - val_loss: 69.1740\n",
      "Epoch 768/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 5.3074 - val_loss: 70.0504\n",
      "Epoch 769/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.3266 - val_loss: 68.5487\n",
      "Epoch 770/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.1883 - val_loss: 69.0887\n",
      "Epoch 771/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 4.8442 - val_loss: 69.4290\n",
      "Epoch 772/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.0444 - val_loss: 69.5237\n",
      "Epoch 773/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.0273 - val_loss: 70.3438\n",
      "Epoch 774/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.6149 - val_loss: 68.9367\n",
      "Epoch 775/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.3271 - val_loss: 68.7450\n",
      "Epoch 776/1000\n",
      "5686/5686 [==============================] - 1s 122us/step - loss: 5.0871 - val_loss: 67.8896\n",
      "Epoch 777/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.2615 - val_loss: 67.9819\n",
      "Epoch 778/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 4.8297 - val_loss: 67.9920\n",
      "Epoch 779/1000\n",
      "5686/5686 [==============================] - 1s 107us/step - loss: 5.2072 - val_loss: 67.1050\n",
      "Epoch 780/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.1126 - val_loss: 67.3566\n",
      "Epoch 781/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.1835 - val_loss: 67.1746\n",
      "Epoch 782/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 5.1348 - val_loss: 68.2236\n",
      "Epoch 783/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 4.9760 - val_loss: 67.1875\n",
      "Epoch 784/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 5.1304 - val_loss: 67.9522\n",
      "Epoch 785/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.2943 - val_loss: 68.4080\n",
      "Epoch 786/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.6962 - val_loss: 69.7030\n",
      "Epoch 787/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.8646 - val_loss: 72.7489\n",
      "Epoch 788/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 6.0487 - val_loss: 69.7441\n",
      "Epoch 789/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 6.3147 - val_loss: 70.4430\n",
      "Epoch 790/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 5.8921 - val_loss: 68.3943\n",
      "Epoch 791/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.4613 - val_loss: 69.0488\n",
      "Epoch 792/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.9378 - val_loss: 73.4955\n",
      "Epoch 793/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 6.8644 - val_loss: 70.2630\n",
      "Epoch 794/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.9206 - val_loss: 70.7960\n",
      "Epoch 795/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 5.1953 - val_loss: 69.9590\n",
      "Epoch 796/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 5.2905 - val_loss: 68.7642\n",
      "Epoch 797/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.1288 - val_loss: 69.7492\n",
      "Epoch 798/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 4.8290 - val_loss: 69.9667\n",
      "Epoch 799/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 5.0779 - val_loss: 69.6555\n",
      "Epoch 800/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 4.9370 - val_loss: 69.8420\n",
      "Epoch 801/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 5.0324 - val_loss: 69.2953\n",
      "Epoch 802/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 5.1507 - val_loss: 77.0077\n",
      "Epoch 803/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.5527 - val_loss: 70.5070\n",
      "Epoch 804/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.8778 - val_loss: 73.1141\n",
      "Epoch 805/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 6.2995 - val_loss: 75.8223\n",
      "Epoch 806/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 6.9661 - val_loss: 80.9787\n",
      "Epoch 807/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 7.4304 - val_loss: 82.1594\n",
      "Epoch 808/1000\n",
      "5686/5686 [==============================] - 1s 122us/step - loss: 7.0178 - val_loss: 80.2642\n",
      "Epoch 809/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 6.4806 - val_loss: 71.1146\n",
      "Epoch 810/1000\n",
      "5686/5686 [==============================] - 6s 1ms/step - loss: 5.6918 - val_loss: 71.4654\n",
      "Epoch 811/1000\n",
      "5686/5686 [==============================] - 1s 181us/step - loss: 5.4631 - val_loss: 70.4355\n",
      "Epoch 812/1000\n",
      "5686/5686 [==============================] - 1s 133us/step - loss: 5.0855 - val_loss: 72.1325\n",
      "Epoch 813/1000\n",
      "5686/5686 [==============================] - 1s 141us/step - loss: 5.3071 - val_loss: 71.3742\n",
      "Epoch 814/1000\n",
      "5686/5686 [==============================] - 1s 135us/step - loss: 4.8431 - val_loss: 70.5685\n",
      "Epoch 815/1000\n",
      "5686/5686 [==============================] - 1s 132us/step - loss: 4.8074 - val_loss: 71.1400\n",
      "Epoch 816/1000\n",
      "5686/5686 [==============================] - 1s 137us/step - loss: 4.9919 - val_loss: 70.3249\n",
      "Epoch 817/1000\n",
      "5686/5686 [==============================] - 1s 148us/step - loss: 4.8490 - val_loss: 69.9314\n",
      "Epoch 818/1000\n",
      "5686/5686 [==============================] - 1s 139us/step - loss: 4.6566 - val_loss: 70.4834\n",
      "Epoch 819/1000\n",
      "5686/5686 [==============================] - 1s 118us/step - loss: 4.9042 - val_loss: 70.3598\n",
      "Epoch 820/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 5.1095 - val_loss: 71.1796\n",
      "Epoch 821/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 5.4711 - val_loss: 71.4474\n",
      "Epoch 822/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 5.2578 - val_loss: 72.4075\n",
      "Epoch 823/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 4.8785 - val_loss: 71.9057\n",
      "Epoch 824/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 5.0953 - val_loss: 69.0895\n",
      "Epoch 825/1000\n",
      "5686/5686 [==============================] - 1s 123us/step - loss: 5.1230 - val_loss: 69.6841\n",
      "Epoch 826/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 4.7448 - val_loss: 69.2812\n",
      "Epoch 827/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.0055 - val_loss: 69.1260\n",
      "Epoch 828/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 4.9522 - val_loss: 68.5583\n",
      "Epoch 829/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 5.1113 - val_loss: 68.9534\n",
      "Epoch 830/1000\n",
      "5686/5686 [==============================] - 1s 120us/step - loss: 5.2905 - val_loss: 69.2136\n",
      "Epoch 831/1000\n",
      "5686/5686 [==============================] - 1s 118us/step - loss: 4.8130 - val_loss: 69.2546\n",
      "Epoch 832/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 5.0328 - val_loss: 69.3449\n",
      "Epoch 833/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 4.5331 - val_loss: 69.8554\n",
      "Epoch 834/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 4.8404 - val_loss: 69.2550\n",
      "Epoch 835/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.1014 - val_loss: 70.0403\n",
      "Epoch 836/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 5.0870 - val_loss: 70.4528\n",
      "Epoch 837/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 5.2154 - val_loss: 70.3157\n",
      "Epoch 838/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 4.8950 - val_loss: 73.6556\n",
      "Epoch 839/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.1292 - val_loss: 80.4742\n",
      "Epoch 840/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 4.7845 - val_loss: 71.4252\n",
      "Epoch 841/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 4.8024 - val_loss: 72.1034\n",
      "Epoch 842/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 5.4868 - val_loss: 69.3887\n",
      "Epoch 843/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 6.3330 - val_loss: 72.8832\n",
      "Epoch 844/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 6.1642 - val_loss: 73.5839\n",
      "Epoch 845/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 7.0833 - val_loss: 73.1610\n",
      "Epoch 846/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 7.0349 - val_loss: 70.6282\n",
      "Epoch 847/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 6.1305 - val_loss: 73.3993\n",
      "Epoch 848/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 5.2850 - val_loss: 76.0264\n",
      "Epoch 849/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 4.7803 - val_loss: 96.9119\n",
      "Epoch 850/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 4.9505 - val_loss: 79.9293\n",
      "Epoch 851/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 5.0514 - val_loss: 70.3727\n",
      "Epoch 852/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 4.9615 - val_loss: 73.2974\n",
      "Epoch 853/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.5224 - val_loss: 64.9986\n",
      "Epoch 854/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.4904 - val_loss: 66.2604\n",
      "Epoch 855/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 5.0726 - val_loss: 66.3796\n",
      "Epoch 856/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 4.9990 - val_loss: 66.6103\n",
      "Epoch 857/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.0343 - val_loss: 68.1886\n",
      "Epoch 858/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 4.9965 - val_loss: 71.2996\n",
      "Epoch 859/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 6.1109 - val_loss: 73.4366\n",
      "Epoch 860/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.6518 - val_loss: 68.2589\n",
      "Epoch 861/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.4465 - val_loss: 67.3914\n",
      "Epoch 862/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 4.9818 - val_loss: 68.0827\n",
      "Epoch 863/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 4.9108 - val_loss: 66.4657\n",
      "Epoch 864/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.0971 - val_loss: 66.7448\n",
      "Epoch 865/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 4.6662 - val_loss: 67.2896\n",
      "Epoch 866/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 5.0297 - val_loss: 69.2530\n",
      "Epoch 867/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.0621 - val_loss: 70.6336\n",
      "Epoch 868/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.6672 - val_loss: 70.1196\n",
      "Epoch 869/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.9336 - val_loss: 72.4105\n",
      "Epoch 870/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 5.6290 - val_loss: 70.3619\n",
      "Epoch 871/1000\n",
      "5686/5686 [==============================] - 1s 112us/step - loss: 5.6276 - val_loss: 67.4368\n",
      "Epoch 872/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 5.0582 - val_loss: 67.9245\n",
      "Epoch 873/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 5.4716 - val_loss: 67.0606\n",
      "Epoch 874/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 4.8795 - val_loss: 68.1266\n",
      "Epoch 875/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 5.0261 - val_loss: 67.1519\n",
      "Epoch 876/1000\n",
      "5686/5686 [==============================] - 1s 127us/step - loss: 4.9050 - val_loss: 67.0252\n",
      "Epoch 877/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 5.0556 - val_loss: 67.7187\n",
      "Epoch 878/1000\n",
      "5686/5686 [==============================] - 1s 118us/step - loss: 5.5428 - val_loss: 68.3463\n",
      "Epoch 879/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 4.8349 - val_loss: 68.3855\n",
      "Epoch 880/1000\n",
      "5686/5686 [==============================] - 1s 121us/step - loss: 4.8834 - val_loss: 68.8001\n",
      "Epoch 881/1000\n",
      "5686/5686 [==============================] - 1s 137us/step - loss: 4.9822 - val_loss: 68.8743\n",
      "Epoch 882/1000\n",
      "5686/5686 [==============================] - 1s 125us/step - loss: 4.7875 - val_loss: 68.0342\n",
      "Epoch 883/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 4.8288 - val_loss: 67.7374\n",
      "Epoch 884/1000\n",
      "5686/5686 [==============================] - 1s 118us/step - loss: 4.9183 - val_loss: 73.4573\n",
      "Epoch 885/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 5.0459 - val_loss: 67.9038\n",
      "Epoch 886/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 5.0886 - val_loss: 66.9410\n",
      "Epoch 887/1000\n",
      "5686/5686 [==============================] - 1s 118us/step - loss: 5.2769 - val_loss: 67.1352\n",
      "Epoch 888/1000\n",
      "5686/5686 [==============================] - 1s 118us/step - loss: 5.2765 - val_loss: 67.9765\n",
      "Epoch 889/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 5.8278 - val_loss: 70.0821\n",
      "Epoch 890/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 6.7445 - val_loss: 77.0725\n",
      "Epoch 891/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 7.4452 - val_loss: 76.6970\n",
      "Epoch 892/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 7.0375 - val_loss: 79.2412\n",
      "Epoch 893/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 6.2227 - val_loss: 71.3795\n",
      "Epoch 894/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 5.9382 - val_loss: 70.8224\n",
      "Epoch 895/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5686/5686 [==============================] - 1s 116us/step - loss: 5.4349 - val_loss: 70.0805\n",
      "Epoch 896/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 5.2091 - val_loss: 70.0603\n",
      "Epoch 897/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 4.8331 - val_loss: 69.4254\n",
      "Epoch 898/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 4.9111 - val_loss: 69.0261\n",
      "Epoch 899/1000\n",
      "5686/5686 [==============================] - 1s 119us/step - loss: 4.9031 - val_loss: 70.6590\n",
      "Epoch 900/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 4.9406 - val_loss: 70.3010\n",
      "Epoch 901/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 4.9470 - val_loss: 69.2417\n",
      "Epoch 902/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 4.7542 - val_loss: 68.2718\n",
      "Epoch 903/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 4.8993 - val_loss: 68.8385\n",
      "Epoch 904/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 4.8109 - val_loss: 68.4349\n",
      "Epoch 905/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 4.6947 - val_loss: 69.0721\n",
      "Epoch 906/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 4.8951 - val_loss: 68.7602\n",
      "Epoch 907/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 4.8469 - val_loss: 70.0431\n",
      "Epoch 908/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 5.0663 - val_loss: 70.8071\n",
      "Epoch 909/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 5.3880 - val_loss: 69.4495\n",
      "Epoch 910/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 5.1782 - val_loss: 70.4821\n",
      "Epoch 911/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 4.7358 - val_loss: 70.8697\n",
      "Epoch 912/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 4.7616 - val_loss: 70.9519\n",
      "Epoch 913/1000\n",
      "5686/5686 [==============================] - 1s 124us/step - loss: 4.6235 - val_loss: 70.7053\n",
      "Epoch 914/1000\n",
      "5686/5686 [==============================] - 1s 123us/step - loss: 4.6189 - val_loss: 69.9374\n",
      "Epoch 915/1000\n",
      "5686/5686 [==============================] - 1s 119us/step - loss: 4.6179 - val_loss: 70.7044\n",
      "Epoch 916/1000\n",
      "5686/5686 [==============================] - 1s 134us/step - loss: 4.7842 - val_loss: 71.5656\n",
      "Epoch 917/1000\n",
      "5686/5686 [==============================] - 1s 124us/step - loss: 4.7689 - val_loss: 69.1025\n",
      "Epoch 918/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 4.7544 - val_loss: 69.9415\n",
      "Epoch 919/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 4.7240 - val_loss: 70.2155\n",
      "Epoch 920/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 4.8975 - val_loss: 68.9812\n",
      "Epoch 921/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 4.8964 - val_loss: 68.3478\n",
      "Epoch 922/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 4.7461 - val_loss: 69.3286\n",
      "Epoch 923/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 4.9441 - val_loss: 71.1006\n",
      "Epoch 924/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 4.9356 - val_loss: 69.1120\n",
      "Epoch 925/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 5.1968 - val_loss: 70.8790\n",
      "Epoch 926/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 4.6868 - val_loss: 67.3072\n",
      "Epoch 927/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 4.8013 - val_loss: 70.8039\n",
      "Epoch 928/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 4.9380 - val_loss: 71.6050\n",
      "Epoch 929/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 5.3575 - val_loss: 78.4220\n",
      "Epoch 930/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 7.0463 - val_loss: 78.8231\n",
      "Epoch 931/1000\n",
      "5686/5686 [==============================] - 1s 122us/step - loss: 8.2823 - val_loss: 74.3752\n",
      "Epoch 932/1000\n",
      "5686/5686 [==============================] - 1s 118us/step - loss: 7.8978 - val_loss: 72.6635\n",
      "Epoch 933/1000\n",
      "5686/5686 [==============================] - 1s 122us/step - loss: 6.3008 - val_loss: 70.3408\n",
      "Epoch 934/1000\n",
      "5686/5686 [==============================] - 1s 121us/step - loss: 5.2439 - val_loss: 70.6548\n",
      "Epoch 935/1000\n",
      "5686/5686 [==============================] - 1s 115us/step - loss: 5.1436 - val_loss: 69.9251\n",
      "Epoch 936/1000\n",
      "5686/5686 [==============================] - 1s 126us/step - loss: 5.1464 - val_loss: 68.2720\n",
      "Epoch 937/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 4.8734 - val_loss: 68.9146\n",
      "Epoch 938/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 5.1750 - val_loss: 69.5749\n",
      "Epoch 939/1000\n",
      "5686/5686 [==============================] - 1s 114us/step - loss: 5.1507 - val_loss: 69.7632\n",
      "Epoch 940/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 5.0661 - val_loss: 70.5366\n",
      "Epoch 941/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.1150 - val_loss: 69.8004\n",
      "Epoch 942/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 4.8380 - val_loss: 69.4333\n",
      "Epoch 943/1000\n",
      "5686/5686 [==============================] - 1s 122us/step - loss: 4.5407 - val_loss: 69.0734\n",
      "Epoch 944/1000\n",
      "5686/5686 [==============================] - 1s 129us/step - loss: 4.6001 - val_loss: 69.6816\n",
      "Epoch 945/1000\n",
      "5686/5686 [==============================] - 1s 121us/step - loss: 4.4228 - val_loss: 66.8562\n",
      "Epoch 946/1000\n",
      "5686/5686 [==============================] - 1s 122us/step - loss: 4.9972 - val_loss: 68.5937\n",
      "Epoch 947/1000\n",
      "5686/5686 [==============================] - 1s 125us/step - loss: 4.6930 - val_loss: 68.0787\n",
      "Epoch 948/1000\n",
      "5686/5686 [==============================] - 1s 125us/step - loss: 4.9192 - val_loss: 70.2467\n",
      "Epoch 949/1000\n",
      "5686/5686 [==============================] - 1s 126us/step - loss: 5.0549 - val_loss: 69.8330\n",
      "Epoch 950/1000\n",
      "5686/5686 [==============================] - 1s 123us/step - loss: 4.5683 - val_loss: 68.7747\n",
      "Epoch 951/1000\n",
      "5686/5686 [==============================] - 1s 121us/step - loss: 4.5517 - val_loss: 69.0230\n",
      "Epoch 952/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 4.8586 - val_loss: 70.5213\n",
      "Epoch 953/1000\n",
      "5686/5686 [==============================] - 1s 118us/step - loss: 5.0766 - val_loss: 72.8605\n",
      "Epoch 954/1000\n",
      "5686/5686 [==============================] - 1s 119us/step - loss: 4.8742 - val_loss: 70.1118\n",
      "Epoch 955/1000\n",
      "5686/5686 [==============================] - 1s 129us/step - loss: 4.9628 - val_loss: 70.7121\n",
      "Epoch 956/1000\n",
      "5686/5686 [==============================] - 1s 120us/step - loss: 5.0219 - val_loss: 72.0742\n",
      "Epoch 957/1000\n",
      "5686/5686 [==============================] - 1s 113us/step - loss: 5.0334 - val_loss: 70.9147\n",
      "Epoch 958/1000\n",
      "5686/5686 [==============================] - 1s 111us/step - loss: 4.8753 - val_loss: 71.9669\n",
      "Epoch 959/1000\n",
      "5686/5686 [==============================] - 1s 116us/step - loss: 5.0452 - val_loss: 72.2323\n",
      "Epoch 960/1000\n",
      "5686/5686 [==============================] - 1s 124us/step - loss: 5.0191 - val_loss: 78.3161\n",
      "Epoch 961/1000\n",
      "5686/5686 [==============================] - 1s 117us/step - loss: 5.3985 - val_loss: 74.8504\n",
      "Epoch 962/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 4.9426 - val_loss: 74.5093\n",
      "Epoch 963/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 4.8532 - val_loss: 69.5412\n",
      "Epoch 964/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 4.8456 - val_loss: 70.6376\n",
      "Epoch 965/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 4.8216 - val_loss: 72.2747\n",
      "Epoch 966/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 4.8249 - val_loss: 74.7137\n",
      "Epoch 967/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 4.5366 - val_loss: 73.2570\n",
      "Epoch 968/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 4.5453 - val_loss: 75.5587\n",
      "Epoch 969/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 4.6550 - val_loss: 79.6471\n",
      "Epoch 970/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 4.5741 - val_loss: 82.9571\n",
      "Epoch 971/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 4.8880 - val_loss: 70.3223\n",
      "Epoch 972/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 4.7114 - val_loss: 70.6707\n",
      "Epoch 973/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 5.0931 - val_loss: 72.2310\n",
      "Epoch 974/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 4.8252 - val_loss: 73.4400\n",
      "Epoch 975/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 4.9185 - val_loss: 68.5661\n",
      "Epoch 976/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 4.9538 - val_loss: 70.6422\n",
      "Epoch 977/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 5.5610 - val_loss: 72.1745\n",
      "Epoch 978/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 5.3983 - val_loss: 68.8738\n",
      "Epoch 979/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 4.9481 - val_loss: 72.8357\n",
      "Epoch 980/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 4.9321 - val_loss: 71.9266\n",
      "Epoch 981/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 4.9112 - val_loss: 71.8357\n",
      "Epoch 982/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.2849 - val_loss: 76.4543\n",
      "Epoch 983/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.0121 - val_loss: 71.1562\n",
      "Epoch 984/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 4.7438 - val_loss: 70.6160\n",
      "Epoch 985/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 4.7626 - val_loss: 71.0383\n",
      "Epoch 986/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 4.6597 - val_loss: 70.3770\n",
      "Epoch 987/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 4.6130 - val_loss: 69.2525\n",
      "Epoch 988/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 4.6989 - val_loss: 69.7524\n",
      "Epoch 989/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 4.6889 - val_loss: 69.5756\n",
      "Epoch 990/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 4.5503 - val_loss: 69.8895\n",
      "Epoch 991/1000\n",
      "5686/5686 [==============================] - 1s 109us/step - loss: 4.5315 - val_loss: 69.9181\n",
      "Epoch 992/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 4.7403 - val_loss: 71.8175\n",
      "Epoch 993/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.3469 - val_loss: 73.1924\n",
      "Epoch 994/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 5.1687 - val_loss: 74.9175\n",
      "Epoch 995/1000\n",
      "5686/5686 [==============================] - 1s 107us/step - loss: 5.7531 - val_loss: 80.8057\n",
      "Epoch 996/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 6.5538 - val_loss: 74.8167\n",
      "Epoch 997/1000\n",
      "5686/5686 [==============================] - 1s 108us/step - loss: 6.4282 - val_loss: 77.5888\n",
      "Epoch 998/1000\n",
      "5686/5686 [==============================] - 1s 110us/step - loss: 8.7056 - val_loss: 77.4256\n",
      "Epoch 999/1000\n",
      "5686/5686 [==============================] - 1s 123us/step - loss: 8.5116 - val_loss: 81.5056\n",
      "Epoch 1000/1000\n",
      "5686/5686 [==============================] - 1s 124us/step - loss: 7.1098 - val_loss: 74.0234\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#stacked LSTMs, can do hyperparameter tuning\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(t_period))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "# fit model\n",
    "history = model.fit(train_X, train_y, epochs=1000, batch_size=100, validation_data=(test_X, test_y), verbose=1, shuffle=False)\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 2.111\n",
      "Test Standardized RMSE: 8.604\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.median(np.abs((y_true - y_pred) / np.mean(y_true))) * 100\n",
    "\n",
    "def standardized_rmse(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "yhat = model.predict(test_X, verbose=0)\n",
    "mape = mean_absolute_percentage_error(test_y, yhat)\n",
    "rmse = standardized_rmse(test_y, yhat)\n",
    "print('MAPE: %.3f' % mape)\n",
    "print('Test Standardized RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
