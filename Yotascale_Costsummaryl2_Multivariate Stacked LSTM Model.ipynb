{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6725, 14)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import copy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "#Import data\n",
    "pd.options.display.float_format = \"{:.5f}\".format\n",
    "df = pd.read_csv('costsummarydayl2_201905101409.csv')\n",
    "df.timedim = pd.to_datetime(df.timedim)\n",
    "\n",
    "#drop useless data\n",
    "df = df.drop([ \"ys_tenant_uuid\",\"ys_billingmonth\", \"payeraccountid\",\"ys_custom_1\",\"ys_custom_2\",\"ys_custom_3\",\n",
    "              \"ys_custom_4\",\"ys_custom_5\",\"ys_custom_6\",\"ys_custom_7\",\"ys_custom_8\",\"ys_custom_9\",\"ys_custom_10\",\n",
    "              \"ys_custom_11\",\"ys_custom_12\",\"ys_custom_13\",\"ys_custom_14\",\"ys_custom_15\",\"ys_custom_16\",\n",
    "              \"ys_custom_17\",\"ys_custom_18\",\"ys_custom_19\",\"ys_custom_20\",\"run_timestamp\", \"ys_updatedon\"], axis=1)\n",
    "df = df.drop([\"blendedcost\", \"ys_usagequantity\", \"ys_reservedusagequantity\", \"linkedaccountid\",\"ys_role\", \n",
    "              \"ys_customer\",\"ys_costcenter\",\"ys_compliance\", \"availabilityzone\", \"ys_instancetype\", \"ys_os\", \n",
    "              \"ys_tenancytype\", \"ys_reservedusagetype\",\"usagetype\",\"ys_usagetype\",\"ys_operationgroup\"], axis=1)\n",
    "\n",
    "#mapping perspective to the data\n",
    "df['perspective'] = np.where(df['ys_operatinghours']=='ANALYTICS-PLATFORM','Analytics Platform',\n",
    "                    np.where(df['ys_operatinghours']=='FORECASTING-AND-BUDGETING','Forecasting and Budgeting',\n",
    "                    np.where(df['ys_operatinghours']=='INVENTORY-AND-TAG-MANAGEMENT','Inventory and Tag Management',\n",
    "                    np.where(df['ys_operatinghours']=='INFRASTRUCTURE','Infrastructure',\n",
    "                    np.where(df['ys_operatinghours']=='PLATFORM','Infrastructure',     \n",
    "                    np.where(df['ys_operatinghours']=='WEB-APP','Webapp',\n",
    "                    np.where(df['ys_operatinghours']=='COST-PROESSING','Cost Processing',\n",
    "                    np.where(df['ys_operatinghours']=='UTILIZATION-ANALYTICS','Utilization Processing',\n",
    "                    np.where(df['ys_operatinghours']=='DEVOPS','Devops',\n",
    "                    np.where(df['ys_operatinghours']=='ANOMALY-DETECTION','Anomaly Detection',\n",
    "                    np.where(df['ys_operatinghours']=='RECOMMENDATION','Recommendations',\n",
    "                    np.where(df['ys_operatinghours']=='RECOMMENDATION-PROCESSING','Recommendations','Untagged'))))))))))))\n",
    "\n",
    "df=df[df.unblendedcost>=0]\n",
    "df=df[df.unblendedcost<df.unblendedcost.max()]\n",
    "df=df[df.unblendedcost<=5*df.unblendedcost.std() + df.unblendedcost.mean()]\n",
    "        \n",
    "data = df.drop(['usagequantity'],axis=1)\n",
    "data['ys_region'].fillna('Untagged', inplace=True)\n",
    "data['operation'].fillna('Untagged', inplace=True)\n",
    "#data aggregation to per product per perspective level: taking the mode of each feature\n",
    "df = data.groupby(['timedim','productname','perspective']).agg({'unblendedcost':'sum','ys_usagetypegroup':lambda x: x.value_counts().index[0],'ys_application':lambda x: x.value_counts().index[0],'ys_owner':lambda x: x.value_counts().index[0],\n",
    "                                                          'ys_cluster':lambda x: x.value_counts().index[0],'operation':lambda x: x.value_counts().index[0],'ys_region':lambda x: x.value_counts().index[0],'ys_environment':lambda x: x.value_counts().index[0],\n",
    "                                                          'ys_operatinghours':lambda x: x.value_counts().index[0],'ys_project':lambda x: x.value_counts().index[0],'ys_type':lambda x: x.value_counts().index[0],'reservedinstance':lambda x: x.value_counts().index[0],\n",
    "                                                               'productname':lambda x: x.value_counts().index[0],'perspective':lambda x: x.value_counts().index[0]})\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236 total features after one-hot encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>unblendedcost</th>\n",
       "      <th>ys_usagetypegroup_Access</th>\n",
       "      <th>ys_usagetypegroup_Instance</th>\n",
       "      <th>ys_usagetypegroup_Network</th>\n",
       "      <th>ys_usagetypegroup_Others</th>\n",
       "      <th>ys_usagetypegroup_Storage</th>\n",
       "      <th>ys_usagetypegroup_Unutilized Reservation</th>\n",
       "      <th>ys_application_ALB</th>\n",
       "      <th>ys_application_BACKEND</th>\n",
       "      <th>ys_application_BACKTEST</th>\n",
       "      <th>...</th>\n",
       "      <th>perspective_Anomaly Detection</th>\n",
       "      <th>perspective_Cost Processing</th>\n",
       "      <th>perspective_Devops</th>\n",
       "      <th>perspective_Forecasting and Budgeting</th>\n",
       "      <th>perspective_Infrastructure</th>\n",
       "      <th>perspective_Inventory and Tag Management</th>\n",
       "      <th>perspective_Recommendations</th>\n",
       "      <th>perspective_Untagged</th>\n",
       "      <th>perspective_Utilization Processing</th>\n",
       "      <th>perspective_Webapp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timedim</th>\n",
       "      <th>productname</th>\n",
       "      <th>perspective</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2019-01-01</th>\n",
       "      <th>AWS Budgets</th>\n",
       "      <th>Untagged</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AWS CloudTrail</th>\n",
       "      <th>Untagged</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AWS Cost Explorer</th>\n",
       "      <th>Untagged</th>\n",
       "      <td>0.29000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AWS Glue</th>\n",
       "      <th>Untagged</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AWS Key Management Service</th>\n",
       "      <th>Infrastructure</th>\n",
       "      <td>0.03226</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 236 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      unblendedcost  \\\n",
       "timedim    productname                perspective                     \n",
       "2019-01-01 AWS Budgets                Untagged              0.00000   \n",
       "           AWS CloudTrail             Untagged              0.00000   \n",
       "           AWS Cost Explorer          Untagged              0.29000   \n",
       "           AWS Glue                   Untagged              0.00000   \n",
       "           AWS Key Management Service Infrastructure        0.03226   \n",
       "\n",
       "                                                      ys_usagetypegroup_Access  \\\n",
       "timedim    productname                perspective                                \n",
       "2019-01-01 AWS Budgets                Untagged                               0   \n",
       "           AWS CloudTrail             Untagged                               0   \n",
       "           AWS Cost Explorer          Untagged                               0   \n",
       "           AWS Glue                   Untagged                               0   \n",
       "           AWS Key Management Service Infrastructure                         0   \n",
       "\n",
       "                                                      ys_usagetypegroup_Instance  \\\n",
       "timedim    productname                perspective                                  \n",
       "2019-01-01 AWS Budgets                Untagged                                 0   \n",
       "           AWS CloudTrail             Untagged                                 0   \n",
       "           AWS Cost Explorer          Untagged                                 0   \n",
       "           AWS Glue                   Untagged                                 0   \n",
       "           AWS Key Management Service Infrastructure                           0   \n",
       "\n",
       "                                                      ys_usagetypegroup_Network  \\\n",
       "timedim    productname                perspective                                 \n",
       "2019-01-01 AWS Budgets                Untagged                                0   \n",
       "           AWS CloudTrail             Untagged                                0   \n",
       "           AWS Cost Explorer          Untagged                                0   \n",
       "           AWS Glue                   Untagged                                0   \n",
       "           AWS Key Management Service Infrastructure                          0   \n",
       "\n",
       "                                                      ys_usagetypegroup_Others  \\\n",
       "timedim    productname                perspective                                \n",
       "2019-01-01 AWS Budgets                Untagged                               1   \n",
       "           AWS CloudTrail             Untagged                               1   \n",
       "           AWS Cost Explorer          Untagged                               1   \n",
       "           AWS Glue                   Untagged                               1   \n",
       "           AWS Key Management Service Infrastructure                         1   \n",
       "\n",
       "                                                      ys_usagetypegroup_Storage  \\\n",
       "timedim    productname                perspective                                 \n",
       "2019-01-01 AWS Budgets                Untagged                                0   \n",
       "           AWS CloudTrail             Untagged                                0   \n",
       "           AWS Cost Explorer          Untagged                                0   \n",
       "           AWS Glue                   Untagged                                0   \n",
       "           AWS Key Management Service Infrastructure                          0   \n",
       "\n",
       "                                                      ys_usagetypegroup_Unutilized Reservation  \\\n",
       "timedim    productname                perspective                                                \n",
       "2019-01-01 AWS Budgets                Untagged                                               0   \n",
       "           AWS CloudTrail             Untagged                                               0   \n",
       "           AWS Cost Explorer          Untagged                                               0   \n",
       "           AWS Glue                   Untagged                                               0   \n",
       "           AWS Key Management Service Infrastructure                                         0   \n",
       "\n",
       "                                                      ys_application_ALB  \\\n",
       "timedim    productname                perspective                          \n",
       "2019-01-01 AWS Budgets                Untagged                         0   \n",
       "           AWS CloudTrail             Untagged                         0   \n",
       "           AWS Cost Explorer          Untagged                         0   \n",
       "           AWS Glue                   Untagged                         0   \n",
       "           AWS Key Management Service Infrastructure                   0   \n",
       "\n",
       "                                                      ys_application_BACKEND  \\\n",
       "timedim    productname                perspective                              \n",
       "2019-01-01 AWS Budgets                Untagged                             0   \n",
       "           AWS CloudTrail             Untagged                             0   \n",
       "           AWS Cost Explorer          Untagged                             0   \n",
       "           AWS Glue                   Untagged                             0   \n",
       "           AWS Key Management Service Infrastructure                       0   \n",
       "\n",
       "                                                      ys_application_BACKTEST  \\\n",
       "timedim    productname                perspective                               \n",
       "2019-01-01 AWS Budgets                Untagged                              0   \n",
       "           AWS CloudTrail             Untagged                              0   \n",
       "           AWS Cost Explorer          Untagged                              0   \n",
       "           AWS Glue                   Untagged                              0   \n",
       "           AWS Key Management Service Infrastructure                        0   \n",
       "\n",
       "                                                      ...  \\\n",
       "timedim    productname                perspective     ...   \n",
       "2019-01-01 AWS Budgets                Untagged        ...   \n",
       "           AWS CloudTrail             Untagged        ...   \n",
       "           AWS Cost Explorer          Untagged        ...   \n",
       "           AWS Glue                   Untagged        ...   \n",
       "           AWS Key Management Service Infrastructure  ...   \n",
       "\n",
       "                                                      perspective_Anomaly Detection  \\\n",
       "timedim    productname                perspective                                     \n",
       "2019-01-01 AWS Budgets                Untagged                                    0   \n",
       "           AWS CloudTrail             Untagged                                    0   \n",
       "           AWS Cost Explorer          Untagged                                    0   \n",
       "           AWS Glue                   Untagged                                    0   \n",
       "           AWS Key Management Service Infrastructure                              0   \n",
       "\n",
       "                                                      perspective_Cost Processing  \\\n",
       "timedim    productname                perspective                                   \n",
       "2019-01-01 AWS Budgets                Untagged                                  0   \n",
       "           AWS CloudTrail             Untagged                                  0   \n",
       "           AWS Cost Explorer          Untagged                                  0   \n",
       "           AWS Glue                   Untagged                                  0   \n",
       "           AWS Key Management Service Infrastructure                            0   \n",
       "\n",
       "                                                      perspective_Devops  \\\n",
       "timedim    productname                perspective                          \n",
       "2019-01-01 AWS Budgets                Untagged                         0   \n",
       "           AWS CloudTrail             Untagged                         0   \n",
       "           AWS Cost Explorer          Untagged                         0   \n",
       "           AWS Glue                   Untagged                         0   \n",
       "           AWS Key Management Service Infrastructure                   0   \n",
       "\n",
       "                                                      perspective_Forecasting and Budgeting  \\\n",
       "timedim    productname                perspective                                             \n",
       "2019-01-01 AWS Budgets                Untagged                                            0   \n",
       "           AWS CloudTrail             Untagged                                            0   \n",
       "           AWS Cost Explorer          Untagged                                            0   \n",
       "           AWS Glue                   Untagged                                            0   \n",
       "           AWS Key Management Service Infrastructure                                      0   \n",
       "\n",
       "                                                      perspective_Infrastructure  \\\n",
       "timedim    productname                perspective                                  \n",
       "2019-01-01 AWS Budgets                Untagged                                 0   \n",
       "           AWS CloudTrail             Untagged                                 0   \n",
       "           AWS Cost Explorer          Untagged                                 0   \n",
       "           AWS Glue                   Untagged                                 0   \n",
       "           AWS Key Management Service Infrastructure                           1   \n",
       "\n",
       "                                                      perspective_Inventory and Tag Management  \\\n",
       "timedim    productname                perspective                                                \n",
       "2019-01-01 AWS Budgets                Untagged                                               0   \n",
       "           AWS CloudTrail             Untagged                                               0   \n",
       "           AWS Cost Explorer          Untagged                                               0   \n",
       "           AWS Glue                   Untagged                                               0   \n",
       "           AWS Key Management Service Infrastructure                                         0   \n",
       "\n",
       "                                                      perspective_Recommendations  \\\n",
       "timedim    productname                perspective                                   \n",
       "2019-01-01 AWS Budgets                Untagged                                  0   \n",
       "           AWS CloudTrail             Untagged                                  0   \n",
       "           AWS Cost Explorer          Untagged                                  0   \n",
       "           AWS Glue                   Untagged                                  0   \n",
       "           AWS Key Management Service Infrastructure                            0   \n",
       "\n",
       "                                                      perspective_Untagged  \\\n",
       "timedim    productname                perspective                            \n",
       "2019-01-01 AWS Budgets                Untagged                           1   \n",
       "           AWS CloudTrail             Untagged                           1   \n",
       "           AWS Cost Explorer          Untagged                           1   \n",
       "           AWS Glue                   Untagged                           1   \n",
       "           AWS Key Management Service Infrastructure                     0   \n",
       "\n",
       "                                                      perspective_Utilization Processing  \\\n",
       "timedim    productname                perspective                                          \n",
       "2019-01-01 AWS Budgets                Untagged                                         0   \n",
       "           AWS CloudTrail             Untagged                                         0   \n",
       "           AWS Cost Explorer          Untagged                                         0   \n",
       "           AWS Glue                   Untagged                                         0   \n",
       "           AWS Key Management Service Infrastructure                                   0   \n",
       "\n",
       "                                                      perspective_Webapp  \n",
       "timedim    productname                perspective                         \n",
       "2019-01-01 AWS Budgets                Untagged                         0  \n",
       "           AWS CloudTrail             Untagged                         0  \n",
       "           AWS Cost Explorer          Untagged                         0  \n",
       "           AWS Glue                   Untagged                         0  \n",
       "           AWS Key Management Service Infrastructure                   0  \n",
       "\n",
       "[5 rows x 236 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one-hot encoding\n",
    "data = pd.get_dummies(df)\n",
    "#data['productname']=df.productname\n",
    "#data['perspective']=df.perspective\n",
    "encoded = list(data.columns)\n",
    "print(\"{} total features after one-hot encoding.\".format(len(encoded)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6725, 236)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.reset_index(inplace=True)\n",
    "data = data.set_index(['productname','perspective'])\n",
    "ori_column_num=data.shape[1]-1\n",
    "ori_column_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leung\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1494: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  return self._getitem_tuple(key)\n"
     ]
    }
   ],
   "source": [
    "#divide the whole dataframe into dictionary, key is the perspective and product, value is the dataframe under this type\n",
    "prob_pers_index=pd.DataFrame(data.index).drop_duplicates()\n",
    "data_dict={}\n",
    "for i in range(len(prob_pers_index)):\n",
    "    temp_df=data.loc[prob_pers_index.iloc[i,0],:]\n",
    "    temp_df.set_index('timedim',inplace=True)\n",
    "    data_dict[prob_pers_index.iloc[i,0]]=temp_df\n",
    "    #print(prob_pers_index.iloc[i,0],len(data_dict[prob_pers_index.iloc[i,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of timesteps to go back for feature engineering\n",
    "def shift_series(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    dff = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    for i in range(n_in, 0, -1):\n",
    "        if i>=len(dff):\n",
    "            # if dataframe is not long enough, fill in with the last time period\n",
    "            cols.append(dff.shift(len(dff)-1))\n",
    "        else:\n",
    "            cols.append(dff.shift(i))\n",
    "        names += [(dff.columns[j]+'(t-%d)' % (i)) for j in range(n_vars)]\n",
    "    for i in range(0, n_out):\n",
    "        if i>=len(dff):\n",
    "            cols.append(dff.shift(1-len(dff)))\n",
    "        else:\n",
    "            cols.append(dff.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [(dff.columns[j]+'(t)') for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [(dff.columns[j]+'(t+%d)' % (i)) for j in range(n_vars)]\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import numpy as np \n",
    "from scipy.stats import randint\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns  \n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD \n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "import itertools\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_period=1\n",
    "multi_time_data_dict={}\n",
    "\n",
    "# for every type (perspective x product), shift data to prepare features and labels\n",
    "for key in data_dict.keys():\n",
    "    temp=shift_series(data_dict[key],t_period,1)\n",
    "    label_index=['unblendedcost(t-'+str(t_period-i-1)+')' for i in range(t_period-1)]+['unblendedcost(t)']\n",
    "    label_data=temp.loc[:,label_index]\n",
    "    label_data.columns=['y(t-'+str(t_period-i-1)+')' for i in range(t_period-1)]+['y(t)']\n",
    "    temp.drop(temp.iloc[:,-ori_column_num:], axis=1, inplace=True)\n",
    "    temp=pd.concat([temp,label_data],axis=1)\n",
    "    #print(reframed_data.columns)\n",
    "    multi_time_data_dict[key]=temp\n",
    "    #print(key,len(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for key in multi_time_data_dict.keys():\n",
    "    values=multi_time_data_dict[key].values\n",
    "    train_index = int(values.shape[0]*0.6)+1\n",
    "    train = values[:train_index, :]\n",
    "    test = values[train_index:, :]\n",
    "    sub_train_X, sub_train_y = train[:, :-t_period], train[:, -t_period:]\n",
    "    sub_test_X, sub_test_y = test[:, :-t_period], test[:, -t_period:]\n",
    "    sub_train_X = sub_train_X.reshape((sub_train_X.shape[0], t_period, ori_column_num))\n",
    "    sub_test_X = sub_test_X.reshape((sub_test_X.shape[0], t_period, ori_column_num))\n",
    "    if i==0:\n",
    "        train_X,train_y,test_X,test_y=sub_train_X,sub_train_y,sub_test_X,sub_test_y\n",
    "    else:\n",
    "        train_X=np.concatenate([train_X,sub_train_X])\n",
    "        train_y=np.concatenate([train_y,sub_train_y])\n",
    "        test_X=np.concatenate([test_X,sub_test_X])\n",
    "        test_y=np.concatenate([test_y,sub_test_y])\n",
    "    i+=1\n",
    "    #print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4023 samples, validate on 2640 samples\n",
      "Epoch 1/2000\n",
      "4023/4023 [==============================] - 2s 594us/step - loss: 3908.8488 - val_loss: 6316.5811\n",
      "Epoch 2/2000\n",
      "2100/4023 [==============>...............] - ETA: 0s - loss: 5869.7638"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leung\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:842: RuntimeWarning: Early stopping conditioned on metric `mean_squared_error` which is not available. Available metrics are: val_loss,loss\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4023/4023 [==============================] - 0s 104us/step - loss: 3663.9644 - val_loss: 6170.3494\n",
      "Epoch 3/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 3593.8389 - val_loss: 6112.2462\n",
      "Epoch 4/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 3536.5177 - val_loss: 6060.1351\n",
      "Epoch 5/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 3483.6536 - val_loss: 6011.2235\n",
      "Epoch 6/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 3435.0864 - val_loss: 5964.7613\n",
      "Epoch 7/2000\n",
      "4023/4023 [==============================] - 0s 106us/step - loss: 3386.4221 - val_loss: 5920.9436\n",
      "Epoch 8/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 3346.6856 - val_loss: 5880.0903\n",
      "Epoch 9/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 3301.1631 - val_loss: 5841.5707\n",
      "Epoch 10/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 3257.1986 - val_loss: 5795.9602\n",
      "Epoch 11/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 3221.1740 - val_loss: 5751.9791\n",
      "Epoch 12/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 3175.7092 - val_loss: 5711.0632\n",
      "Epoch 13/2000\n",
      "4023/4023 [==============================] - 0s 109us/step - loss: 3139.1361 - val_loss: 5673.8779\n",
      "Epoch 14/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 3106.4900 - val_loss: 5641.2821\n",
      "Epoch 15/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 3067.9133 - val_loss: 5605.8315\n",
      "Epoch 16/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 3025.5880 - val_loss: 5565.9135\n",
      "Epoch 17/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 2992.2183 - val_loss: 5526.3577\n",
      "Epoch 18/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 2961.3016 - val_loss: 5490.0015\n",
      "Epoch 19/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 2929.8514 - val_loss: 5451.8069\n",
      "Epoch 20/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 2887.2604 - val_loss: 5418.1624\n",
      "Epoch 21/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 2858.4760 - val_loss: 5388.8755\n",
      "Epoch 22/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 2842.5553 - val_loss: 5363.7936\n",
      "Epoch 23/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 2794.8595 - val_loss: 5323.6483\n",
      "Epoch 24/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 2767.7756 - val_loss: 5285.3470\n",
      "Epoch 25/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 2727.4596 - val_loss: 5252.4238\n",
      "Epoch 26/2000\n",
      "4023/4023 [==============================] - 0s 107us/step - loss: 2699.6533 - val_loss: 5222.1630\n",
      "Epoch 27/2000\n",
      "4023/4023 [==============================] - 0s 113us/step - loss: 2672.7641 - val_loss: 5191.3219\n",
      "Epoch 28/2000\n",
      "4023/4023 [==============================] - 0s 111us/step - loss: 2639.7998 - val_loss: 5165.9853\n",
      "Epoch 29/2000\n",
      "4023/4023 [==============================] - 0s 105us/step - loss: 2623.7788 - val_loss: 5134.8715\n",
      "Epoch 30/2000\n",
      "4023/4023 [==============================] - 1s 127us/step - loss: 2583.4407 - val_loss: 5103.5233\n",
      "Epoch 31/2000\n",
      "4023/4023 [==============================] - 0s 110us/step - loss: 2556.5126 - val_loss: 5070.4408\n",
      "Epoch 32/2000\n",
      "4023/4023 [==============================] - 1s 125us/step - loss: 2527.3899 - val_loss: 5038.9958\n",
      "Epoch 33/2000\n",
      "4023/4023 [==============================] - 1s 128us/step - loss: 2496.3203 - val_loss: 5015.0118\n",
      "Epoch 34/2000\n",
      "4023/4023 [==============================] - 0s 108us/step - loss: 2464.6085 - val_loss: 4984.3667\n",
      "Epoch 35/2000\n",
      "4023/4023 [==============================] - 0s 124us/step - loss: 2434.5961 - val_loss: 4954.1244\n",
      "Epoch 36/2000\n",
      "4023/4023 [==============================] - 1s 127us/step - loss: 2411.1313 - val_loss: 4923.5276\n",
      "Epoch 37/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 2390.7577 - val_loss: 4898.2381\n",
      "Epoch 38/2000\n",
      "4023/4023 [==============================] - 0s 107us/step - loss: 2366.4451 - val_loss: 4871.5824\n",
      "Epoch 39/2000\n",
      "4023/4023 [==============================] - 0s 108us/step - loss: 2340.0771 - val_loss: 4838.7798\n",
      "Epoch 40/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 2308.7135 - val_loss: 4810.4059\n",
      "Epoch 41/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 2290.7059 - val_loss: 4785.5531\n",
      "Epoch 42/2000\n",
      "4023/4023 [==============================] - 0s 119us/step - loss: 2255.1019 - val_loss: 4757.9691\n",
      "Epoch 43/2000\n",
      "4023/4023 [==============================] - 0s 108us/step - loss: 2233.9135 - val_loss: 4731.3657\n",
      "Epoch 44/2000\n",
      "4023/4023 [==============================] - 1s 127us/step - loss: 2203.4465 - val_loss: 4704.4385\n",
      "Epoch 45/2000\n",
      "4023/4023 [==============================] - 0s 116us/step - loss: 2178.7406 - val_loss: 4675.5972\n",
      "Epoch 46/2000\n",
      "4023/4023 [==============================] - 0s 105us/step - loss: 2156.2071 - val_loss: 4650.9529\n",
      "Epoch 47/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 2130.1289 - val_loss: 4630.5688\n",
      "Epoch 48/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 2102.8850 - val_loss: 4599.7915\n",
      "Epoch 49/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 2081.1634 - val_loss: 4572.6694\n",
      "Epoch 50/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 2056.9462 - val_loss: 4543.4556\n",
      "Epoch 51/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 2036.8259 - val_loss: 4520.5225\n",
      "Epoch 52/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 2029.3030 - val_loss: 4498.9050\n",
      "Epoch 53/2000\n",
      "4023/4023 [==============================] - 0s 122us/step - loss: 1993.7434 - val_loss: 4478.5507\n",
      "Epoch 54/2000\n",
      "4023/4023 [==============================] - 0s 120us/step - loss: 1968.8402 - val_loss: 4455.2510\n",
      "Epoch 55/2000\n",
      "4023/4023 [==============================] - 1s 128us/step - loss: 1948.0811 - val_loss: 4424.5495\n",
      "Epoch 56/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 1920.3909 - val_loss: 4396.7874\n",
      "Epoch 57/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 1906.3218 - val_loss: 4375.7733\n",
      "Epoch 58/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 1881.7425 - val_loss: 4357.2235\n",
      "Epoch 59/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 1856.1860 - val_loss: 4330.8939\n",
      "Epoch 60/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 1830.1601 - val_loss: 4305.9485\n",
      "Epoch 61/2000\n",
      "4023/4023 [==============================] - 0s 118us/step - loss: 1815.8481 - val_loss: 4283.1885\n",
      "Epoch 62/2000\n",
      "4023/4023 [==============================] - 0s 123us/step - loss: 1800.3042 - val_loss: 4261.4256\n",
      "Epoch 63/2000\n",
      "4023/4023 [==============================] - 0s 119us/step - loss: 1774.0897 - val_loss: 4241.3900\n",
      "Epoch 64/2000\n",
      "4023/4023 [==============================] - 0s 112us/step - loss: 1752.8051 - val_loss: 4210.9641\n",
      "Epoch 65/2000\n",
      "4023/4023 [==============================] - 0s 114us/step - loss: 1742.5302 - val_loss: 4194.5570\n",
      "Epoch 66/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 1719.7840 - val_loss: 4171.3909\n",
      "Epoch 67/2000\n",
      "4023/4023 [==============================] - 0s 107us/step - loss: 1699.5852 - val_loss: 4153.7857\n",
      "Epoch 68/2000\n",
      "4023/4023 [==============================] - 0s 113us/step - loss: 1682.1889 - val_loss: 4124.8587\n",
      "Epoch 69/2000\n",
      "4023/4023 [==============================] - 0s 118us/step - loss: 1666.9122 - val_loss: 4111.2966\n",
      "Epoch 70/2000\n",
      "4023/4023 [==============================] - 0s 107us/step - loss: 1646.4596 - val_loss: 4086.2785\n",
      "Epoch 71/2000\n",
      "4023/4023 [==============================] - 0s 109us/step - loss: 1612.2460 - val_loss: 4077.6724\n",
      "Epoch 72/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 1598.3861 - val_loss: 4046.1977\n",
      "Epoch 73/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 1587.3344 - val_loss: 4029.7713\n",
      "Epoch 74/2000\n",
      "4023/4023 [==============================] - 0s 118us/step - loss: 1562.6449 - val_loss: 4007.3825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/2000\n",
      "4023/4023 [==============================] - 0s 107us/step - loss: 1564.1294 - val_loss: 3979.8701\n",
      "Epoch 76/2000\n",
      "4023/4023 [==============================] - 0s 110us/step - loss: 1549.8117 - val_loss: 3969.1132\n",
      "Epoch 77/2000\n",
      "4023/4023 [==============================] - 0s 112us/step - loss: 1524.6167 - val_loss: 3950.1093\n",
      "Epoch 78/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 1504.8547 - val_loss: 3921.9765\n",
      "Epoch 79/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 1483.6835 - val_loss: 3903.8729\n",
      "Epoch 80/2000\n",
      "4023/4023 [==============================] - 0s 107us/step - loss: 1471.0941 - val_loss: 3894.1151\n",
      "Epoch 81/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 1456.0091 - val_loss: 3864.9878\n",
      "Epoch 82/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 1432.0792 - val_loss: 3848.3840\n",
      "Epoch 83/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 1416.6840 - val_loss: 3830.7015\n",
      "Epoch 84/2000\n",
      "4023/4023 [==============================] - 0s 112us/step - loss: 1398.9051 - val_loss: 3825.7511\n",
      "Epoch 85/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 1385.2063 - val_loss: 3832.1275\n",
      "Epoch 86/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 1372.4874 - val_loss: 3776.9614\n",
      "Epoch 87/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 1373.6424 - val_loss: 3782.7464\n",
      "Epoch 88/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 1360.3874 - val_loss: 3758.2511\n",
      "Epoch 89/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 1336.8117 - val_loss: 3723.1873\n",
      "Epoch 90/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 1318.2668 - val_loss: 3713.4392\n",
      "Epoch 91/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 1310.6401 - val_loss: 3685.3862\n",
      "Epoch 92/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 1290.8260 - val_loss: 3676.9079\n",
      "Epoch 93/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 1270.7888 - val_loss: 3656.1815\n",
      "Epoch 94/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 1265.2027 - val_loss: 3646.3625\n",
      "Epoch 95/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 1246.6743 - val_loss: 3628.7810\n",
      "Epoch 96/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 1242.1671 - val_loss: 3616.0859\n",
      "Epoch 97/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 1225.5968 - val_loss: 3597.8412\n",
      "Epoch 98/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 1219.4080 - val_loss: 3568.4769\n",
      "Epoch 99/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 1214.5191 - val_loss: 3616.3896\n",
      "Epoch 100/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 1191.0331 - val_loss: 3553.3326\n",
      "Epoch 101/2000\n",
      "4023/4023 [==============================] - 0s 107us/step - loss: 1169.1078 - val_loss: 3533.2919\n",
      "Epoch 102/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 1161.2527 - val_loss: 3532.8630\n",
      "Epoch 103/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 1155.6232 - val_loss: 3495.0706\n",
      "Epoch 104/2000\n",
      "4023/4023 [==============================] - 0s 108us/step - loss: 1145.0154 - val_loss: 3499.8851\n",
      "Epoch 105/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 1136.7493 - val_loss: 3464.5964\n",
      "Epoch 106/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 1119.4624 - val_loss: 3455.9828\n",
      "Epoch 107/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 1134.5031 - val_loss: 3473.9875\n",
      "Epoch 108/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 1109.8925 - val_loss: 3433.2273\n",
      "Epoch 109/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 1101.9381 - val_loss: 3412.3786\n",
      "Epoch 110/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 1076.4008 - val_loss: 3397.6608\n",
      "Epoch 111/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 1051.7314 - val_loss: 3377.6528\n",
      "Epoch 112/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 1064.6539 - val_loss: 3388.5263\n",
      "Epoch 113/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 1054.6804 - val_loss: 3356.8451\n",
      "Epoch 114/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 1030.0591 - val_loss: 3349.6260\n",
      "Epoch 115/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 1024.8361 - val_loss: 3345.2564\n",
      "Epoch 116/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 1032.3689 - val_loss: 3309.0829\n",
      "Epoch 117/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 1014.7898 - val_loss: 3315.6942\n",
      "Epoch 118/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 996.6461 - val_loss: 3290.7432\n",
      "Epoch 119/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 1003.5847 - val_loss: 3293.0414\n",
      "Epoch 120/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 986.9794 - val_loss: 3269.6916\n",
      "Epoch 121/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 966.0204 - val_loss: 3260.0769\n",
      "Epoch 122/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 974.9316 - val_loss: 3248.7731\n",
      "Epoch 123/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 969.7997 - val_loss: 3239.0773\n",
      "Epoch 124/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 944.3919 - val_loss: 3234.2309\n",
      "Epoch 125/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 942.1886 - val_loss: 3210.9475\n",
      "Epoch 126/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 932.4176 - val_loss: 3243.0852\n",
      "Epoch 127/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 942.2928 - val_loss: 3192.8974\n",
      "Epoch 128/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 934.8444 - val_loss: 3212.6078\n",
      "Epoch 129/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 911.2569 - val_loss: 3170.1339\n",
      "Epoch 130/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 901.5267 - val_loss: 3186.4088\n",
      "Epoch 131/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 911.4553 - val_loss: 3136.1124\n",
      "Epoch 132/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 900.1095 - val_loss: 3255.2415\n",
      "Epoch 133/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 905.0211 - val_loss: 3115.6813\n",
      "Epoch 134/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 946.1722 - val_loss: 3216.7028\n",
      "Epoch 135/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 872.7535 - val_loss: 3186.6966\n",
      "Epoch 136/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 872.6840 - val_loss: 3106.9523\n",
      "Epoch 137/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 869.6990 - val_loss: 3165.4413\n",
      "Epoch 138/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 1040.9052 - val_loss: 3119.5806\n",
      "Epoch 139/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 889.3924 - val_loss: 3065.3889\n",
      "Epoch 140/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 882.0226 - val_loss: 3101.2281\n",
      "Epoch 141/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 869.1732 - val_loss: 3071.3341\n",
      "Epoch 142/2000\n",
      "4023/4023 [==============================] - 0s 105us/step - loss: 853.2272 - val_loss: 3018.9267\n",
      "Epoch 143/2000\n",
      "4023/4023 [==============================] - 0s 105us/step - loss: 823.2342 - val_loss: 3043.7913\n",
      "Epoch 144/2000\n",
      "4023/4023 [==============================] - 0s 108us/step - loss: 833.0597 - val_loss: 3003.8961\n",
      "Epoch 145/2000\n",
      "4023/4023 [==============================] - 0s 108us/step - loss: 818.8160 - val_loss: 2996.3247\n",
      "Epoch 146/2000\n",
      "4023/4023 [==============================] - 0s 105us/step - loss: 828.6273 - val_loss: 3020.7978\n",
      "Epoch 147/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4023/4023 [==============================] - 0s 104us/step - loss: 811.1567 - val_loss: 2982.6963\n",
      "Epoch 148/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 806.4958 - val_loss: 2963.7074\n",
      "Epoch 149/2000\n",
      "4023/4023 [==============================] - 0s 105us/step - loss: 828.1883 - val_loss: 3020.5419\n",
      "Epoch 150/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 811.5493 - val_loss: 2956.1066\n",
      "Epoch 151/2000\n",
      "4023/4023 [==============================] - 0s 105us/step - loss: 838.0012 - val_loss: 2975.6322\n",
      "Epoch 152/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 806.3107 - val_loss: 2945.8306\n",
      "Epoch 153/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 794.7324 - val_loss: 2946.1257\n",
      "Epoch 154/2000\n",
      "4023/4023 [==============================] - 0s 108us/step - loss: 786.1509 - val_loss: 2934.3362\n",
      "Epoch 155/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 785.6289 - val_loss: 2915.6143\n",
      "Epoch 156/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 771.8700 - val_loss: 2927.0596\n",
      "Epoch 157/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 791.2774 - val_loss: 2895.0666\n",
      "Epoch 158/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 792.2724 - val_loss: 2923.8301\n",
      "Epoch 159/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 772.6919 - val_loss: 2890.3622\n",
      "Epoch 160/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 764.5595 - val_loss: 2881.8338\n",
      "Epoch 161/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 740.5864 - val_loss: 2869.8069\n",
      "Epoch 162/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 762.2261 - val_loss: 2866.7982\n",
      "Epoch 163/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 739.9498 - val_loss: 2863.9129\n",
      "Epoch 164/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 753.2928 - val_loss: 2870.0718\n",
      "Epoch 165/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 751.1458 - val_loss: 2840.9684\n",
      "Epoch 166/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 739.6848 - val_loss: 2879.3938\n",
      "Epoch 167/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 730.4571 - val_loss: 2831.0293\n",
      "Epoch 168/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 731.0382 - val_loss: 2829.2753\n",
      "Epoch 169/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 730.1056 - val_loss: 2813.6241\n",
      "Epoch 170/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 787.9502 - val_loss: 2831.1973\n",
      "Epoch 171/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 749.0222 - val_loss: 2809.9720\n",
      "Epoch 172/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 734.3084 - val_loss: 2897.0570\n",
      "Epoch 173/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 747.1890 - val_loss: 2812.8208\n",
      "Epoch 174/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 714.9956 - val_loss: 2818.5173\n",
      "Epoch 175/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 739.2663 - val_loss: 2810.0454\n",
      "Epoch 176/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 730.7301 - val_loss: 2788.2604\n",
      "Epoch 177/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 715.1797 - val_loss: 2847.5123\n",
      "Epoch 178/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 731.7085 - val_loss: 2798.9282\n",
      "Epoch 179/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 713.9983 - val_loss: 2772.8528\n",
      "Epoch 180/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 710.6264 - val_loss: 2804.8996\n",
      "Epoch 181/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 721.8935 - val_loss: 2753.2849\n",
      "Epoch 182/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 681.4341 - val_loss: 2769.2182\n",
      "Epoch 183/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 728.0486 - val_loss: 2759.5985\n",
      "Epoch 184/2000\n",
      "4023/4023 [==============================] - 0s 105us/step - loss: 707.8292 - val_loss: 2793.6192\n",
      "Epoch 185/2000\n",
      "4023/4023 [==============================] - 0s 109us/step - loss: 694.2770 - val_loss: 2752.8354\n",
      "Epoch 186/2000\n",
      "4023/4023 [==============================] - 0s 106us/step - loss: 708.2472 - val_loss: 2729.5721\n",
      "Epoch 187/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 679.4825 - val_loss: 2812.8205\n",
      "Epoch 188/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 734.8056 - val_loss: 2744.6885\n",
      "Epoch 189/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 734.3100 - val_loss: 2674.9396\n",
      "Epoch 190/2000\n",
      "4023/4023 [==============================] - 0s 107us/step - loss: 728.4912 - val_loss: 2785.1558\n",
      "Epoch 191/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 745.4990 - val_loss: 2758.8159\n",
      "Epoch 192/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 748.5853 - val_loss: 2721.2254\n",
      "Epoch 193/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 681.2910 - val_loss: 3055.0626\n",
      "Epoch 194/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 1036.2089 - val_loss: 2761.1016\n",
      "Epoch 195/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 759.6796 - val_loss: 2696.2795\n",
      "Epoch 196/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 788.8245 - val_loss: 2685.3142\n",
      "Epoch 197/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 744.4741 - val_loss: 2683.0065\n",
      "Epoch 198/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 779.5931 - val_loss: 2673.2773\n",
      "Epoch 199/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 723.2201 - val_loss: 2674.7731\n",
      "Epoch 200/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 707.8122 - val_loss: 2686.2670\n",
      "Epoch 201/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 721.9423 - val_loss: 2703.5244\n",
      "Epoch 202/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 814.1678 - val_loss: 2697.0774\n",
      "Epoch 203/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 750.8738 - val_loss: 2662.8723\n",
      "Epoch 204/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 678.2535 - val_loss: 2845.6691\n",
      "Epoch 205/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 911.1708 - val_loss: 2718.5017\n",
      "Epoch 206/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 778.1322 - val_loss: 2711.4993\n",
      "Epoch 207/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 728.9287 - val_loss: 2761.5754\n",
      "Epoch 208/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 907.9950 - val_loss: 2673.3736\n",
      "Epoch 209/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 769.6247 - val_loss: 2608.4477\n",
      "Epoch 210/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 793.6762 - val_loss: 2649.2430\n",
      "Epoch 211/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 704.7086 - val_loss: 2625.3066\n",
      "Epoch 212/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 692.8699 - val_loss: 2610.8618\n",
      "Epoch 213/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 849.7293 - val_loss: 2611.7223\n",
      "Epoch 214/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 693.9694 - val_loss: 2610.8572\n",
      "Epoch 215/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 703.7885 - val_loss: 2622.2911\n",
      "Epoch 216/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 668.2539 - val_loss: 2580.8083\n",
      "Epoch 217/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 654.5334 - val_loss: 2596.1638\n",
      "Epoch 218/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 646.0367 - val_loss: 2521.4908\n",
      "Epoch 219/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 675.0159 - val_loss: 2615.7694\n",
      "Epoch 220/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4023/4023 [==============================] - 0s 92us/step - loss: 644.0122 - val_loss: 2578.5064\n",
      "Epoch 221/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 682.6516 - val_loss: 2611.6990\n",
      "Epoch 222/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 681.4576 - val_loss: 2591.2818\n",
      "Epoch 223/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 648.3637 - val_loss: 2594.0349\n",
      "Epoch 224/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 664.5998 - val_loss: 2569.0394\n",
      "Epoch 225/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 700.0286 - val_loss: 2583.6087\n",
      "Epoch 226/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 632.2030 - val_loss: 2556.6036\n",
      "Epoch 227/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 603.0831 - val_loss: 2534.9719\n",
      "Epoch 228/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 711.1070 - val_loss: 2583.0277\n",
      "Epoch 229/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 651.2305 - val_loss: 2565.0769\n",
      "Epoch 230/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 654.7025 - val_loss: 2569.8351\n",
      "Epoch 231/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 629.4417 - val_loss: 2543.1981\n",
      "Epoch 232/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 658.0099 - val_loss: 2566.8099\n",
      "Epoch 233/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 629.0117 - val_loss: 2522.8283\n",
      "Epoch 234/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 627.2358 - val_loss: 2526.7137\n",
      "Epoch 235/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 604.7770 - val_loss: 2496.8607\n",
      "Epoch 236/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 636.7966 - val_loss: 2533.2777\n",
      "Epoch 237/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 637.9619 - val_loss: 2473.5308\n",
      "Epoch 238/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 644.3912 - val_loss: 2550.4068\n",
      "Epoch 239/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 647.0310 - val_loss: 2486.6928\n",
      "Epoch 240/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 599.7879 - val_loss: 2473.2058\n",
      "Epoch 241/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 612.4446 - val_loss: 2433.1420\n",
      "Epoch 242/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 580.8710 - val_loss: 2516.4083\n",
      "Epoch 243/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 600.6180 - val_loss: 2491.4762\n",
      "Epoch 244/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 594.8054 - val_loss: 2419.9591\n",
      "Epoch 245/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 616.7134 - val_loss: 2528.2608\n",
      "Epoch 246/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 605.4568 - val_loss: 2458.2503\n",
      "Epoch 247/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 576.6760 - val_loss: 2456.7161\n",
      "Epoch 248/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 571.6707 - val_loss: 2431.6245\n",
      "Epoch 249/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 571.0077 - val_loss: 2414.1160\n",
      "Epoch 250/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 554.1893 - val_loss: 2463.7302\n",
      "Epoch 251/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 579.7839 - val_loss: 2399.2400\n",
      "Epoch 252/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 576.7313 - val_loss: 2450.3155\n",
      "Epoch 253/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 595.4956 - val_loss: 2385.3029\n",
      "Epoch 254/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 553.2091 - val_loss: 2407.9727\n",
      "Epoch 255/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 561.3091 - val_loss: 2376.6985\n",
      "Epoch 256/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 546.7617 - val_loss: 2359.1321\n",
      "Epoch 257/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 558.5066 - val_loss: 2431.0943\n",
      "Epoch 258/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 538.7045 - val_loss: 2345.3445\n",
      "Epoch 259/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 523.3298 - val_loss: 2378.9669\n",
      "Epoch 260/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 540.9335 - val_loss: 2351.4186\n",
      "Epoch 261/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 541.2862 - val_loss: 2440.2564\n",
      "Epoch 262/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 552.3028 - val_loss: 2346.0558\n",
      "Epoch 263/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 515.9502 - val_loss: 2352.4746\n",
      "Epoch 264/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 529.3815 - val_loss: 2330.8606\n",
      "Epoch 265/2000\n",
      "4023/4023 [==============================] - 0s 111us/step - loss: 497.6558 - val_loss: 2297.8187\n",
      "Epoch 266/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 521.3379 - val_loss: 2389.7178\n",
      "Epoch 267/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 535.2104 - val_loss: 2317.2390\n",
      "Epoch 268/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 514.5110 - val_loss: 2298.8044\n",
      "Epoch 269/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 524.1501 - val_loss: 2318.4679\n",
      "Epoch 270/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 498.8248 - val_loss: 2320.9828\n",
      "Epoch 271/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 546.3070 - val_loss: 2468.7839\n",
      "Epoch 272/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 619.4282 - val_loss: 2668.8729\n",
      "Epoch 273/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 945.9665 - val_loss: 2601.8590\n",
      "Epoch 274/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 709.4540 - val_loss: 2454.1167\n",
      "Epoch 275/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 668.4046 - val_loss: 2374.8531\n",
      "Epoch 276/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 607.4132 - val_loss: 2313.2801\n",
      "Epoch 277/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 565.0792 - val_loss: 2345.9392\n",
      "Epoch 278/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 540.8070 - val_loss: 2496.2130\n",
      "Epoch 279/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 664.2291 - val_loss: 2444.5692\n",
      "Epoch 280/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 595.1308 - val_loss: 2338.5932\n",
      "Epoch 281/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 605.8257 - val_loss: 2369.4602\n",
      "Epoch 282/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 546.4665 - val_loss: 2302.7770\n",
      "Epoch 283/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 578.5128 - val_loss: 2373.6235\n",
      "Epoch 284/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 575.9685 - val_loss: 2279.9990\n",
      "Epoch 285/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 584.1170 - val_loss: 2269.4091\n",
      "Epoch 286/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 500.7907 - val_loss: 2273.0044\n",
      "Epoch 287/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 548.8664 - val_loss: 2234.4473\n",
      "Epoch 288/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 486.6669 - val_loss: 2225.3073\n",
      "Epoch 289/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 508.0584 - val_loss: 2209.5154\n",
      "Epoch 290/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 490.9957 - val_loss: 2231.0498\n",
      "Epoch 291/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 492.1686 - val_loss: 2196.0214\n",
      "Epoch 292/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 531.1192 - val_loss: 2239.9757\n",
      "Epoch 293/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4023/4023 [==============================] - 0s 95us/step - loss: 495.1099 - val_loss: 2176.8985\n",
      "Epoch 294/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 529.6310 - val_loss: 2231.2625\n",
      "Epoch 295/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 501.5855 - val_loss: 2155.8698\n",
      "Epoch 296/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 471.7454 - val_loss: 2264.6838\n",
      "Epoch 297/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 486.8848 - val_loss: 2131.6582\n",
      "Epoch 298/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 508.8238 - val_loss: 2242.6961\n",
      "Epoch 299/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 463.9626 - val_loss: 2148.3560\n",
      "Epoch 300/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 505.9324 - val_loss: 2183.5018\n",
      "Epoch 301/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 484.8029 - val_loss: 2152.7351\n",
      "Epoch 302/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 510.0492 - val_loss: 2169.7013\n",
      "Epoch 303/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 473.5577 - val_loss: 2119.0448\n",
      "Epoch 304/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 458.4810 - val_loss: 2148.3112\n",
      "Epoch 305/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 473.4985 - val_loss: 2147.2734\n",
      "Epoch 306/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 457.3010 - val_loss: 2128.4045\n",
      "Epoch 307/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 444.3423 - val_loss: 2125.2818\n",
      "Epoch 308/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 451.0522 - val_loss: 2109.2910\n",
      "Epoch 309/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 466.7312 - val_loss: 2141.7575\n",
      "Epoch 310/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 447.7937 - val_loss: 2081.0310\n",
      "Epoch 311/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 462.2049 - val_loss: 2107.2626\n",
      "Epoch 312/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 445.2599 - val_loss: 2066.8037\n",
      "Epoch 313/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 464.4941 - val_loss: 2164.7966\n",
      "Epoch 314/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 451.9412 - val_loss: 2073.5771\n",
      "Epoch 315/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 456.4577 - val_loss: 2100.1410\n",
      "Epoch 316/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 448.9764 - val_loss: 2063.7745\n",
      "Epoch 317/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 459.4264 - val_loss: 2124.9948\n",
      "Epoch 318/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 472.5661 - val_loss: 2036.0822\n",
      "Epoch 319/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 465.4829 - val_loss: 2146.9952\n",
      "Epoch 320/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 471.2357 - val_loss: 2102.0489\n",
      "Epoch 321/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 526.3723 - val_loss: 2179.4258\n",
      "Epoch 322/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 475.5565 - val_loss: 2057.9049\n",
      "Epoch 323/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 461.1188 - val_loss: 2112.1233\n",
      "Epoch 324/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 468.4234 - val_loss: 2037.2953\n",
      "Epoch 325/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 490.6955 - val_loss: 2268.7286\n",
      "Epoch 326/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 535.1733 - val_loss: 2048.5418\n",
      "Epoch 327/2000\n",
      "4023/4023 [==============================] - 0s 108us/step - loss: 456.3472 - val_loss: 2075.5693\n",
      "Epoch 328/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 438.5076 - val_loss: 2041.6220\n",
      "Epoch 329/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 424.0926 - val_loss: 2079.4178\n",
      "Epoch 330/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 401.3269 - val_loss: 2024.6103\n",
      "Epoch 331/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 400.3199 - val_loss: 2061.6093\n",
      "Epoch 332/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 408.7120 - val_loss: 2004.7413\n",
      "Epoch 333/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 418.7522 - val_loss: 2095.1941\n",
      "Epoch 334/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 435.8171 - val_loss: 1990.2455\n",
      "Epoch 335/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 452.7965 - val_loss: 2073.0415\n",
      "Epoch 336/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 430.7189 - val_loss: 2008.7090\n",
      "Epoch 337/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 420.9450 - val_loss: 1990.7816\n",
      "Epoch 338/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 399.4793 - val_loss: 2005.5782\n",
      "Epoch 339/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 384.3907 - val_loss: 1974.5453\n",
      "Epoch 340/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 415.7661 - val_loss: 2032.3672\n",
      "Epoch 341/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 416.5873 - val_loss: 1962.4562\n",
      "Epoch 342/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 460.8969 - val_loss: 2019.2921\n",
      "Epoch 343/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 408.8766 - val_loss: 1983.3524\n",
      "Epoch 344/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 437.5568 - val_loss: 2017.7312\n",
      "Epoch 345/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 386.4501 - val_loss: 1984.4087\n",
      "Epoch 346/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 399.5094 - val_loss: 2103.4611\n",
      "Epoch 347/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 446.0295 - val_loss: 2079.1036\n",
      "Epoch 348/2000\n",
      "4023/4023 [==============================] - 0s 105us/step - loss: 509.5429 - val_loss: 2190.2316\n",
      "Epoch 349/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 509.7120 - val_loss: 2398.6761\n",
      "Epoch 350/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 629.9539 - val_loss: 2312.7236\n",
      "Epoch 351/2000\n",
      "4023/4023 [==============================] - 0s 106us/step - loss: 544.5290 - val_loss: 2191.7114\n",
      "Epoch 352/2000\n",
      "4023/4023 [==============================] - 0s 105us/step - loss: 496.7786 - val_loss: 2245.7808\n",
      "Epoch 353/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 459.3624 - val_loss: 2594.0108\n",
      "Epoch 354/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 685.5654 - val_loss: 2500.6171\n",
      "Epoch 355/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 655.6885 - val_loss: 2595.2181\n",
      "Epoch 356/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 856.8295 - val_loss: 2520.2722\n",
      "Epoch 357/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 679.4670 - val_loss: 2439.7219\n",
      "Epoch 358/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 617.7949 - val_loss: 2401.9305\n",
      "Epoch 359/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 609.9883 - val_loss: 2272.8544\n",
      "Epoch 360/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 587.5613 - val_loss: 2263.8212\n",
      "Epoch 361/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 596.1451 - val_loss: 2212.0491\n",
      "Epoch 362/2000\n",
      "4023/4023 [==============================] - 0s 111us/step - loss: 533.3362 - val_loss: 2069.1491\n",
      "Epoch 363/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 638.6623 - val_loss: 2339.5379\n",
      "Epoch 364/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 559.0028 - val_loss: 2126.3399\n",
      "Epoch 365/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 502.5133 - val_loss: 2109.3234\n",
      "Epoch 366/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4023/4023 [==============================] - 0s 96us/step - loss: 447.2029 - val_loss: 2074.9558\n",
      "Epoch 367/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 516.0782 - val_loss: 2302.3721\n",
      "Epoch 368/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 613.3518 - val_loss: 2052.3476\n",
      "Epoch 369/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 493.0197 - val_loss: 2024.7088\n",
      "Epoch 370/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 419.6790 - val_loss: 2037.9033\n",
      "Epoch 371/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 483.5960 - val_loss: 1958.6750\n",
      "Epoch 372/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 465.8299 - val_loss: 1976.4790\n",
      "Epoch 373/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 450.9475 - val_loss: 1981.0558\n",
      "Epoch 374/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 448.1001 - val_loss: 2089.3558\n",
      "Epoch 375/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 539.8351 - val_loss: 2041.3794\n",
      "Epoch 376/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 476.9549 - val_loss: 1973.7327\n",
      "Epoch 377/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 516.6031 - val_loss: 1978.0730\n",
      "Epoch 378/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 501.2563 - val_loss: 1970.7192\n",
      "Epoch 379/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 446.7910 - val_loss: 2040.9306\n",
      "Epoch 380/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 535.1451 - val_loss: 2413.9541\n",
      "Epoch 381/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 690.9989 - val_loss: 2042.2018\n",
      "Epoch 382/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 477.7427 - val_loss: 1933.1671\n",
      "Epoch 383/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 463.0583 - val_loss: 1970.5959\n",
      "Epoch 384/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 480.2912 - val_loss: 2008.5715\n",
      "Epoch 385/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 500.9350 - val_loss: 2146.6977\n",
      "Epoch 386/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 549.6873 - val_loss: 2180.6393\n",
      "Epoch 387/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 731.5318 - val_loss: 2374.5341\n",
      "Epoch 388/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 581.2698 - val_loss: 2127.9496\n",
      "Epoch 389/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 563.0085 - val_loss: 2151.9277\n",
      "Epoch 390/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 465.1121 - val_loss: 2115.9865\n",
      "Epoch 391/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 491.9553 - val_loss: 2086.6723\n",
      "Epoch 392/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 487.1701 - val_loss: 2017.3476\n",
      "Epoch 393/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 448.4849 - val_loss: 1937.8218\n",
      "Epoch 394/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 430.0533 - val_loss: 2008.5210\n",
      "Epoch 395/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 471.7384 - val_loss: 1938.9299\n",
      "Epoch 396/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 403.2320 - val_loss: 1883.2585\n",
      "Epoch 397/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 445.2017 - val_loss: 2068.6579\n",
      "Epoch 398/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 515.1998 - val_loss: 1924.0315\n",
      "Epoch 399/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 452.0432 - val_loss: 1903.5286\n",
      "Epoch 400/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 431.8293 - val_loss: 1904.3261\n",
      "Epoch 401/2000\n",
      "4023/4023 [==============================] - 0s 108us/step - loss: 461.5013 - val_loss: 1914.6601\n",
      "Epoch 402/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 453.0065 - val_loss: 1976.1942\n",
      "Epoch 403/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 466.1678 - val_loss: 1873.5804\n",
      "Epoch 404/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 419.8136 - val_loss: 1916.6066\n",
      "Epoch 405/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 393.7005 - val_loss: 1915.3279\n",
      "Epoch 406/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 439.2834 - val_loss: 2015.7973\n",
      "Epoch 407/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 490.1062 - val_loss: 1837.0711\n",
      "Epoch 408/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 447.0285 - val_loss: 1898.8440\n",
      "Epoch 409/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 412.2769 - val_loss: 1835.9623\n",
      "Epoch 410/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 401.8971 - val_loss: 1873.5095\n",
      "Epoch 411/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 426.3302 - val_loss: 1842.1523\n",
      "Epoch 412/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 396.4302 - val_loss: 1971.8830\n",
      "Epoch 413/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 446.7379 - val_loss: 1851.0924\n",
      "Epoch 414/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 389.0997 - val_loss: 1889.1253\n",
      "Epoch 415/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 420.9722 - val_loss: 1782.6331\n",
      "Epoch 416/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 419.6969 - val_loss: 2005.4431\n",
      "Epoch 417/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 493.2948 - val_loss: 1832.3039\n",
      "Epoch 418/2000\n",
      "4023/4023 [==============================] - 0s 122us/step - loss: 488.1584 - val_loss: 1898.6877\n",
      "Epoch 419/2000\n",
      "4023/4023 [==============================] - 0s 105us/step - loss: 507.8922 - val_loss: 1962.1404\n",
      "Epoch 420/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 440.5398 - val_loss: 1917.5474\n",
      "Epoch 421/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 475.6465 - val_loss: 2051.4044\n",
      "Epoch 422/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 471.6607 - val_loss: 1880.2278\n",
      "Epoch 423/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 489.3201 - val_loss: 1958.2361\n",
      "Epoch 424/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 452.9853 - val_loss: 1903.0601\n",
      "Epoch 425/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 435.6185 - val_loss: 1892.7657\n",
      "Epoch 426/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 384.5754 - val_loss: 1951.1246\n",
      "Epoch 427/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 493.4434 - val_loss: 1969.2467\n",
      "Epoch 428/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 441.9721 - val_loss: 1867.8916\n",
      "Epoch 429/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 410.3576 - val_loss: 1943.7569\n",
      "Epoch 430/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 404.5982 - val_loss: 1805.3358\n",
      "Epoch 431/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 419.1717 - val_loss: 1909.2917\n",
      "Epoch 432/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 445.5278 - val_loss: 1997.2576\n",
      "Epoch 433/2000\n",
      "4023/4023 [==============================] - 0s 107us/step - loss: 548.7363 - val_loss: 1940.9038\n",
      "Epoch 434/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 444.8657 - val_loss: 1860.8818\n",
      "Epoch 435/2000\n",
      "4023/4023 [==============================] - 0s 107us/step - loss: 389.9874 - val_loss: 1873.4253\n",
      "Epoch 436/2000\n",
      "4023/4023 [==============================] - 0s 106us/step - loss: 385.0789 - val_loss: 1822.0488\n",
      "Epoch 437/2000\n",
      "4023/4023 [==============================] - 0s 106us/step - loss: 413.1397 - val_loss: 1892.6661\n",
      "Epoch 438/2000\n",
      "4023/4023 [==============================] - 0s 110us/step - loss: 464.3465 - val_loss: 1954.5612\n",
      "Epoch 439/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4023/4023 [==============================] - 0s 105us/step - loss: 484.5271 - val_loss: 1876.9681\n",
      "Epoch 440/2000\n",
      "4023/4023 [==============================] - 0s 110us/step - loss: 456.0576 - val_loss: 1914.4036\n",
      "Epoch 441/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 500.1911 - val_loss: 1963.3940\n",
      "Epoch 442/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 448.0377 - val_loss: 1796.9631\n",
      "Epoch 443/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 420.0330 - val_loss: 1861.0696\n",
      "Epoch 444/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 382.1262 - val_loss: 1811.2972\n",
      "Epoch 445/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 428.7062 - val_loss: 2015.1267\n",
      "Epoch 446/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 503.6241 - val_loss: 1919.8198\n",
      "Epoch 447/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 414.6544 - val_loss: 1849.9504\n",
      "Epoch 448/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 409.7301 - val_loss: 1816.6663\n",
      "Epoch 449/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 434.7269 - val_loss: 1818.1239\n",
      "Epoch 450/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 369.4723 - val_loss: 1798.6852\n",
      "Epoch 451/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 347.7130 - val_loss: 1778.0975\n",
      "Epoch 452/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 420.5001 - val_loss: 1822.6831\n",
      "Epoch 453/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 382.2767 - val_loss: 1919.7142\n",
      "Epoch 454/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 531.7733 - val_loss: 2005.4991\n",
      "Epoch 455/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 514.2417 - val_loss: 2237.3883\n",
      "Epoch 456/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 640.9966 - val_loss: 2107.2178\n",
      "Epoch 457/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 559.5911 - val_loss: 2005.4178\n",
      "Epoch 458/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 487.7251 - val_loss: 1963.0330\n",
      "Epoch 459/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 464.5406 - val_loss: 1883.4616\n",
      "Epoch 460/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 477.4197 - val_loss: 1900.6932\n",
      "Epoch 461/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 462.7636 - val_loss: 1869.5954\n",
      "Epoch 462/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 414.8736 - val_loss: 1800.7097\n",
      "Epoch 463/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 425.4845 - val_loss: 1787.1127\n",
      "Epoch 464/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 461.1604 - val_loss: 1841.9910\n",
      "Epoch 465/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 472.0533 - val_loss: 1911.9941\n",
      "Epoch 466/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 444.4837 - val_loss: 1787.0167\n",
      "Epoch 467/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 455.9331 - val_loss: 1862.7640\n",
      "Epoch 468/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 395.5733 - val_loss: 1784.2696\n",
      "Epoch 469/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 400.3756 - val_loss: 1899.5205\n",
      "Epoch 470/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 456.2344 - val_loss: 1804.2693\n",
      "Epoch 471/2000\n",
      "4023/4023 [==============================] - 0s 105us/step - loss: 446.5639 - val_loss: 1760.3241\n",
      "Epoch 472/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 468.5947 - val_loss: 1764.8251\n",
      "Epoch 473/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 493.8951 - val_loss: 1847.4630\n",
      "Epoch 474/2000\n",
      "4023/4023 [==============================] - 0s 105us/step - loss: 433.7509 - val_loss: 1721.2760\n",
      "Epoch 475/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 477.7830 - val_loss: 1809.4786\n",
      "Epoch 476/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 396.7628 - val_loss: 1757.6718\n",
      "Epoch 477/2000\n",
      "4023/4023 [==============================] - 0s 106us/step - loss: 442.7897 - val_loss: 1880.2726\n",
      "Epoch 478/2000\n",
      "4023/4023 [==============================] - 0s 106us/step - loss: 473.5614 - val_loss: 1827.9796\n",
      "Epoch 479/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 418.4471 - val_loss: 1887.0688\n",
      "Epoch 480/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 427.9259 - val_loss: 1766.4002\n",
      "Epoch 481/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 430.7027 - val_loss: 1968.2187\n",
      "Epoch 482/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 475.8273 - val_loss: 1730.7266\n",
      "Epoch 483/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 391.6136 - val_loss: 1935.9915\n",
      "Epoch 484/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 398.8270 - val_loss: 1770.8498\n",
      "Epoch 485/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 423.4060 - val_loss: 1829.7612\n",
      "Epoch 486/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 447.4418 - val_loss: 1745.1544\n",
      "Epoch 487/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 358.2417 - val_loss: 1769.3962\n",
      "Epoch 488/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 405.5622 - val_loss: 1750.2795\n",
      "Epoch 489/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 424.5838 - val_loss: 1758.5337\n",
      "Epoch 490/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 457.3143 - val_loss: 1738.6622\n",
      "Epoch 491/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 376.5902 - val_loss: 1682.5041\n",
      "Epoch 492/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 379.0371 - val_loss: 1744.6394\n",
      "Epoch 493/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 369.1421 - val_loss: 1651.3662\n",
      "Epoch 494/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 409.7142 - val_loss: 1981.0713\n",
      "Epoch 495/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 480.8070 - val_loss: 1721.8013\n",
      "Epoch 496/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 495.0930 - val_loss: 1856.9786\n",
      "Epoch 497/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 409.5372 - val_loss: 1805.9049\n",
      "Epoch 498/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 490.0883 - val_loss: 2004.6049\n",
      "Epoch 499/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 533.0736 - val_loss: 1822.0177\n",
      "Epoch 500/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 524.3328 - val_loss: 1903.6493\n",
      "Epoch 501/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 438.9569 - val_loss: 1924.7618\n",
      "Epoch 502/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 570.7603 - val_loss: 2086.4846\n",
      "Epoch 503/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 513.9318 - val_loss: 1909.8426\n",
      "Epoch 504/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 432.0913 - val_loss: 1933.8253\n",
      "Epoch 505/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 430.1667 - val_loss: 1875.5234\n",
      "Epoch 506/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 443.5646 - val_loss: 1999.7445\n",
      "Epoch 507/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 464.3847 - val_loss: 1862.6038\n",
      "Epoch 508/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 446.6527 - val_loss: 1843.8402\n",
      "Epoch 509/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 434.3083 - val_loss: 1855.6945\n",
      "Epoch 510/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 415.6263 - val_loss: 1769.4297\n",
      "Epoch 511/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 450.5855 - val_loss: 1846.3362\n",
      "Epoch 512/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4023/4023 [==============================] - 0s 99us/step - loss: 402.5539 - val_loss: 1931.2928\n",
      "Epoch 513/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 562.6079 - val_loss: 1995.2930\n",
      "Epoch 514/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 462.8223 - val_loss: 1911.3691\n",
      "Epoch 515/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 441.7081 - val_loss: 1879.2134\n",
      "Epoch 516/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 452.0168 - val_loss: 1817.2437\n",
      "Epoch 517/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 398.2002 - val_loss: 1789.2982\n",
      "Epoch 518/2000\n",
      "4023/4023 [==============================] - 0s 107us/step - loss: 430.3642 - val_loss: 1768.9819\n",
      "Epoch 519/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 449.3390 - val_loss: 1922.3818\n",
      "Epoch 520/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 500.6191 - val_loss: 1800.9419\n",
      "Epoch 521/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 448.9404 - val_loss: 1739.3967\n",
      "Epoch 522/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 408.8327 - val_loss: 1833.0155\n",
      "Epoch 523/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 459.5025 - val_loss: 1701.5789\n",
      "Epoch 524/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 414.2253 - val_loss: 1810.1706\n",
      "Epoch 525/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 407.8035 - val_loss: 1665.6866\n",
      "Epoch 526/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 429.6805 - val_loss: 2020.9286\n",
      "Epoch 527/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 509.9229 - val_loss: 1715.3140\n",
      "Epoch 528/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 440.4961 - val_loss: 1893.9619\n",
      "Epoch 529/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 442.3768 - val_loss: 1649.3394\n",
      "Epoch 530/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 392.0166 - val_loss: 1752.3150\n",
      "Epoch 531/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 417.1474 - val_loss: 1788.8658\n",
      "Epoch 532/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 399.5464 - val_loss: 1797.4910\n",
      "Epoch 533/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 470.7638 - val_loss: 1638.3153\n",
      "Epoch 534/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 365.2323 - val_loss: 1887.2905\n",
      "Epoch 535/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 443.9216 - val_loss: 1672.2545\n",
      "Epoch 536/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 451.3795 - val_loss: 1771.1852\n",
      "Epoch 537/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 428.5707 - val_loss: 1723.9713\n",
      "Epoch 538/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 440.3507 - val_loss: 1820.3161\n",
      "Epoch 539/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 450.3654 - val_loss: 1722.0476\n",
      "Epoch 540/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 449.6639 - val_loss: 1878.2843\n",
      "Epoch 541/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 447.6387 - val_loss: 1706.9583\n",
      "Epoch 542/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 375.3779 - val_loss: 1691.9788\n",
      "Epoch 543/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 350.3450 - val_loss: 1705.9465\n",
      "Epoch 544/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 419.7255 - val_loss: 1710.2439\n",
      "Epoch 545/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 412.8195 - val_loss: 1878.3830\n",
      "Epoch 546/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 441.1880 - val_loss: 1626.7277\n",
      "Epoch 547/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 393.0842 - val_loss: 1845.5114\n",
      "Epoch 548/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 473.8785 - val_loss: 1690.9638\n",
      "Epoch 549/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 423.5738 - val_loss: 1678.0889\n",
      "Epoch 550/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 363.9990 - val_loss: 1670.5292\n",
      "Epoch 551/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 388.4136 - val_loss: 1638.3622\n",
      "Epoch 552/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 449.7325 - val_loss: 1837.7010\n",
      "Epoch 553/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 402.1897 - val_loss: 1649.1705\n",
      "Epoch 554/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 388.9724 - val_loss: 1676.0309\n",
      "Epoch 555/2000\n",
      "4023/4023 [==============================] - 0s 109us/step - loss: 408.6471 - val_loss: 1653.6463\n",
      "Epoch 556/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 413.6073 - val_loss: 1766.4701\n",
      "Epoch 557/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 416.1083 - val_loss: 1773.0342\n",
      "Epoch 558/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 428.6348 - val_loss: 1663.7998\n",
      "Epoch 559/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 380.0354 - val_loss: 1603.0031\n",
      "Epoch 560/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 398.5713 - val_loss: 1747.3063\n",
      "Epoch 561/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 390.7520 - val_loss: 1601.8323\n",
      "Epoch 562/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 372.1633 - val_loss: 1905.9917\n",
      "Epoch 563/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 480.3228 - val_loss: 1631.2151\n",
      "Epoch 564/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 371.2667 - val_loss: 1805.5388\n",
      "Epoch 565/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 442.9867 - val_loss: 1632.0321\n",
      "Epoch 566/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 396.8379 - val_loss: 1686.3734\n",
      "Epoch 567/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 411.8965 - val_loss: 1630.5345\n",
      "Epoch 568/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 348.5246 - val_loss: 1616.7207\n",
      "Epoch 569/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 315.5489 - val_loss: 1781.2149\n",
      "Epoch 570/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 391.9450 - val_loss: 1592.3210\n",
      "Epoch 571/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 361.6449 - val_loss: 1719.9202\n",
      "Epoch 572/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 385.4710 - val_loss: 1821.2165\n",
      "Epoch 573/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 593.1727 - val_loss: 1746.1631\n",
      "Epoch 574/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 416.5982 - val_loss: 1577.4052\n",
      "Epoch 575/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 387.3692 - val_loss: 1673.8356\n",
      "Epoch 576/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 371.3299 - val_loss: 1604.5030\n",
      "Epoch 577/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 363.3258 - val_loss: 1706.0385\n",
      "Epoch 578/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 414.0818 - val_loss: 1677.3255\n",
      "Epoch 579/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 386.7698 - val_loss: 1636.5614\n",
      "Epoch 580/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 369.8342 - val_loss: 1713.6144\n",
      "Epoch 581/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 406.5180 - val_loss: 1650.8587\n",
      "Epoch 582/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 340.4251 - val_loss: 1594.1562\n",
      "Epoch 583/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 358.0291 - val_loss: 1623.9740\n",
      "Epoch 584/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 333.1006 - val_loss: 1568.4928\n",
      "Epoch 585/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4023/4023 [==============================] - 0s 92us/step - loss: 375.4769 - val_loss: 1653.6240\n",
      "Epoch 586/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 363.6813 - val_loss: 1647.8928\n",
      "Epoch 587/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 451.1870 - val_loss: 1970.0904\n",
      "Epoch 588/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 558.1884 - val_loss: 1680.2015\n",
      "Epoch 589/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 620.8919 - val_loss: 1639.9867\n",
      "Epoch 590/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 339.4166 - val_loss: 1563.6948\n",
      "Epoch 591/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 399.8117 - val_loss: 1677.5605\n",
      "Epoch 592/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 440.9466 - val_loss: 1567.5495\n",
      "Epoch 593/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 366.6332 - val_loss: 1589.8031\n",
      "Epoch 594/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 319.8466 - val_loss: 1613.0066\n",
      "Epoch 595/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 380.1846 - val_loss: 1575.3859\n",
      "Epoch 596/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 370.9663 - val_loss: 1593.8073\n",
      "Epoch 597/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 313.6723 - val_loss: 1556.8330\n",
      "Epoch 598/2000\n",
      "4023/4023 [==============================] - 0s 105us/step - loss: 384.5143 - val_loss: 1565.3639\n",
      "Epoch 599/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 342.4585 - val_loss: 1678.5946\n",
      "Epoch 600/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 391.4620 - val_loss: 1677.6201\n",
      "Epoch 601/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 346.6721 - val_loss: 1585.0300\n",
      "Epoch 602/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 352.5977 - val_loss: 1637.0141\n",
      "Epoch 603/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 324.4157 - val_loss: 1553.9619\n",
      "Epoch 604/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 409.3172 - val_loss: 1626.6776\n",
      "Epoch 605/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 317.9670 - val_loss: 1534.4493\n",
      "Epoch 606/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 331.2993 - val_loss: 1577.4598\n",
      "Epoch 607/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 340.2917 - val_loss: 1569.7148\n",
      "Epoch 608/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 341.8923 - val_loss: 1644.9591\n",
      "Epoch 609/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 430.0149 - val_loss: 1574.3513\n",
      "Epoch 610/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 328.0591 - val_loss: 1544.6400\n",
      "Epoch 611/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 386.9422 - val_loss: 1602.2864\n",
      "Epoch 612/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 350.8203 - val_loss: 1544.5586\n",
      "Epoch 613/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 360.2705 - val_loss: 1608.4747\n",
      "Epoch 614/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 338.1463 - val_loss: 1543.9772\n",
      "Epoch 615/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 361.5782 - val_loss: 1569.9137\n",
      "Epoch 616/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 366.2725 - val_loss: 1630.5992\n",
      "Epoch 617/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 383.5020 - val_loss: 1635.1688\n",
      "Epoch 618/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 372.4897 - val_loss: 1544.6722\n",
      "Epoch 619/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 375.0758 - val_loss: 1683.3726\n",
      "Epoch 620/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 356.7974 - val_loss: 1554.2187\n",
      "Epoch 621/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 346.1015 - val_loss: 1556.8624\n",
      "Epoch 622/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 384.0307 - val_loss: 1595.1638\n",
      "Epoch 623/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 387.2376 - val_loss: 1646.5272\n",
      "Epoch 624/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 410.9772 - val_loss: 1534.6741\n",
      "Epoch 625/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 389.2272 - val_loss: 1566.3250\n",
      "Epoch 626/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 367.5587 - val_loss: 1498.4876\n",
      "Epoch 627/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 423.2754 - val_loss: 1588.7261\n",
      "Epoch 628/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 322.4976 - val_loss: 1506.5398\n",
      "Epoch 629/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 388.2154 - val_loss: 1556.5803\n",
      "Epoch 630/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 346.2389 - val_loss: 1527.9300\n",
      "Epoch 631/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 384.9374 - val_loss: 1546.1918\n",
      "Epoch 632/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 357.6657 - val_loss: 1511.4985\n",
      "Epoch 633/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 351.8206 - val_loss: 1523.4952\n",
      "Epoch 634/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 359.2162 - val_loss: 1565.7438\n",
      "Epoch 635/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 384.3234 - val_loss: 1787.5113\n",
      "Epoch 636/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 401.1626 - val_loss: 1455.3978\n",
      "Epoch 637/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 390.4089 - val_loss: 1591.5921\n",
      "Epoch 638/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 372.2582 - val_loss: 1482.6299\n",
      "Epoch 639/2000\n",
      "4023/4023 [==============================] - 0s 105us/step - loss: 389.2440 - val_loss: 1651.6602\n",
      "Epoch 640/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 425.2765 - val_loss: 1583.6461\n",
      "Epoch 641/2000\n",
      "4023/4023 [==============================] - 0s 105us/step - loss: 459.3113 - val_loss: 1859.7950\n",
      "Epoch 642/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 499.7334 - val_loss: 1520.0360\n",
      "Epoch 643/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 432.3550 - val_loss: 1608.2024\n",
      "Epoch 644/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 353.3751 - val_loss: 1563.9490\n",
      "Epoch 645/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 404.4913 - val_loss: 1592.1774\n",
      "Epoch 646/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 372.6552 - val_loss: 1520.2210\n",
      "Epoch 647/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 402.4745 - val_loss: 1595.3274\n",
      "Epoch 648/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 352.9387 - val_loss: 1507.0333\n",
      "Epoch 649/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 349.3631 - val_loss: 1542.3299\n",
      "Epoch 650/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 339.3221 - val_loss: 1529.8601\n",
      "Epoch 651/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 344.7344 - val_loss: 1755.6584\n",
      "Epoch 652/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 412.0931 - val_loss: 1546.2658\n",
      "Epoch 653/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 307.7363 - val_loss: 1536.5862\n",
      "Epoch 654/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 332.3150 - val_loss: 1519.8003\n",
      "Epoch 655/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 343.3158 - val_loss: 1563.2631\n",
      "Epoch 656/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 382.0908 - val_loss: 1495.0911\n",
      "Epoch 657/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 327.6066 - val_loss: 1520.7911\n",
      "Epoch 658/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4023/4023 [==============================] - 0s 91us/step - loss: 306.7510 - val_loss: 1502.0184\n",
      "Epoch 659/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 320.7073 - val_loss: 1756.6202\n",
      "Epoch 660/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 415.3583 - val_loss: 1538.7754\n",
      "Epoch 661/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 376.1979 - val_loss: 1694.3501\n",
      "Epoch 662/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 412.6588 - val_loss: 1540.0486\n",
      "Epoch 663/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 368.3412 - val_loss: 1647.0108\n",
      "Epoch 664/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 372.3855 - val_loss: 1525.9021\n",
      "Epoch 665/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 365.8585 - val_loss: 1599.8221\n",
      "Epoch 666/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 404.4199 - val_loss: 1536.7950\n",
      "Epoch 667/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 409.0706 - val_loss: 1602.9993\n",
      "Epoch 668/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 327.6485 - val_loss: 1500.9412\n",
      "Epoch 669/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 346.3177 - val_loss: 1724.0233\n",
      "Epoch 670/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 419.9096 - val_loss: 1500.0546\n",
      "Epoch 671/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 353.9670 - val_loss: 1609.2547\n",
      "Epoch 672/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 351.1110 - val_loss: 1675.0836\n",
      "Epoch 673/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 475.1757 - val_loss: 1558.7208\n",
      "Epoch 674/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 336.3252 - val_loss: 1545.8869\n",
      "Epoch 675/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 307.8897 - val_loss: 1522.6924\n",
      "Epoch 676/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 347.1120 - val_loss: 1493.7392\n",
      "Epoch 677/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 371.4270 - val_loss: 1565.1109\n",
      "Epoch 678/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 346.5684 - val_loss: 1531.1883\n",
      "Epoch 679/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 386.2580 - val_loss: 1582.8526\n",
      "Epoch 680/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 348.9084 - val_loss: 1493.6772\n",
      "Epoch 681/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 361.8013 - val_loss: 1771.0937\n",
      "Epoch 682/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 441.9797 - val_loss: 1500.2087\n",
      "Epoch 683/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 387.3225 - val_loss: 1762.6681\n",
      "Epoch 684/2000\n",
      "4023/4023 [==============================] - 0s 111us/step - loss: 444.2127 - val_loss: 1515.7433\n",
      "Epoch 685/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 394.8380 - val_loss: 1536.7925\n",
      "Epoch 686/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 332.1625 - val_loss: 1506.9450\n",
      "Epoch 687/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 348.2749 - val_loss: 1538.5744\n",
      "Epoch 688/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 333.6106 - val_loss: 1546.9630\n",
      "Epoch 689/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 356.7139 - val_loss: 1562.9703\n",
      "Epoch 690/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 378.7950 - val_loss: 1513.5084\n",
      "Epoch 691/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 313.2073 - val_loss: 1521.7245\n",
      "Epoch 692/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 334.1660 - val_loss: 1732.1327\n",
      "Epoch 693/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 447.0907 - val_loss: 1522.7673\n",
      "Epoch 694/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 344.7840 - val_loss: 1506.2331\n",
      "Epoch 695/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 358.8456 - val_loss: 1679.0305\n",
      "Epoch 696/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 362.6223 - val_loss: 1500.0818\n",
      "Epoch 697/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 403.6182 - val_loss: 1818.4692\n",
      "Epoch 698/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 457.6564 - val_loss: 1515.8366\n",
      "Epoch 699/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 361.0856 - val_loss: 1555.2002\n",
      "Epoch 700/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 365.6018 - val_loss: 1478.7963\n",
      "Epoch 701/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 333.5061 - val_loss: 1756.2292\n",
      "Epoch 702/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 428.9694 - val_loss: 1493.2867\n",
      "Epoch 703/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 405.0927 - val_loss: 1561.4123\n",
      "Epoch 704/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 316.8659 - val_loss: 1486.9313\n",
      "Epoch 705/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 381.8790 - val_loss: 1568.0335\n",
      "Epoch 706/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 377.1112 - val_loss: 1559.1759\n",
      "Epoch 707/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 352.9044 - val_loss: 1568.5848\n",
      "Epoch 708/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 370.2492 - val_loss: 1469.7269\n",
      "Epoch 709/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 399.6951 - val_loss: 1572.4915\n",
      "Epoch 710/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 334.6381 - val_loss: 1474.7577\n",
      "Epoch 711/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 370.9695 - val_loss: 1580.0174\n",
      "Epoch 712/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 350.7397 - val_loss: 1572.4842\n",
      "Epoch 713/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 385.3658 - val_loss: 1701.9423\n",
      "Epoch 714/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 396.6628 - val_loss: 1608.5995\n",
      "Epoch 715/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 374.6439 - val_loss: 1595.5131\n",
      "Epoch 716/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 316.1401 - val_loss: 1555.2810\n",
      "Epoch 717/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 399.0687 - val_loss: 1595.0778\n",
      "Epoch 718/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 356.7508 - val_loss: 1541.0704\n",
      "Epoch 719/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 408.0284 - val_loss: 1614.9866\n",
      "Epoch 720/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 377.2759 - val_loss: 1539.8735\n",
      "Epoch 721/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 351.1941 - val_loss: 1530.4939\n",
      "Epoch 722/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 350.8333 - val_loss: 1520.9030\n",
      "Epoch 723/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 320.7163 - val_loss: 1591.2015\n",
      "Epoch 724/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 347.7076 - val_loss: 1510.1397\n",
      "Epoch 725/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 366.5995 - val_loss: 1635.9222\n",
      "Epoch 726/2000\n",
      "4023/4023 [==============================] - 0s 107us/step - loss: 419.4458 - val_loss: 1772.7124\n",
      "Epoch 727/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 571.2797 - val_loss: 1852.4041\n",
      "Epoch 728/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 517.9943 - val_loss: 1761.9682\n",
      "Epoch 729/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 567.1440 - val_loss: 1783.0127\n",
      "Epoch 730/2000\n",
      "4023/4023 [==============================] - 0s 106us/step - loss: 551.1884 - val_loss: 1794.2031\n",
      "Epoch 731/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4023/4023 [==============================] - 0s 101us/step - loss: 500.1039 - val_loss: 1832.9301\n",
      "Epoch 732/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 538.4637 - val_loss: 1692.7887\n",
      "Epoch 733/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 555.3651 - val_loss: 1892.9577\n",
      "Epoch 734/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 542.9848 - val_loss: 1645.8483\n",
      "Epoch 735/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 540.2980 - val_loss: 1823.7102\n",
      "Epoch 736/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 534.7185 - val_loss: 1796.0124\n",
      "Epoch 737/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 629.7474 - val_loss: 2028.1916\n",
      "Epoch 738/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 752.6149 - val_loss: 1861.5197\n",
      "Epoch 739/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 657.9792 - val_loss: 2069.8844\n",
      "Epoch 740/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 667.4360 - val_loss: 2065.8938\n",
      "Epoch 741/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 653.1527 - val_loss: 2036.0443\n",
      "Epoch 742/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 575.8485 - val_loss: 1950.6971\n",
      "Epoch 743/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 559.1302 - val_loss: 1866.9189\n",
      "Epoch 744/2000\n",
      "4023/4023 [==============================] - ETA: 0s - loss: 566.944 - 0s 91us/step - loss: 502.5921 - val_loss: 1868.0782\n",
      "Epoch 745/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 506.2909 - val_loss: 1814.2408\n",
      "Epoch 746/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 526.2784 - val_loss: 1811.4038\n",
      "Epoch 747/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 489.1731 - val_loss: 1803.8747\n",
      "Epoch 748/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 520.6709 - val_loss: 1802.4809\n",
      "Epoch 749/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 474.6146 - val_loss: 1777.2313\n",
      "Epoch 750/2000\n",
      "4023/4023 [==============================] - 0s 107us/step - loss: 502.4445 - val_loss: 1773.5334\n",
      "Epoch 751/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 523.8288 - val_loss: 1773.3070\n",
      "Epoch 752/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 516.9243 - val_loss: 1703.3625\n",
      "Epoch 753/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 457.9535 - val_loss: 1943.7764\n",
      "Epoch 754/2000\n",
      "4023/4023 [==============================] - 0s 106us/step - loss: 598.4228 - val_loss: 1914.9497\n",
      "Epoch 755/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 619.0794 - val_loss: 1888.0461\n",
      "Epoch 756/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 565.1732 - val_loss: 1926.5912\n",
      "Epoch 757/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 537.6588 - val_loss: 1847.8570\n",
      "Epoch 758/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 508.7215 - val_loss: 1907.6825\n",
      "Epoch 759/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 536.3815 - val_loss: 1838.5071\n",
      "Epoch 760/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 514.8849 - val_loss: 1794.1630\n",
      "Epoch 761/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 544.2809 - val_loss: 1783.4769\n",
      "Epoch 762/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 479.6241 - val_loss: 1771.6375\n",
      "Epoch 763/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 475.9348 - val_loss: 1789.4584\n",
      "Epoch 764/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 471.8212 - val_loss: 1847.3094\n",
      "Epoch 765/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 530.1946 - val_loss: 1931.3937\n",
      "Epoch 766/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 591.4191 - val_loss: 1689.2699\n",
      "Epoch 767/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 615.2707 - val_loss: 1797.7762\n",
      "Epoch 768/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 569.2752 - val_loss: 1766.4122\n",
      "Epoch 769/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 512.8021 - val_loss: 1797.0273\n",
      "Epoch 770/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 489.0702 - val_loss: 1798.5211\n",
      "Epoch 771/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 560.1686 - val_loss: 1820.9384\n",
      "Epoch 772/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 516.1068 - val_loss: 1837.1879\n",
      "Epoch 773/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 487.2997 - val_loss: 1818.0367\n",
      "Epoch 774/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 499.0844 - val_loss: 1837.7918\n",
      "Epoch 775/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 503.3371 - val_loss: 1835.6153\n",
      "Epoch 776/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 496.6595 - val_loss: 1822.3877\n",
      "Epoch 777/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 470.1007 - val_loss: 1864.1282\n",
      "Epoch 778/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 485.8390 - val_loss: 1871.5062\n",
      "Epoch 779/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 491.6880 - val_loss: 1852.2613\n",
      "Epoch 780/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 510.2632 - val_loss: 1856.9671\n",
      "Epoch 781/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 528.9794 - val_loss: 1800.9104\n",
      "Epoch 782/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 470.7994 - val_loss: 1870.7746\n",
      "Epoch 783/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 478.0527 - val_loss: 1734.9223\n",
      "Epoch 784/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 482.2173 - val_loss: 1925.6728\n",
      "Epoch 785/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 471.6898 - val_loss: 1836.6507\n",
      "Epoch 786/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 599.9012 - val_loss: 1954.8105\n",
      "Epoch 787/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 533.1987 - val_loss: 1869.6564\n",
      "Epoch 788/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 507.8957 - val_loss: 1808.1072\n",
      "Epoch 789/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 532.9301 - val_loss: 1886.5543\n",
      "Epoch 790/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 486.5017 - val_loss: 1804.2856\n",
      "Epoch 791/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 526.7635 - val_loss: 1924.4901\n",
      "Epoch 792/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 564.0106 - val_loss: 1870.7744\n",
      "Epoch 793/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 522.2307 - val_loss: 1872.2358\n",
      "Epoch 794/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 508.8845 - val_loss: 1844.2395\n",
      "Epoch 795/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 508.7867 - val_loss: 1845.5985\n",
      "Epoch 796/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 537.7971 - val_loss: 1862.5953\n",
      "Epoch 797/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 485.6858 - val_loss: 1808.3176\n",
      "Epoch 798/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 394.2881 - val_loss: 2144.8287\n",
      "Epoch 799/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 536.1743 - val_loss: 2137.2445\n",
      "Epoch 800/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 540.7501 - val_loss: 2117.7413\n",
      "Epoch 801/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 497.1840 - val_loss: 2104.8417\n",
      "Epoch 802/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 554.5695 - val_loss: 2122.3022\n",
      "Epoch 803/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 527.1813 - val_loss: 2112.2769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 804/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 489.4685 - val_loss: 2123.6701\n",
      "Epoch 805/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 481.7128 - val_loss: 2112.9143\n",
      "Epoch 806/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 502.6858 - val_loss: 2100.6028\n",
      "Epoch 807/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 520.4226 - val_loss: 2123.1863\n",
      "Epoch 808/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 561.0576 - val_loss: 2161.7898\n",
      "Epoch 809/2000\n",
      "4023/4023 [==============================] - 0s 105us/step - loss: 510.5036 - val_loss: 2213.6543\n",
      "Epoch 810/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 586.0842 - val_loss: 2146.6612\n",
      "Epoch 811/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 502.6616 - val_loss: 2039.7761\n",
      "Epoch 812/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 552.5088 - val_loss: 2154.7921\n",
      "Epoch 813/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 527.9539 - val_loss: 2097.9263\n",
      "Epoch 814/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 514.6733 - val_loss: 2123.5911\n",
      "Epoch 815/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 517.8491 - val_loss: 2087.6050\n",
      "Epoch 816/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 489.1330 - val_loss: 2068.9465\n",
      "Epoch 817/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 510.2663 - val_loss: 2068.2004\n",
      "Epoch 818/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 498.8832 - val_loss: 2936.7593\n",
      "Epoch 819/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 872.3460 - val_loss: 2486.4097\n",
      "Epoch 820/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 748.3340 - val_loss: 2344.3062\n",
      "Epoch 821/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 643.6079 - val_loss: 2300.1233\n",
      "Epoch 822/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 565.9813 - val_loss: 2280.3879\n",
      "Epoch 823/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 568.4858 - val_loss: 2566.9956\n",
      "Epoch 824/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 785.7445 - val_loss: 2489.7923\n",
      "Epoch 825/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 694.3480 - val_loss: 2368.6713\n",
      "Epoch 826/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 661.3715 - val_loss: 2382.8720\n",
      "Epoch 827/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 616.6228 - val_loss: 2329.2367\n",
      "Epoch 828/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 615.3312 - val_loss: 2299.0301\n",
      "Epoch 829/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 615.5077 - val_loss: 2311.2415\n",
      "Epoch 830/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 665.4213 - val_loss: 2307.0141\n",
      "Epoch 831/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 646.2462 - val_loss: 2364.4269\n",
      "Epoch 832/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 606.9050 - val_loss: 2193.3733\n",
      "Epoch 833/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 694.4593 - val_loss: 2250.9030\n",
      "Epoch 834/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 525.1453 - val_loss: 2439.6903\n",
      "Epoch 835/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 737.5242 - val_loss: 2355.0863\n",
      "Epoch 836/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 609.3721 - val_loss: 2311.1842\n",
      "Epoch 837/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 594.5970 - val_loss: 2265.3607\n",
      "Epoch 838/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 551.1859 - val_loss: 2201.0594\n",
      "Epoch 839/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 577.1154 - val_loss: 2220.3101\n",
      "Epoch 840/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 577.7690 - val_loss: 2161.7456\n",
      "Epoch 841/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 567.1499 - val_loss: 2239.5559\n",
      "Epoch 842/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 524.7088 - val_loss: 2132.1542\n",
      "Epoch 843/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 606.5240 - val_loss: 2242.3912\n",
      "Epoch 844/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 549.1633 - val_loss: 2104.0194\n",
      "Epoch 845/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 551.1544 - val_loss: 2134.9832\n",
      "Epoch 846/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 586.3338 - val_loss: 2164.1734\n",
      "Epoch 847/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 561.8415 - val_loss: 2202.6293\n",
      "Epoch 848/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 573.5849 - val_loss: 2197.7454\n",
      "Epoch 849/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 540.3353 - val_loss: 2191.1403\n",
      "Epoch 850/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 537.7959 - val_loss: 2122.4995\n",
      "Epoch 851/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 518.3341 - val_loss: 2086.4298\n",
      "Epoch 852/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 550.9159 - val_loss: 2091.9127\n",
      "Epoch 853/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 489.5229 - val_loss: 2045.5571\n",
      "Epoch 854/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 548.4977 - val_loss: 2038.2229\n",
      "Epoch 855/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 471.3774 - val_loss: 2041.9960\n",
      "Epoch 856/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 510.7954 - val_loss: 2005.4050\n",
      "Epoch 857/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 559.8276 - val_loss: 2046.8509\n",
      "Epoch 858/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 563.0585 - val_loss: 1966.2851\n",
      "Epoch 859/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 511.9406 - val_loss: 2058.1826\n",
      "Epoch 860/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 487.2128 - val_loss: 2062.3814\n",
      "Epoch 861/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 503.9599 - val_loss: 2079.2230\n",
      "Epoch 862/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 507.6107 - val_loss: 2071.1427\n",
      "Epoch 863/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 481.0697 - val_loss: 2029.5136\n",
      "Epoch 864/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 479.1386 - val_loss: 1990.5356\n",
      "Epoch 865/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 447.5389 - val_loss: 2025.3514\n",
      "Epoch 866/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 484.8566 - val_loss: 1939.1341\n",
      "Epoch 867/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 495.9941 - val_loss: 2045.9874\n",
      "Epoch 868/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 489.0017 - val_loss: 1956.0498\n",
      "Epoch 869/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 486.9350 - val_loss: 1954.4757\n",
      "Epoch 870/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 476.1534 - val_loss: 1925.4155\n",
      "Epoch 871/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 457.7440 - val_loss: 1887.0071\n",
      "Epoch 872/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 456.8322 - val_loss: 1856.7907\n",
      "Epoch 873/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 437.7806 - val_loss: 1981.0969\n",
      "Epoch 874/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 464.4501 - val_loss: 1921.9458\n",
      "Epoch 875/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 523.0077 - val_loss: 2103.8592\n",
      "Epoch 876/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 567.4381 - val_loss: 2176.6522\n",
      "Epoch 877/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4023/4023 [==============================] - 0s 91us/step - loss: 525.6933 - val_loss: 2193.4094\n",
      "Epoch 878/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 542.9430 - val_loss: 2000.0157\n",
      "Epoch 879/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 537.5731 - val_loss: 2046.3801\n",
      "Epoch 880/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 476.4045 - val_loss: 1906.2574\n",
      "Epoch 881/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 485.3925 - val_loss: 1967.8856\n",
      "Epoch 882/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 488.0670 - val_loss: 1984.3602\n",
      "Epoch 883/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 474.8473 - val_loss: 2024.4253\n",
      "Epoch 884/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 505.7701 - val_loss: 1997.5726\n",
      "Epoch 885/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 447.9363 - val_loss: 2056.5532\n",
      "Epoch 886/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 466.0556 - val_loss: 1993.7540\n",
      "Epoch 887/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 478.5708 - val_loss: 1919.5114\n",
      "Epoch 888/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 427.9586 - val_loss: 2063.5718\n",
      "Epoch 889/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 576.5554 - val_loss: 2115.6186\n",
      "Epoch 890/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 552.9185 - val_loss: 1962.0074\n",
      "Epoch 891/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 531.1929 - val_loss: 1919.1757\n",
      "Epoch 892/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 425.4775 - val_loss: 2055.9680\n",
      "Epoch 893/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 518.5439 - val_loss: 2079.4560\n",
      "Epoch 894/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 500.7462 - val_loss: 2061.7144\n",
      "Epoch 895/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 484.4824 - val_loss: 2012.6505\n",
      "Epoch 896/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 471.5595 - val_loss: 1954.3394\n",
      "Epoch 897/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 442.6594 - val_loss: 1932.0854\n",
      "Epoch 898/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 472.7923 - val_loss: 1916.6272\n",
      "Epoch 899/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 498.2526 - val_loss: 1909.0806\n",
      "Epoch 900/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 431.2991 - val_loss: 1905.0731\n",
      "Epoch 901/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 433.0302 - val_loss: 1892.6003\n",
      "Epoch 902/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 426.7040 - val_loss: 1859.5117\n",
      "Epoch 903/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 433.7331 - val_loss: 1832.4102\n",
      "Epoch 904/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 441.9592 - val_loss: 1833.1549\n",
      "Epoch 905/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 445.8407 - val_loss: 1907.8164\n",
      "Epoch 906/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 432.3190 - val_loss: 1892.2641\n",
      "Epoch 907/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 446.6191 - val_loss: 1854.5773\n",
      "Epoch 908/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 480.9687 - val_loss: 1904.2372\n",
      "Epoch 909/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 456.6198 - val_loss: 1864.8147\n",
      "Epoch 910/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 430.9513 - val_loss: 1864.7997\n",
      "Epoch 911/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 430.9767 - val_loss: 1859.8871\n",
      "Epoch 912/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 455.1811 - val_loss: 1890.1226\n",
      "Epoch 913/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 430.6855 - val_loss: 1822.3522\n",
      "Epoch 914/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 454.6528 - val_loss: 1845.9018\n",
      "Epoch 915/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 449.5138 - val_loss: 1824.8358\n",
      "Epoch 916/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 485.6150 - val_loss: 1788.8293\n",
      "Epoch 917/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 418.4688 - val_loss: 1829.8759\n",
      "Epoch 918/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 419.2205 - val_loss: 1811.1385\n",
      "Epoch 919/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 435.7933 - val_loss: 1865.4915\n",
      "Epoch 920/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 457.4336 - val_loss: 1827.3874\n",
      "Epoch 921/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 425.6458 - val_loss: 1880.8320\n",
      "Epoch 922/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 422.3337 - val_loss: 1940.4419\n",
      "Epoch 923/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 462.1571 - val_loss: 2015.5786\n",
      "Epoch 924/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 467.2089 - val_loss: 1991.4211\n",
      "Epoch 925/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 535.6146 - val_loss: 1981.5265\n",
      "Epoch 926/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 488.4813 - val_loss: 1903.5897\n",
      "Epoch 927/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 441.7519 - val_loss: 1883.1699\n",
      "Epoch 928/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 426.0262 - val_loss: 1857.1235\n",
      "Epoch 929/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 445.3700 - val_loss: 1880.9420\n",
      "Epoch 930/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 429.9525 - val_loss: 1859.4445\n",
      "Epoch 931/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 428.7998 - val_loss: 1832.2851\n",
      "Epoch 932/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 445.5909 - val_loss: 1884.1768\n",
      "Epoch 933/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 406.2125 - val_loss: 1785.4371\n",
      "Epoch 934/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 408.8042 - val_loss: 1794.2549\n",
      "Epoch 935/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 424.1504 - val_loss: 1898.9946\n",
      "Epoch 936/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 441.4626 - val_loss: 1772.4765\n",
      "Epoch 937/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 481.3725 - val_loss: 1812.4138\n",
      "Epoch 938/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 441.7682 - val_loss: 1785.6229\n",
      "Epoch 939/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 473.4226 - val_loss: 1836.1894\n",
      "Epoch 940/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 447.4940 - val_loss: 1840.6103\n",
      "Epoch 941/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 438.2596 - val_loss: 1821.4364\n",
      "Epoch 942/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 417.4575 - val_loss: 1775.0283\n",
      "Epoch 943/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 455.6838 - val_loss: 1662.5044\n",
      "Epoch 944/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 438.7079 - val_loss: 1854.0296\n",
      "Epoch 945/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 429.2074 - val_loss: 1821.5019\n",
      "Epoch 946/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 461.6948 - val_loss: 1944.7952\n",
      "Epoch 947/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 521.2354 - val_loss: 1783.3581\n",
      "Epoch 948/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 602.6849 - val_loss: 1933.1541\n",
      "Epoch 949/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 485.9920 - val_loss: 1812.3287\n",
      "Epoch 950/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4023/4023 [==============================] - 0s 92us/step - loss: 432.7303 - val_loss: 1897.3778\n",
      "Epoch 951/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 447.3527 - val_loss: 1990.5881\n",
      "Epoch 952/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 469.0099 - val_loss: 1961.0981\n",
      "Epoch 953/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 443.4423 - val_loss: 1904.7536\n",
      "Epoch 954/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 435.5024 - val_loss: 1871.9482\n",
      "Epoch 955/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 436.3184 - val_loss: 1850.1994\n",
      "Epoch 956/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 420.4691 - val_loss: 1810.0299\n",
      "Epoch 957/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 411.8283 - val_loss: 1785.4179\n",
      "Epoch 958/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 443.3273 - val_loss: 1715.5191\n",
      "Epoch 959/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 374.2198 - val_loss: 1753.7492\n",
      "Epoch 960/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 391.8880 - val_loss: 2397.6566\n",
      "Epoch 961/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 732.8298 - val_loss: 2227.5911\n",
      "Epoch 962/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 559.4185 - val_loss: 2139.1318\n",
      "Epoch 963/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 564.8925 - val_loss: 2222.4573\n",
      "Epoch 964/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 542.9531 - val_loss: 2115.0409\n",
      "Epoch 965/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 535.4583 - val_loss: 2104.1787\n",
      "Epoch 966/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 502.8328 - val_loss: 2072.3113\n",
      "Epoch 967/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 463.3608 - val_loss: 2051.7974\n",
      "Epoch 968/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 503.2650 - val_loss: 2079.6934\n",
      "Epoch 969/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 454.3570 - val_loss: 1963.3532\n",
      "Epoch 970/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 413.0208 - val_loss: 2144.8121\n",
      "Epoch 971/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 431.4559 - val_loss: 1995.5548\n",
      "Epoch 972/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 435.6998 - val_loss: 2011.8543\n",
      "Epoch 973/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 429.8589 - val_loss: 1987.1622\n",
      "Epoch 974/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 418.8796 - val_loss: 1999.1917\n",
      "Epoch 975/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 431.8410 - val_loss: 1946.8300\n",
      "Epoch 976/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 410.4971 - val_loss: 1828.9654\n",
      "Epoch 977/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 403.0417 - val_loss: 1827.7496\n",
      "Epoch 978/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 396.0732 - val_loss: 1846.5706\n",
      "Epoch 979/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 433.1678 - val_loss: 1830.6170\n",
      "Epoch 980/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 510.8844 - val_loss: 1876.8390\n",
      "Epoch 981/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 423.4612 - val_loss: 1833.5978\n",
      "Epoch 982/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 389.4869 - val_loss: 1794.6752\n",
      "Epoch 983/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 433.5202 - val_loss: 1901.4439\n",
      "Epoch 984/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 437.0339 - val_loss: 1803.2513\n",
      "Epoch 985/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 363.9764 - val_loss: 1862.4530\n",
      "Epoch 986/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 421.7670 - val_loss: 1768.7143\n",
      "Epoch 987/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 460.2677 - val_loss: 1907.8115\n",
      "Epoch 988/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 452.5774 - val_loss: 1811.4465\n",
      "Epoch 989/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 422.9971 - val_loss: 1839.7677\n",
      "Epoch 990/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 373.9651 - val_loss: 1793.0417\n",
      "Epoch 991/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 421.6883 - val_loss: 1861.5055\n",
      "Epoch 992/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 393.5778 - val_loss: 1883.3303\n",
      "Epoch 993/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 438.7296 - val_loss: 1931.9449\n",
      "Epoch 994/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 408.9871 - val_loss: 1850.5454\n",
      "Epoch 995/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 418.6403 - val_loss: 1837.9875\n",
      "Epoch 996/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 391.5477 - val_loss: 1929.9570\n",
      "Epoch 997/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 451.5653 - val_loss: 1833.3861\n",
      "Epoch 998/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 415.7494 - val_loss: 1847.3749\n",
      "Epoch 999/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 416.9730 - val_loss: 1774.5292\n",
      "Epoch 1000/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 411.1823 - val_loss: 1799.0513\n",
      "Epoch 1001/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 430.6263 - val_loss: 1809.2905\n",
      "Epoch 1002/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 389.2175 - val_loss: 1772.6778\n",
      "Epoch 1003/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 394.7637 - val_loss: 1895.4189\n",
      "Epoch 1004/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 431.0909 - val_loss: 1727.5333\n",
      "Epoch 1005/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 435.1076 - val_loss: 1822.2887\n",
      "Epoch 1006/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 527.8715 - val_loss: 1737.6038\n",
      "Epoch 1007/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 438.5825 - val_loss: 1817.0602\n",
      "Epoch 1008/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 426.6484 - val_loss: 1761.1882\n",
      "Epoch 1009/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 409.8715 - val_loss: 1915.9143\n",
      "Epoch 1010/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 636.6295 - val_loss: 1978.1298\n",
      "Epoch 1011/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 566.3242 - val_loss: 1873.9176\n",
      "Epoch 1012/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 449.6808 - val_loss: 1843.9506\n",
      "Epoch 1013/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 433.6429 - val_loss: 1817.9577\n",
      "Epoch 1014/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 501.5995 - val_loss: 1930.0186\n",
      "Epoch 1015/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 486.6879 - val_loss: 1764.4442\n",
      "Epoch 1016/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 382.8189 - val_loss: 1790.6034\n",
      "Epoch 1017/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 425.8046 - val_loss: 1806.0208\n",
      "Epoch 1018/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 446.3411 - val_loss: 1928.6702\n",
      "Epoch 1019/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 463.5599 - val_loss: 2338.7362\n",
      "Epoch 1020/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 736.3335 - val_loss: 2106.6954\n",
      "Epoch 1021/2000\n",
      "4023/4023 [==============================] - 0s 109us/step - loss: 515.0113 - val_loss: 2022.8255\n",
      "Epoch 1022/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 522.1805 - val_loss: 1990.6726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1023/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 524.9893 - val_loss: 2030.3265\n",
      "Epoch 1024/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 505.8338 - val_loss: 1947.5191\n",
      "Epoch 1025/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 477.2267 - val_loss: 1949.9809\n",
      "Epoch 1026/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 486.5059 - val_loss: 1890.6431\n",
      "Epoch 1027/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 538.7627 - val_loss: 1889.4013\n",
      "Epoch 1028/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 505.6883 - val_loss: 1914.8065\n",
      "Epoch 1029/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 515.7518 - val_loss: 1872.5604\n",
      "Epoch 1030/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 509.4685 - val_loss: 1838.7435\n",
      "Epoch 1031/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 453.6699 - val_loss: 1812.9096\n",
      "Epoch 1032/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 531.3655 - val_loss: 1792.5613\n",
      "Epoch 1033/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 482.9536 - val_loss: 1835.3800\n",
      "Epoch 1034/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 443.9858 - val_loss: 1763.1976\n",
      "Epoch 1035/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 448.0961 - val_loss: 1710.7056\n",
      "Epoch 1036/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 468.2059 - val_loss: 1776.5866\n",
      "Epoch 1037/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 453.6540 - val_loss: 1601.7596\n",
      "Epoch 1038/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 633.5798 - val_loss: 1883.6647\n",
      "Epoch 1039/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 501.0808 - val_loss: 1495.4970\n",
      "Epoch 1040/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 401.3935 - val_loss: 1461.2781\n",
      "Epoch 1041/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 402.3156 - val_loss: 1520.0893\n",
      "Epoch 1042/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 395.2768 - val_loss: 1476.4644\n",
      "Epoch 1043/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 350.6263 - val_loss: 1482.7603\n",
      "Epoch 1044/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 414.6592 - val_loss: 1495.0757\n",
      "Epoch 1045/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 396.1719 - val_loss: 1484.0959\n",
      "Epoch 1046/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 345.2755 - val_loss: 1488.2285\n",
      "Epoch 1047/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 355.4620 - val_loss: 1459.7763\n",
      "Epoch 1048/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 348.3968 - val_loss: 1467.4051\n",
      "Epoch 1049/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 383.6458 - val_loss: 1417.9970\n",
      "Epoch 1050/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 344.1248 - val_loss: 1468.3286\n",
      "Epoch 1051/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 388.9588 - val_loss: 1494.7348\n",
      "Epoch 1052/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 371.0182 - val_loss: 1464.8738\n",
      "Epoch 1053/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 398.9178 - val_loss: 1596.2243\n",
      "Epoch 1054/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 389.8656 - val_loss: 1471.0147\n",
      "Epoch 1055/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 377.4037 - val_loss: 1544.7913\n",
      "Epoch 1056/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 327.3724 - val_loss: 1434.7859\n",
      "Epoch 1057/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 384.5004 - val_loss: 1580.0372\n",
      "Epoch 1058/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 380.9758 - val_loss: 1535.8220\n",
      "Epoch 1059/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 366.2682 - val_loss: 1554.5458\n",
      "Epoch 1060/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 378.6281 - val_loss: 1509.4516\n",
      "Epoch 1061/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 419.0032 - val_loss: 1530.9710\n",
      "Epoch 1062/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 392.8503 - val_loss: 1486.8774\n",
      "Epoch 1063/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 349.0885 - val_loss: 1471.4316\n",
      "Epoch 1064/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 353.9054 - val_loss: 1438.6482\n",
      "Epoch 1065/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 321.9360 - val_loss: 1447.9321\n",
      "Epoch 1066/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 361.5172 - val_loss: 1421.5584\n",
      "Epoch 1067/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 361.8229 - val_loss: 1529.1656\n",
      "Epoch 1068/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 393.1969 - val_loss: 1467.0703\n",
      "Epoch 1069/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 403.4071 - val_loss: 1715.1058\n",
      "Epoch 1070/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 529.3384 - val_loss: 1600.4442\n",
      "Epoch 1071/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 502.5142 - val_loss: 1542.6437\n",
      "Epoch 1072/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 443.3243 - val_loss: 1494.7554\n",
      "Epoch 1073/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 357.0228 - val_loss: 1503.3947\n",
      "Epoch 1074/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 391.3227 - val_loss: 1481.1392\n",
      "Epoch 1075/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 374.7440 - val_loss: 1527.0638\n",
      "Epoch 1076/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 362.4412 - val_loss: 1493.5862\n",
      "Epoch 1077/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 342.9491 - val_loss: 1487.1489\n",
      "Epoch 1078/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 340.8970 - val_loss: 1523.4447\n",
      "Epoch 1079/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 329.6859 - val_loss: 1489.6082\n",
      "Epoch 1080/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 312.6454 - val_loss: 1507.9406\n",
      "Epoch 1081/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 323.8778 - val_loss: 1475.2607\n",
      "Epoch 1082/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 337.4421 - val_loss: 1469.3403\n",
      "Epoch 1083/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 324.9115 - val_loss: 1477.6869\n",
      "Epoch 1084/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 365.6729 - val_loss: 1445.7165\n",
      "Epoch 1085/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 291.4923 - val_loss: 1471.9276\n",
      "Epoch 1086/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 334.5858 - val_loss: 1450.3463\n",
      "Epoch 1087/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 346.3395 - val_loss: 1415.2132\n",
      "Epoch 1088/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 353.7395 - val_loss: 1459.7839\n",
      "Epoch 1089/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 344.8872 - val_loss: 1432.0144\n",
      "Epoch 1090/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 379.8326 - val_loss: 1465.0611\n",
      "Epoch 1091/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 336.8022 - val_loss: 1432.5415\n",
      "Epoch 1092/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 330.9101 - val_loss: 1470.0116\n",
      "Epoch 1093/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 318.9855 - val_loss: 1417.0795\n",
      "Epoch 1094/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 340.0042 - val_loss: 1443.6584\n",
      "Epoch 1095/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4023/4023 [==============================] - 0s 93us/step - loss: 349.0415 - val_loss: 1433.8927\n",
      "Epoch 1096/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 375.3345 - val_loss: 1464.3552\n",
      "Epoch 1097/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 296.8028 - val_loss: 1401.5318\n",
      "Epoch 1098/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 351.8058 - val_loss: 1438.0542\n",
      "Epoch 1099/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 339.3666 - val_loss: 1411.5177\n",
      "Epoch 1100/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 292.1752 - val_loss: 1392.6280\n",
      "Epoch 1101/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 309.2923 - val_loss: 1422.6036\n",
      "Epoch 1102/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 347.7686 - val_loss: 1389.0956\n",
      "Epoch 1103/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 310.6657 - val_loss: 1464.5831\n",
      "Epoch 1104/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 337.1055 - val_loss: 1391.2018\n",
      "Epoch 1105/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 365.6952 - val_loss: 1496.0307\n",
      "Epoch 1106/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 359.7831 - val_loss: 1372.9767\n",
      "Epoch 1107/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 375.2921 - val_loss: 1476.6396\n",
      "Epoch 1108/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 336.4584 - val_loss: 1365.8230\n",
      "Epoch 1109/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 356.7068 - val_loss: 1459.1474\n",
      "Epoch 1110/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 378.3017 - val_loss: 1585.8160\n",
      "Epoch 1111/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 445.8679 - val_loss: 1393.1387\n",
      "Epoch 1112/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 349.1188 - val_loss: 1409.8275\n",
      "Epoch 1113/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 335.3452 - val_loss: 1419.9298\n",
      "Epoch 1114/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 331.9582 - val_loss: 1384.0826\n",
      "Epoch 1115/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 312.0412 - val_loss: 1441.3808\n",
      "Epoch 1116/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 308.1766 - val_loss: 1372.4962\n",
      "Epoch 1117/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 383.7534 - val_loss: 1414.1921\n",
      "Epoch 1118/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 388.1515 - val_loss: 1468.1130\n",
      "Epoch 1119/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 365.6213 - val_loss: 1367.4387\n",
      "Epoch 1120/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 351.0592 - val_loss: 1537.5122\n",
      "Epoch 1121/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 467.0484 - val_loss: 1378.5935\n",
      "Epoch 1122/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 397.8888 - val_loss: 1416.8578\n",
      "Epoch 1123/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 376.5935 - val_loss: 1534.8695\n",
      "Epoch 1124/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 549.1563 - val_loss: 1473.7615\n",
      "Epoch 1125/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 327.6542 - val_loss: 1486.1162\n",
      "Epoch 1126/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 334.3446 - val_loss: 1498.4802\n",
      "Epoch 1127/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 329.5324 - val_loss: 1454.9906\n",
      "Epoch 1128/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 310.7545 - val_loss: 1424.1456\n",
      "Epoch 1129/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 329.9615 - val_loss: 1416.8751\n",
      "Epoch 1130/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 319.2286 - val_loss: 1409.5301\n",
      "Epoch 1131/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 324.2256 - val_loss: 1425.6832\n",
      "Epoch 1132/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 374.1526 - val_loss: 1414.3242\n",
      "Epoch 1133/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 314.3930 - val_loss: 1421.8065\n",
      "Epoch 1134/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 349.8142 - val_loss: 1382.8019\n",
      "Epoch 1135/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 316.5132 - val_loss: 1412.6870\n",
      "Epoch 1136/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 339.2802 - val_loss: 1360.4851\n",
      "Epoch 1137/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 317.4004 - val_loss: 1366.7176\n",
      "Epoch 1138/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 312.9897 - val_loss: 1317.9820\n",
      "Epoch 1139/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 343.3914 - val_loss: 1394.6798\n",
      "Epoch 1140/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 393.7710 - val_loss: 1371.5399\n",
      "Epoch 1141/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 435.1707 - val_loss: 1501.8900\n",
      "Epoch 1142/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 402.5325 - val_loss: 1442.0583\n",
      "Epoch 1143/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 497.1909 - val_loss: 1742.4267\n",
      "Epoch 1144/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 536.4274 - val_loss: 1652.0669\n",
      "Epoch 1145/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 523.9221 - val_loss: 1827.7401\n",
      "Epoch 1146/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 540.9386 - val_loss: 1815.1300\n",
      "Epoch 1147/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 485.4011 - val_loss: 1683.4817\n",
      "Epoch 1148/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 428.4071 - val_loss: 1736.5444\n",
      "Epoch 1149/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 392.6370 - val_loss: 1595.3132\n",
      "Epoch 1150/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 364.9814 - val_loss: 1652.4594\n",
      "Epoch 1151/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 378.5476 - val_loss: 1568.8279\n",
      "Epoch 1152/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 410.3367 - val_loss: 1667.3769\n",
      "Epoch 1153/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 393.2492 - val_loss: 1566.9189\n",
      "Epoch 1154/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 365.3101 - val_loss: 1584.5513\n",
      "Epoch 1155/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 371.4816 - val_loss: 1523.0689\n",
      "Epoch 1156/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 405.2053 - val_loss: 1617.4551\n",
      "Epoch 1157/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 421.8296 - val_loss: 1530.2904\n",
      "Epoch 1158/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 383.3097 - val_loss: 1531.8374\n",
      "Epoch 1159/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 400.6217 - val_loss: 1563.3346\n",
      "Epoch 1160/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 385.2361 - val_loss: 1547.3247\n",
      "Epoch 1161/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 385.1260 - val_loss: 1524.5569\n",
      "Epoch 1162/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 493.0430 - val_loss: 1597.0002\n",
      "Epoch 1163/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 454.6744 - val_loss: 1481.9563\n",
      "Epoch 1164/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 347.6755 - val_loss: 1480.2091\n",
      "Epoch 1165/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 348.0975 - val_loss: 1466.1144\n",
      "Epoch 1166/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 374.8490 - val_loss: 1469.9885\n",
      "Epoch 1167/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4023/4023 [==============================] - 0s 90us/step - loss: 340.5771 - val_loss: 1449.9072\n",
      "Epoch 1168/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 437.2176 - val_loss: 1516.6873\n",
      "Epoch 1169/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 337.4049 - val_loss: 1461.9393\n",
      "Epoch 1170/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 458.3865 - val_loss: 1510.3526\n",
      "Epoch 1171/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 355.8652 - val_loss: 1469.8972\n",
      "Epoch 1172/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 340.5960 - val_loss: 1460.6651\n",
      "Epoch 1173/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 356.0509 - val_loss: 1437.5093\n",
      "Epoch 1174/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 336.1417 - val_loss: 1459.6337\n",
      "Epoch 1175/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 336.6583 - val_loss: 1436.2765\n",
      "Epoch 1176/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 335.2535 - val_loss: 1452.3964\n",
      "Epoch 1177/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 357.1691 - val_loss: 1444.3615\n",
      "Epoch 1178/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 345.2861 - val_loss: 1504.2121\n",
      "Epoch 1179/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 349.9508 - val_loss: 1477.4516\n",
      "Epoch 1180/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 381.0524 - val_loss: 1443.0500\n",
      "Epoch 1181/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 323.7404 - val_loss: 1641.6596\n",
      "Epoch 1182/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 380.4044 - val_loss: 1594.8751\n",
      "Epoch 1183/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 325.8522 - val_loss: 1559.2592\n",
      "Epoch 1184/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 349.3034 - val_loss: 1545.3533\n",
      "Epoch 1185/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 349.4182 - val_loss: 1534.7411\n",
      "Epoch 1186/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 351.7516 - val_loss: 1508.9645\n",
      "Epoch 1187/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 363.1463 - val_loss: 1487.9392\n",
      "Epoch 1188/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 288.4880 - val_loss: 1477.4190\n",
      "Epoch 1189/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 326.3661 - val_loss: 1467.9341\n",
      "Epoch 1190/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 345.8222 - val_loss: 1462.7680\n",
      "Epoch 1191/2000\n",
      "4023/4023 [==============================] - ETA: 0s - loss: 406.331 - 0s 96us/step - loss: 363.8613 - val_loss: 1425.4246\n",
      "Epoch 1192/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 393.0822 - val_loss: 1415.8364\n",
      "Epoch 1193/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 362.5490 - val_loss: 1476.9251\n",
      "Epoch 1194/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 342.1871 - val_loss: 1527.3746\n",
      "Epoch 1195/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 392.7790 - val_loss: 1462.0248\n",
      "Epoch 1196/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 323.5304 - val_loss: 1462.6175\n",
      "Epoch 1197/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 381.0946 - val_loss: 1389.9486\n",
      "Epoch 1198/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 420.7608 - val_loss: 1510.1444\n",
      "Epoch 1199/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 337.2950 - val_loss: 1398.8741\n",
      "Epoch 1200/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 376.3059 - val_loss: 1419.7951\n",
      "Epoch 1201/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 357.6574 - val_loss: 1511.8533\n",
      "Epoch 1202/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 408.5403 - val_loss: 1461.2552\n",
      "Epoch 1203/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 394.5162 - val_loss: 1555.0773\n",
      "Epoch 1204/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 458.6777 - val_loss: 1431.9473\n",
      "Epoch 1205/2000\n",
      "4023/4023 [==============================] - 0s 118us/step - loss: 324.6739 - val_loss: 1433.7598\n",
      "Epoch 1206/2000\n",
      "4023/4023 [==============================] - 0s 108us/step - loss: 359.7709 - val_loss: 1420.8900\n",
      "Epoch 1207/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 375.6264 - val_loss: 1413.0598\n",
      "Epoch 1208/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 339.1231 - val_loss: 1404.0162\n",
      "Epoch 1209/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 359.2083 - val_loss: 1472.0123\n",
      "Epoch 1210/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 398.1730 - val_loss: 1417.6596\n",
      "Epoch 1211/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 331.2965 - val_loss: 1480.1206\n",
      "Epoch 1212/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 373.3132 - val_loss: 1415.0987\n",
      "Epoch 1213/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 370.8125 - val_loss: 1461.8673\n",
      "Epoch 1214/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 315.4755 - val_loss: 1407.0572\n",
      "Epoch 1215/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 418.5476 - val_loss: 1483.7961\n",
      "Epoch 1216/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 322.7884 - val_loss: 1432.1118\n",
      "Epoch 1217/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 368.5495 - val_loss: 1438.1685\n",
      "Epoch 1218/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 353.0474 - val_loss: 1398.3101\n",
      "Epoch 1219/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 373.0198 - val_loss: 1446.9588\n",
      "Epoch 1220/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 390.6461 - val_loss: 1406.1206\n",
      "Epoch 1221/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 376.7067 - val_loss: 1459.7676\n",
      "Epoch 1222/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 334.6161 - val_loss: 1429.2815\n",
      "Epoch 1223/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 305.8988 - val_loss: 1445.8546\n",
      "Epoch 1224/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 348.3261 - val_loss: 1386.0093\n",
      "Epoch 1225/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 321.4636 - val_loss: 1422.9721\n",
      "Epoch 1226/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 369.8196 - val_loss: 1382.4500\n",
      "Epoch 1227/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 305.4119 - val_loss: 1409.7706\n",
      "Epoch 1228/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 342.1861 - val_loss: 1402.5164\n",
      "Epoch 1229/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 341.4027 - val_loss: 1392.6186\n",
      "Epoch 1230/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 311.9157 - val_loss: 1395.9930\n",
      "Epoch 1231/2000\n",
      "4023/4023 [==============================] - 0s 106us/step - loss: 346.0916 - val_loss: 1364.8741\n",
      "Epoch 1232/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 336.4794 - val_loss: 1432.4159\n",
      "Epoch 1233/2000\n",
      "4023/4023 [==============================] - 0s 105us/step - loss: 282.5050 - val_loss: 1389.7956\n",
      "Epoch 1234/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 338.8968 - val_loss: 1385.8047\n",
      "Epoch 1235/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 327.2701 - val_loss: 1380.6315\n",
      "Epoch 1236/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 303.6762 - val_loss: 1412.6956\n",
      "Epoch 1237/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 300.3100 - val_loss: 1358.7649\n",
      "Epoch 1238/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 323.8847 - val_loss: 1328.1222\n",
      "Epoch 1239/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4023/4023 [==============================] - 0s 94us/step - loss: 338.7561 - val_loss: 1397.4984\n",
      "Epoch 1240/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 347.0744 - val_loss: 1312.9070\n",
      "Epoch 1241/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 406.3015 - val_loss: 1399.5267\n",
      "Epoch 1242/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 312.2918 - val_loss: 1365.0876\n",
      "Epoch 1243/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 362.3957 - val_loss: 1365.1088\n",
      "Epoch 1244/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 326.5643 - val_loss: 1339.3770\n",
      "Epoch 1245/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 322.0375 - val_loss: 1354.9863\n",
      "Epoch 1246/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 316.7477 - val_loss: 1400.6645\n",
      "Epoch 1247/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 360.3815 - val_loss: 1355.7125\n",
      "Epoch 1248/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 305.8617 - val_loss: 1466.4661\n",
      "Epoch 1249/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 357.3999 - val_loss: 1717.0978\n",
      "Epoch 1250/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 492.3929 - val_loss: 2041.3587\n",
      "Epoch 1251/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 475.4903 - val_loss: 1927.0430\n",
      "Epoch 1252/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 428.6303 - val_loss: 1867.6846\n",
      "Epoch 1253/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 488.9310 - val_loss: 1949.9879\n",
      "Epoch 1254/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 476.0951 - val_loss: 1840.2919\n",
      "Epoch 1255/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 489.5780 - val_loss: 1813.3064\n",
      "Epoch 1256/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 429.8171 - val_loss: 1843.2970\n",
      "Epoch 1257/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 450.3154 - val_loss: 2061.6086\n",
      "Epoch 1258/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 536.8282 - val_loss: 1981.5207\n",
      "Epoch 1259/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 527.6486 - val_loss: 1923.1565\n",
      "Epoch 1260/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 462.6933 - val_loss: 1824.1813\n",
      "Epoch 1261/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 496.6757 - val_loss: 2193.1018\n",
      "Epoch 1262/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 700.5930 - val_loss: 2096.6528\n",
      "Epoch 1263/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 567.0183 - val_loss: 2037.4447\n",
      "Epoch 1264/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 528.2819 - val_loss: 1952.2739\n",
      "Epoch 1265/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 531.5183 - val_loss: 1936.7531\n",
      "Epoch 1266/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 518.9668 - val_loss: 1965.6557\n",
      "Epoch 1267/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 514.6039 - val_loss: 1911.4079\n",
      "Epoch 1268/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 480.5212 - val_loss: 2013.3658\n",
      "Epoch 1269/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 561.3015 - val_loss: 1973.1459\n",
      "Epoch 1270/2000\n",
      "4023/4023 [==============================] - 0s 108us/step - loss: 538.9623 - val_loss: 1966.0741\n",
      "Epoch 1271/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 508.9593 - val_loss: 2179.8295\n",
      "Epoch 1272/2000\n",
      "4023/4023 [==============================] - 0s 106us/step - loss: 622.1207 - val_loss: 2285.7241\n",
      "Epoch 1273/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 667.8567 - val_loss: 2194.2347\n",
      "Epoch 1274/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 615.9734 - val_loss: 1938.5280\n",
      "Epoch 1275/2000\n",
      "4023/4023 [==============================] - 0s 105us/step - loss: 584.9741 - val_loss: 1918.0272\n",
      "Epoch 1276/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 527.2245 - val_loss: 1997.4743\n",
      "Epoch 1277/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 703.5443 - val_loss: 2047.2139\n",
      "Epoch 1278/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 548.6805 - val_loss: 2008.6385\n",
      "Epoch 1279/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 539.1661 - val_loss: 2019.0137\n",
      "Epoch 1280/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 541.3348 - val_loss: 2061.9926\n",
      "Epoch 1281/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 588.3267 - val_loss: 2068.6580\n",
      "Epoch 1282/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 533.3410 - val_loss: 2073.4418\n",
      "Epoch 1283/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 499.0063 - val_loss: 2171.7476\n",
      "Epoch 1284/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 558.5041 - val_loss: 2134.2488\n",
      "Epoch 1285/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 538.1482 - val_loss: 2094.2755\n",
      "Epoch 1286/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 504.6848 - val_loss: 2076.4021\n",
      "Epoch 1287/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 580.5640 - val_loss: 2069.0261\n",
      "Epoch 1288/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 512.1263 - val_loss: 2071.5588\n",
      "Epoch 1289/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 515.9714 - val_loss: 2022.2337\n",
      "Epoch 1290/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 513.9591 - val_loss: 1968.8952\n",
      "Epoch 1291/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 538.8916 - val_loss: 2018.7572\n",
      "Epoch 1292/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 515.7361 - val_loss: 1991.9364\n",
      "Epoch 1293/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 502.2337 - val_loss: 1995.3097\n",
      "Epoch 1294/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 468.0349 - val_loss: 1983.3818\n",
      "Epoch 1295/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 539.8279 - val_loss: 1994.6167\n",
      "Epoch 1296/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 476.9687 - val_loss: 2002.6316\n",
      "Epoch 1297/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 521.1943 - val_loss: 1951.1052\n",
      "Epoch 1298/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 534.5258 - val_loss: 1994.9838\n",
      "Epoch 1299/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 482.8944 - val_loss: 1985.1751\n",
      "Epoch 1300/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 478.7215 - val_loss: 1971.5810\n",
      "Epoch 1301/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 470.9128 - val_loss: 1942.0629\n",
      "Epoch 1302/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 448.1002 - val_loss: 1956.8926\n",
      "Epoch 1303/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 487.6372 - val_loss: 1954.5531\n",
      "Epoch 1304/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 444.7087 - val_loss: 1990.6757\n",
      "Epoch 1305/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 506.8777 - val_loss: 1911.0276\n",
      "Epoch 1306/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 462.6150 - val_loss: 1985.7190\n",
      "Epoch 1307/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 522.9441 - val_loss: 1969.2791\n",
      "Epoch 1308/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 516.3252 - val_loss: 2025.6344\n",
      "Epoch 1309/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 459.2587 - val_loss: 2000.5383\n",
      "Epoch 1310/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 513.7189 - val_loss: 1960.3149\n",
      "Epoch 1311/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4023/4023 [==============================] - 0s 92us/step - loss: 461.2937 - val_loss: 1927.5743\n",
      "Epoch 1312/2000\n",
      "4023/4023 [==============================] - 0s 106us/step - loss: 452.9506 - val_loss: 1926.0296\n",
      "Epoch 1313/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 486.3000 - val_loss: 1942.4866\n",
      "Epoch 1314/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 498.1165 - val_loss: 1916.2633\n",
      "Epoch 1315/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 478.5609 - val_loss: 1909.1852\n",
      "Epoch 1316/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 507.6543 - val_loss: 1908.6746\n",
      "Epoch 1317/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 464.8042 - val_loss: 1984.7913\n",
      "Epoch 1318/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 532.5736 - val_loss: 1889.1726\n",
      "Epoch 1319/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 479.4029 - val_loss: 1864.8927\n",
      "Epoch 1320/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 441.6464 - val_loss: 1824.6887\n",
      "Epoch 1321/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 447.4855 - val_loss: 1820.9478\n",
      "Epoch 1322/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 461.5176 - val_loss: 1807.8916\n",
      "Epoch 1323/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 450.6546 - val_loss: 1783.1903\n",
      "Epoch 1324/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 503.0180 - val_loss: 1911.3172\n",
      "Epoch 1325/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 513.2828 - val_loss: 1783.3455\n",
      "Epoch 1326/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 469.2072 - val_loss: 1795.9679\n",
      "Epoch 1327/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 450.1666 - val_loss: 1802.0049\n",
      "Epoch 1328/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 420.0881 - val_loss: 1843.8544\n",
      "Epoch 1329/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 488.8670 - val_loss: 1845.4144\n",
      "Epoch 1330/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 471.9258 - val_loss: 1778.4999\n",
      "Epoch 1331/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 407.2684 - val_loss: 1752.9175\n",
      "Epoch 1332/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 414.4763 - val_loss: 1758.6776\n",
      "Epoch 1333/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 430.8898 - val_loss: 1747.1058\n",
      "Epoch 1334/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 426.7119 - val_loss: 1762.4924\n",
      "Epoch 1335/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 415.8299 - val_loss: 1697.4188\n",
      "Epoch 1336/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 409.8646 - val_loss: 1831.7109\n",
      "Epoch 1337/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 443.2433 - val_loss: 1774.4224\n",
      "Epoch 1338/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 445.9660 - val_loss: 1741.6731\n",
      "Epoch 1339/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 409.2259 - val_loss: 1737.6968\n",
      "Epoch 1340/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 392.4744 - val_loss: 1686.5038\n",
      "Epoch 1341/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 370.7588 - val_loss: 1705.7608\n",
      "Epoch 1342/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 372.1070 - val_loss: 1721.1553\n",
      "Epoch 1343/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 406.5237 - val_loss: 1783.5917\n",
      "Epoch 1344/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 479.5708 - val_loss: 1672.1766\n",
      "Epoch 1345/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 467.3652 - val_loss: 1676.6249\n",
      "Epoch 1346/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 443.0633 - val_loss: 1704.0189\n",
      "Epoch 1347/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 404.8573 - val_loss: 1766.7340\n",
      "Epoch 1348/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 469.4777 - val_loss: 1811.1275\n",
      "Epoch 1349/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 487.2119 - val_loss: 1729.2116\n",
      "Epoch 1350/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 438.0619 - val_loss: 1633.9535\n",
      "Epoch 1351/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 383.6654 - val_loss: 1650.7063\n",
      "Epoch 1352/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 374.7994 - val_loss: 1683.6259\n",
      "Epoch 1353/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 365.7480 - val_loss: 1630.0687\n",
      "Epoch 1354/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 393.7269 - val_loss: 1807.5863\n",
      "Epoch 1355/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 438.2406 - val_loss: 1651.1948\n",
      "Epoch 1356/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 405.2202 - val_loss: 1637.4355\n",
      "Epoch 1357/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 375.6567 - val_loss: 1666.2377\n",
      "Epoch 1358/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 337.6895 - val_loss: 1640.5927\n",
      "Epoch 1359/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 340.2996 - val_loss: 1642.8361\n",
      "Epoch 1360/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 396.3192 - val_loss: 1680.2386\n",
      "Epoch 1361/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 398.8401 - val_loss: 1754.4186\n",
      "Epoch 1362/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 504.7159 - val_loss: 1916.0112\n",
      "Epoch 1363/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 570.2943 - val_loss: 1765.7029\n",
      "Epoch 1364/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 588.7806 - val_loss: 1981.9886\n",
      "Epoch 1365/2000\n",
      "4023/4023 [==============================] - 0s 106us/step - loss: 562.8397 - val_loss: 1936.5642\n",
      "Epoch 1366/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 687.7866 - val_loss: 1684.5453\n",
      "Epoch 1367/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 411.4106 - val_loss: 1654.7710\n",
      "Epoch 1368/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 381.0650 - val_loss: 1633.6370\n",
      "Epoch 1369/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 443.1279 - val_loss: 1686.4682\n",
      "Epoch 1370/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 417.0765 - val_loss: 1625.0250\n",
      "Epoch 1371/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 420.6447 - val_loss: 1597.0628\n",
      "Epoch 1372/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 368.9490 - val_loss: 1630.0989\n",
      "Epoch 1373/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 440.4764 - val_loss: 1623.8421\n",
      "Epoch 1374/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 375.5888 - val_loss: 1679.4251\n",
      "Epoch 1375/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 445.1730 - val_loss: 1604.3276\n",
      "Epoch 1376/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 360.0322 - val_loss: 1673.8968\n",
      "Epoch 1377/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 427.2635 - val_loss: 1620.0599\n",
      "Epoch 1378/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 413.4898 - val_loss: 1596.7605\n",
      "Epoch 1379/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 361.9476 - val_loss: 1585.0419\n",
      "Epoch 1380/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 360.0164 - val_loss: 1619.5511\n",
      "Epoch 1381/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 422.2228 - val_loss: 1541.2276\n",
      "Epoch 1382/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 343.1890 - val_loss: 1621.6970\n",
      "Epoch 1383/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4023/4023 [==============================] - 0s 91us/step - loss: 396.7315 - val_loss: 1542.1067\n",
      "Epoch 1384/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 340.3441 - val_loss: 1603.7744\n",
      "Epoch 1385/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 407.4944 - val_loss: 1565.6033\n",
      "Epoch 1386/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 435.8386 - val_loss: 1541.3244\n",
      "Epoch 1387/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 390.5799 - val_loss: 1526.6739\n",
      "Epoch 1388/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 447.7050 - val_loss: 1526.9216\n",
      "Epoch 1389/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 380.1618 - val_loss: 1635.0789\n",
      "Epoch 1390/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 437.0366 - val_loss: 1562.0315\n",
      "Epoch 1391/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 407.1444 - val_loss: 1549.4442\n",
      "Epoch 1392/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 400.1916 - val_loss: 1554.6132\n",
      "Epoch 1393/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 449.7062 - val_loss: 1559.4775\n",
      "Epoch 1394/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 405.4948 - val_loss: 1657.8299\n",
      "Epoch 1395/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 514.1280 - val_loss: 1540.2933\n",
      "Epoch 1396/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 361.8753 - val_loss: 1590.9745\n",
      "Epoch 1397/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 393.6756 - val_loss: 1531.5114\n",
      "Epoch 1398/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 361.4020 - val_loss: 1630.1969\n",
      "Epoch 1399/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 390.0156 - val_loss: 1585.8006\n",
      "Epoch 1400/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 435.2886 - val_loss: 1663.9495\n",
      "Epoch 1401/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 481.0160 - val_loss: 1555.0951\n",
      "Epoch 1402/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 390.0412 - val_loss: 1610.5002\n",
      "Epoch 1403/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 369.4176 - val_loss: 1550.4727\n",
      "Epoch 1404/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 349.0168 - val_loss: 1548.7516\n",
      "Epoch 1405/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 320.2607 - val_loss: 1667.8024\n",
      "Epoch 1406/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 462.4971 - val_loss: 1572.4464\n",
      "Epoch 1407/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 422.6655 - val_loss: 1587.0696\n",
      "Epoch 1408/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 425.9494 - val_loss: 1516.2581\n",
      "Epoch 1409/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 336.7384 - val_loss: 1555.9457\n",
      "Epoch 1410/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 319.7495 - val_loss: 1527.8555\n",
      "Epoch 1411/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 397.9225 - val_loss: 1517.6276\n",
      "Epoch 1412/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 385.1898 - val_loss: 1532.5429\n",
      "Epoch 1413/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 375.8716 - val_loss: 1506.9958\n",
      "Epoch 1414/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 376.5728 - val_loss: 1480.5569\n",
      "Epoch 1415/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 402.7116 - val_loss: 1524.1001\n",
      "Epoch 1416/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 364.0348 - val_loss: 1491.5846\n",
      "Epoch 1417/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 326.1041 - val_loss: 1490.6929\n",
      "Epoch 1418/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 373.5446 - val_loss: 1497.6864\n",
      "Epoch 1419/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 353.2060 - val_loss: 1507.3576\n",
      "Epoch 1420/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 315.0807 - val_loss: 1545.7636\n",
      "Epoch 1421/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 369.8399 - val_loss: 1503.0460\n",
      "Epoch 1422/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 365.5245 - val_loss: 1481.2488\n",
      "Epoch 1423/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 360.0587 - val_loss: 1484.2643\n",
      "Epoch 1424/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 366.2318 - val_loss: 1518.9497\n",
      "Epoch 1425/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 380.1404 - val_loss: 1530.3332\n",
      "Epoch 1426/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 357.7510 - val_loss: 1462.0910\n",
      "Epoch 1427/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 371.3577 - val_loss: 1496.9961\n",
      "Epoch 1428/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 342.7028 - val_loss: 1478.4213\n",
      "Epoch 1429/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 380.6226 - val_loss: 1554.6851\n",
      "Epoch 1430/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 438.4283 - val_loss: 1515.3733\n",
      "Epoch 1431/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 381.2609 - val_loss: 1488.3205\n",
      "Epoch 1432/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 366.2888 - val_loss: 1473.7744\n",
      "Epoch 1433/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 331.7238 - val_loss: 1544.3816\n",
      "Epoch 1434/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 424.3421 - val_loss: 1750.6321\n",
      "Epoch 1435/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 565.3592 - val_loss: 1565.7489\n",
      "Epoch 1436/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 364.4883 - val_loss: 1530.1008\n",
      "Epoch 1437/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 405.8147 - val_loss: 1589.1874\n",
      "Epoch 1438/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 450.9334 - val_loss: 1526.8142\n",
      "Epoch 1439/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 413.2050 - val_loss: 1528.1148\n",
      "Epoch 1440/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 374.5146 - val_loss: 1500.7403\n",
      "Epoch 1441/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 362.4366 - val_loss: 1609.4934\n",
      "Epoch 1442/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 384.0661 - val_loss: 1739.1920\n",
      "Epoch 1443/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 605.9548 - val_loss: 1732.1234\n",
      "Epoch 1444/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 528.4469 - val_loss: 1603.6003\n",
      "Epoch 1445/2000\n",
      "4023/4023 [==============================] - 0s 106us/step - loss: 418.2284 - val_loss: 1568.6239\n",
      "Epoch 1446/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 401.2148 - val_loss: 1511.9100\n",
      "Epoch 1447/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 388.6486 - val_loss: 1506.4947\n",
      "Epoch 1448/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 368.9137 - val_loss: 1517.6108\n",
      "Epoch 1449/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 364.2235 - val_loss: 1512.9369\n",
      "Epoch 1450/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 345.8706 - val_loss: 1503.5027\n",
      "Epoch 1451/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 394.3103 - val_loss: 1490.8749\n",
      "Epoch 1452/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 397.3718 - val_loss: 1506.3611\n",
      "Epoch 1453/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 366.6404 - val_loss: 1458.0267\n",
      "Epoch 1454/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 334.5229 - val_loss: 1487.2625\n",
      "Epoch 1455/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4023/4023 [==============================] - 0s 91us/step - loss: 340.2818 - val_loss: 1465.8559\n",
      "Epoch 1456/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 360.9601 - val_loss: 1474.3378\n",
      "Epoch 1457/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 383.2616 - val_loss: 1516.3196\n",
      "Epoch 1458/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 375.0897 - val_loss: 1497.7879\n",
      "Epoch 1459/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 361.4317 - val_loss: 1521.9303\n",
      "Epoch 1460/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 340.5364 - val_loss: 1469.5788\n",
      "Epoch 1461/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 332.9689 - val_loss: 1458.0632\n",
      "Epoch 1462/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 357.4575 - val_loss: 1430.9304\n",
      "Epoch 1463/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 440.5001 - val_loss: 1498.6735\n",
      "Epoch 1464/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 325.5392 - val_loss: 1436.7508\n",
      "Epoch 1465/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 294.2433 - val_loss: 1435.1468\n",
      "Epoch 1466/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 370.7314 - val_loss: 1447.2094\n",
      "Epoch 1467/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 316.1285 - val_loss: 1430.5612\n",
      "Epoch 1468/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 371.7777 - val_loss: 1548.7367\n",
      "Epoch 1469/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 377.9700 - val_loss: 1580.6294\n",
      "Epoch 1470/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 418.5948 - val_loss: 1575.5719\n",
      "Epoch 1471/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 478.4482 - val_loss: 1603.3229\n",
      "Epoch 1472/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 386.0677 - val_loss: 1494.6307\n",
      "Epoch 1473/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 368.0245 - val_loss: 1429.7187\n",
      "Epoch 1474/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 357.2452 - val_loss: 1469.7807\n",
      "Epoch 1475/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 352.2920 - val_loss: 1516.4706\n",
      "Epoch 1476/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 373.7496 - val_loss: 1487.1426\n",
      "Epoch 1477/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 389.7407 - val_loss: 1499.0146\n",
      "Epoch 1478/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 398.6710 - val_loss: 1462.0993\n",
      "Epoch 1479/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 372.0033 - val_loss: 1478.6310\n",
      "Epoch 1480/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 364.5955 - val_loss: 1531.0378\n",
      "Epoch 1481/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 310.2389 - val_loss: 1491.7183\n",
      "Epoch 1482/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 354.6747 - val_loss: 1552.2869\n",
      "Epoch 1483/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 358.9627 - val_loss: 1474.5298\n",
      "Epoch 1484/2000\n",
      "4023/4023 [==============================] - 0s 106us/step - loss: 346.5903 - val_loss: 1621.1426\n",
      "Epoch 1485/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 373.1430 - val_loss: 1528.8714\n",
      "Epoch 1486/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 399.7463 - val_loss: 1537.2040\n",
      "Epoch 1487/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 399.2984 - val_loss: 1460.6446\n",
      "Epoch 1488/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 355.9360 - val_loss: 1538.6240\n",
      "Epoch 1489/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 340.3027 - val_loss: 1516.8267\n",
      "Epoch 1490/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 402.3860 - val_loss: 1457.7990\n",
      "Epoch 1491/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 364.9753 - val_loss: 1469.4208\n",
      "Epoch 1492/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 346.9847 - val_loss: 1469.3238\n",
      "Epoch 1493/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 337.2349 - val_loss: 1485.5610\n",
      "Epoch 1494/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 347.7544 - val_loss: 1483.5472\n",
      "Epoch 1495/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 407.6384 - val_loss: 1535.3351\n",
      "Epoch 1496/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 361.7188 - val_loss: 1471.9196\n",
      "Epoch 1497/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 367.8197 - val_loss: 1466.4184\n",
      "Epoch 1498/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 299.4301 - val_loss: 1485.2665\n",
      "Epoch 1499/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 374.4107 - val_loss: 1511.6079\n",
      "Epoch 1500/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 401.0603 - val_loss: 1439.0611\n",
      "Epoch 1501/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 362.1125 - val_loss: 1476.8115\n",
      "Epoch 1502/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 363.8763 - val_loss: 1489.6675\n",
      "Epoch 1503/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 393.7765 - val_loss: 1477.9506\n",
      "Epoch 1504/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 351.6198 - val_loss: 1489.4954\n",
      "Epoch 1505/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 376.2364 - val_loss: 1483.1635\n",
      "Epoch 1506/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 350.2962 - val_loss: 1484.9725\n",
      "Epoch 1507/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 369.5379 - val_loss: 1540.2056\n",
      "Epoch 1508/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 389.8807 - val_loss: 1466.2309\n",
      "Epoch 1509/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 348.9768 - val_loss: 1560.0604\n",
      "Epoch 1510/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 343.2156 - val_loss: 1755.0194\n",
      "Epoch 1511/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 619.9192 - val_loss: 1840.3504\n",
      "Epoch 1512/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 563.4010 - val_loss: 1829.4359\n",
      "Epoch 1513/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 542.1736 - val_loss: 1833.5038\n",
      "Epoch 1514/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 558.1545 - val_loss: 1805.6919\n",
      "Epoch 1515/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 530.3826 - val_loss: 1776.6285\n",
      "Epoch 1516/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 504.8371 - val_loss: 1780.5132\n",
      "Epoch 1517/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 481.7288 - val_loss: 1758.3898\n",
      "Epoch 1518/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 518.0318 - val_loss: 1779.2083\n",
      "Epoch 1519/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 509.2051 - val_loss: 1774.8121\n",
      "Epoch 1520/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 506.4833 - val_loss: 1785.4700\n",
      "Epoch 1521/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 573.8280 - val_loss: 2423.1686\n",
      "Epoch 1522/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 1079.9281 - val_loss: 2164.0606\n",
      "Epoch 1523/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 731.5839 - val_loss: 2108.4237\n",
      "Epoch 1524/2000\n",
      "4023/4023 [==============================] - 0s 106us/step - loss: 735.6439 - val_loss: 2093.3331\n",
      "Epoch 1525/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 654.7607 - val_loss: 2087.9246\n",
      "Epoch 1526/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 653.3245 - val_loss: 2084.6695\n",
      "Epoch 1527/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4023/4023 [==============================] - 0s 97us/step - loss: 597.8315 - val_loss: 2062.4214\n",
      "Epoch 1528/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 591.7606 - val_loss: 2088.5945\n",
      "Epoch 1529/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 597.0576 - val_loss: 2109.8852\n",
      "Epoch 1530/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 576.8580 - val_loss: 1995.0597\n",
      "Epoch 1531/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 635.0526 - val_loss: 2077.6145\n",
      "Epoch 1532/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 530.8809 - val_loss: 2012.1572\n",
      "Epoch 1533/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 579.6066 - val_loss: 2067.5076\n",
      "Epoch 1534/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 532.7868 - val_loss: 2024.4809\n",
      "Epoch 1535/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 510.2678 - val_loss: 2115.5692\n",
      "Epoch 1536/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 549.4759 - val_loss: 1965.1655\n",
      "Epoch 1537/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 703.9284 - val_loss: 2042.3619\n",
      "Epoch 1538/2000\n",
      "4023/4023 [==============================] - 0s 111us/step - loss: 548.7827 - val_loss: 1996.2675\n",
      "Epoch 1539/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 539.4497 - val_loss: 2012.8599\n",
      "Epoch 1540/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 552.3832 - val_loss: 2022.8461\n",
      "Epoch 1541/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 569.9521 - val_loss: 2023.7318\n",
      "Epoch 1542/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 511.2724 - val_loss: 1973.7017\n",
      "Epoch 1543/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 558.8038 - val_loss: 2104.0644\n",
      "Epoch 1544/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 569.7108 - val_loss: 2036.0378\n",
      "Epoch 1545/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 632.7104 - val_loss: 2079.6441\n",
      "Epoch 1546/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 499.0779 - val_loss: 2230.6971\n",
      "Epoch 1547/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 714.7441 - val_loss: 2160.7467\n",
      "Epoch 1548/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 629.7885 - val_loss: 2037.6642\n",
      "Epoch 1549/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 650.6286 - val_loss: 2108.6382\n",
      "Epoch 1550/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 652.5090 - val_loss: 2011.1565\n",
      "Epoch 1551/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 596.3076 - val_loss: 2086.3077\n",
      "Epoch 1552/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 584.2564 - val_loss: 1956.9489\n",
      "Epoch 1553/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 623.2129 - val_loss: 1985.4891\n",
      "Epoch 1554/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 613.5362 - val_loss: 2024.1391\n",
      "Epoch 1555/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 568.3362 - val_loss: 2016.4378\n",
      "Epoch 1556/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 569.3094 - val_loss: 2037.5015\n",
      "Epoch 1557/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 569.9620 - val_loss: 2040.4751\n",
      "Epoch 1558/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 562.9355 - val_loss: 2044.7135\n",
      "Epoch 1559/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 538.0206 - val_loss: 2043.7434\n",
      "Epoch 1560/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 611.6037 - val_loss: 2011.6015\n",
      "Epoch 1561/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 558.1319 - val_loss: 1998.2725\n",
      "Epoch 1562/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 611.9500 - val_loss: 2021.0640\n",
      "Epoch 1563/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 584.3296 - val_loss: 1957.1743\n",
      "Epoch 1564/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 627.3319 - val_loss: 2047.5888\n",
      "Epoch 1565/2000\n",
      "4023/4023 [==============================] - 0s 107us/step - loss: 624.9537 - val_loss: 2033.6533\n",
      "Epoch 1566/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 552.5083 - val_loss: 2000.5332\n",
      "Epoch 1567/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 547.8268 - val_loss: 1927.9180\n",
      "Epoch 1568/2000\n",
      "4023/4023 [==============================] - 0s 105us/step - loss: 488.1161 - val_loss: 1799.8971\n",
      "Epoch 1569/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 471.6993 - val_loss: 1847.2474\n",
      "Epoch 1570/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 551.7597 - val_loss: 1863.5877\n",
      "Epoch 1571/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 544.9500 - val_loss: 1740.5781\n",
      "Epoch 1572/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 445.1939 - val_loss: 1786.1864\n",
      "Epoch 1573/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 491.9026 - val_loss: 1701.2478\n",
      "Epoch 1574/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 421.8700 - val_loss: 1669.1934\n",
      "Epoch 1575/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 459.9327 - val_loss: 1660.1641\n",
      "Epoch 1576/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 424.9876 - val_loss: 1685.4218\n",
      "Epoch 1577/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 378.9400 - val_loss: 1661.9377\n",
      "Epoch 1578/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 443.9169 - val_loss: 1680.4678\n",
      "Epoch 1579/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 468.7717 - val_loss: 1641.2178\n",
      "Epoch 1580/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 430.8632 - val_loss: 1670.3799\n",
      "Epoch 1581/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 426.2514 - val_loss: 1577.1554\n",
      "Epoch 1582/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 403.7462 - val_loss: 1873.0051\n",
      "Epoch 1583/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 575.6954 - val_loss: 2204.2538\n",
      "Epoch 1584/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 527.6588 - val_loss: 2173.0950\n",
      "Epoch 1585/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 568.5625 - val_loss: 2157.0335\n",
      "Epoch 1586/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 583.8329 - val_loss: 2129.7483\n",
      "Epoch 1587/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 545.3086 - val_loss: 2161.5877\n",
      "Epoch 1588/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 556.8865 - val_loss: 2159.6477\n",
      "Epoch 1589/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 529.4301 - val_loss: 2184.5668\n",
      "Epoch 1590/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 639.9722 - val_loss: 2141.9117\n",
      "Epoch 1591/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 547.6761 - val_loss: 2119.9359\n",
      "Epoch 1592/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 594.0568 - val_loss: 2086.8119\n",
      "Epoch 1593/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 565.8161 - val_loss: 2136.1582\n",
      "Epoch 1594/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 581.6655 - val_loss: 2118.1437\n",
      "Epoch 1595/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 536.7095 - val_loss: 2148.7042\n",
      "Epoch 1596/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 497.7653 - val_loss: 2097.9804\n",
      "Epoch 1597/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 623.9406 - val_loss: 2203.6387\n",
      "Epoch 1598/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 554.4279 - val_loss: 2083.1960\n",
      "Epoch 1599/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4023/4023 [==============================] - 0s 91us/step - loss: 594.8929 - val_loss: 2089.4679\n",
      "Epoch 1600/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 537.3602 - val_loss: 2183.4966\n",
      "Epoch 1601/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 563.5836 - val_loss: 2141.9049\n",
      "Epoch 1602/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 532.9992 - val_loss: 2129.9399\n",
      "Epoch 1603/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 524.9384 - val_loss: 2135.7186\n",
      "Epoch 1604/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 508.6394 - val_loss: 2131.5460\n",
      "Epoch 1605/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 524.1952 - val_loss: 2113.7750\n",
      "Epoch 1606/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 445.9401 - val_loss: 2061.7220\n",
      "Epoch 1607/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 579.1796 - val_loss: 2169.6827\n",
      "Epoch 1608/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 567.2800 - val_loss: 2054.0910\n",
      "Epoch 1609/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 543.8659 - val_loss: 2103.5561\n",
      "Epoch 1610/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 520.1935 - val_loss: 2127.6790\n",
      "Epoch 1611/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 517.6184 - val_loss: 2112.5406\n",
      "Epoch 1612/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 554.4094 - val_loss: 2108.1556\n",
      "Epoch 1613/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 526.1169 - val_loss: 2100.6432\n",
      "Epoch 1614/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 559.9350 - val_loss: 2097.0606\n",
      "Epoch 1615/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 548.3347 - val_loss: 2062.2951\n",
      "Epoch 1616/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 715.6506 - val_loss: 2190.4582\n",
      "Epoch 1617/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 571.6169 - val_loss: 2172.7313\n",
      "Epoch 1618/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 602.5889 - val_loss: 2173.7669\n",
      "Epoch 1619/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 543.3433 - val_loss: 2151.1168\n",
      "Epoch 1620/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 576.4076 - val_loss: 2157.8424\n",
      "Epoch 1621/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 552.0805 - val_loss: 2162.1492\n",
      "Epoch 1622/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 561.2511 - val_loss: 2156.0714\n",
      "Epoch 1623/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 525.9089 - val_loss: 2156.3314\n",
      "Epoch 1624/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 600.4601 - val_loss: 2133.7585\n",
      "Epoch 1625/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 531.3293 - val_loss: 2149.4309\n",
      "Epoch 1626/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 516.0088 - val_loss: 2127.5401\n",
      "Epoch 1627/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 588.0869 - val_loss: 2150.9411\n",
      "Epoch 1628/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 585.6378 - val_loss: 2169.1684\n",
      "Epoch 1629/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 540.4213 - val_loss: 2167.4871\n",
      "Epoch 1630/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 572.3597 - val_loss: 2166.0034\n",
      "Epoch 1631/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 562.0544 - val_loss: 2190.9943\n",
      "Epoch 1632/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 548.3335 - val_loss: 2185.1233\n",
      "Epoch 1633/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 548.5225 - val_loss: 2189.0499\n",
      "Epoch 1634/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 539.0731 - val_loss: 2158.6148\n",
      "Epoch 1635/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 587.8346 - val_loss: 2204.2110\n",
      "Epoch 1636/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 522.5957 - val_loss: 2184.8204\n",
      "Epoch 1637/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 571.0446 - val_loss: 2186.8811\n",
      "Epoch 1638/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 561.4428 - val_loss: 2166.7550\n",
      "Epoch 1639/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 539.2085 - val_loss: 2170.5189\n",
      "Epoch 1640/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 588.3008 - val_loss: 2178.6028\n",
      "Epoch 1641/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 515.6280 - val_loss: 2176.1884\n",
      "Epoch 1642/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 527.3548 - val_loss: 2201.7249\n",
      "Epoch 1643/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 551.8948 - val_loss: 2217.2218\n",
      "Epoch 1644/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 550.2032 - val_loss: 2210.7425\n",
      "Epoch 1645/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 549.9985 - val_loss: 2229.8070\n",
      "Epoch 1646/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 538.3146 - val_loss: 2156.2245\n",
      "Epoch 1647/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 598.1975 - val_loss: 2193.5536\n",
      "Epoch 1648/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 541.8772 - val_loss: 2150.9195\n",
      "Epoch 1649/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 552.3997 - val_loss: 2184.5603\n",
      "Epoch 1650/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 521.2447 - val_loss: 2167.4684\n",
      "Epoch 1651/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 533.1468 - val_loss: 2174.6662\n",
      "Epoch 1652/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 550.9377 - val_loss: 2187.9443\n",
      "Epoch 1653/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 514.9144 - val_loss: 2183.6464\n",
      "Epoch 1654/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 565.0953 - val_loss: 2212.3024\n",
      "Epoch 1655/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 512.9015 - val_loss: 2191.1734\n",
      "Epoch 1656/2000\n",
      "4023/4023 [==============================] - 0s 107us/step - loss: 509.9419 - val_loss: 2187.0063\n",
      "Epoch 1657/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 492.3187 - val_loss: 2207.2880\n",
      "Epoch 1658/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 564.9262 - val_loss: 2173.7568\n",
      "Epoch 1659/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 455.9760 - val_loss: 2029.0014\n",
      "Epoch 1660/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 453.2190 - val_loss: 2268.3361\n",
      "Epoch 1661/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 535.2426 - val_loss: 2243.5262\n",
      "Epoch 1662/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 572.6831 - val_loss: 2210.5892\n",
      "Epoch 1663/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 520.0855 - val_loss: 2283.3208\n",
      "Epoch 1664/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 560.6557 - val_loss: 2237.5743\n",
      "Epoch 1665/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 517.2864 - val_loss: 2205.4335\n",
      "Epoch 1666/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 560.7264 - val_loss: 2159.8263\n",
      "Epoch 1667/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 563.4610 - val_loss: 2174.3989\n",
      "Epoch 1668/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 505.1311 - val_loss: 2123.4009\n",
      "Epoch 1669/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 520.9082 - val_loss: 2153.4331\n",
      "Epoch 1670/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 522.8116 - val_loss: 2076.6341\n",
      "Epoch 1671/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4023/4023 [==============================] - 0s 92us/step - loss: 551.6106 - val_loss: 2208.2787\n",
      "Epoch 1672/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 503.3654 - val_loss: 2140.5478\n",
      "Epoch 1673/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 459.2921 - val_loss: 2178.6934\n",
      "Epoch 1674/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 490.8599 - val_loss: 2135.1002\n",
      "Epoch 1675/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 546.4092 - val_loss: 2080.2653\n",
      "Epoch 1676/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 553.8476 - val_loss: 2172.0382\n",
      "Epoch 1677/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 507.7763 - val_loss: 2083.2866\n",
      "Epoch 1678/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 486.4463 - val_loss: 2165.2993\n",
      "Epoch 1679/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 519.4873 - val_loss: 2152.3552\n",
      "Epoch 1680/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 532.9183 - val_loss: 2149.9890\n",
      "Epoch 1681/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 495.5466 - val_loss: 2083.8169\n",
      "Epoch 1682/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 548.7761 - val_loss: 2139.9327\n",
      "Epoch 1683/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 557.1522 - val_loss: 2177.9899\n",
      "Epoch 1684/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 493.7418 - val_loss: 2150.9870\n",
      "Epoch 1685/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 512.7895 - val_loss: 2118.5448\n",
      "Epoch 1686/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 584.2166 - val_loss: 2181.4576\n",
      "Epoch 1687/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 544.6828 - val_loss: 2169.3463\n",
      "Epoch 1688/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 527.2730 - val_loss: 2187.3463\n",
      "Epoch 1689/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 566.0098 - val_loss: 2174.9123\n",
      "Epoch 1690/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 541.0029 - val_loss: 2281.8012\n",
      "Epoch 1691/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 564.7216 - val_loss: 2110.9704\n",
      "Epoch 1692/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 578.8504 - val_loss: 2144.6410\n",
      "Epoch 1693/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 515.8871 - val_loss: 2228.6604\n",
      "Epoch 1694/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 545.8748 - val_loss: 2138.1636\n",
      "Epoch 1695/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 529.1535 - val_loss: 2147.3157\n",
      "Epoch 1696/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 505.4935 - val_loss: 2182.9842\n",
      "Epoch 1697/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 540.9031 - val_loss: 2170.0785\n",
      "Epoch 1698/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 532.3654 - val_loss: 2169.6622\n",
      "Epoch 1699/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 500.9610 - val_loss: 2168.7535\n",
      "Epoch 1700/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 485.1049 - val_loss: 2143.1475\n",
      "Epoch 1701/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 543.6360 - val_loss: 2208.7006\n",
      "Epoch 1702/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 498.6229 - val_loss: 2191.5970\n",
      "Epoch 1703/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 475.0444 - val_loss: 2147.8749\n",
      "Epoch 1704/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 492.0077 - val_loss: 2192.6105\n",
      "Epoch 1705/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 507.5729 - val_loss: 2159.0487\n",
      "Epoch 1706/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 499.9901 - val_loss: 2145.2539\n",
      "Epoch 1707/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 479.4650 - val_loss: 2171.7198\n",
      "Epoch 1708/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 499.0442 - val_loss: 2159.7237\n",
      "Epoch 1709/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 533.1978 - val_loss: 2182.7862\n",
      "Epoch 1710/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 496.0463 - val_loss: 2135.6788\n",
      "Epoch 1711/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 459.5525 - val_loss: 2109.3531\n",
      "Epoch 1712/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 487.8588 - val_loss: 2141.4984\n",
      "Epoch 1713/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 467.4720 - val_loss: 2113.2161\n",
      "Epoch 1714/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 478.8038 - val_loss: 2110.5138\n",
      "Epoch 1715/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 489.8702 - val_loss: 2137.7050\n",
      "Epoch 1716/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 468.1094 - val_loss: 2123.4237\n",
      "Epoch 1717/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 489.4606 - val_loss: 2054.2920\n",
      "Epoch 1718/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 539.4721 - val_loss: 2158.1195\n",
      "Epoch 1719/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 504.1192 - val_loss: 2134.9164\n",
      "Epoch 1720/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 469.1901 - val_loss: 2033.7543\n",
      "Epoch 1721/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 485.7876 - val_loss: 2023.7683\n",
      "Epoch 1722/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 443.8179 - val_loss: 2073.2312\n",
      "Epoch 1723/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 483.4310 - val_loss: 2084.8784\n",
      "Epoch 1724/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 578.1265 - val_loss: 2187.7977\n",
      "Epoch 1725/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 509.1605 - val_loss: 1980.9117\n",
      "Epoch 1726/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 545.3635 - val_loss: 2073.5559\n",
      "Epoch 1727/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 541.2309 - val_loss: 2113.4435\n",
      "Epoch 1728/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 532.3250 - val_loss: 2107.2117\n",
      "Epoch 1729/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 501.8499 - val_loss: 2097.2268\n",
      "Epoch 1730/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 521.7110 - val_loss: 2080.1036\n",
      "Epoch 1731/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 512.8857 - val_loss: 2094.1455\n",
      "Epoch 1732/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 507.2972 - val_loss: 2096.8890\n",
      "Epoch 1733/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 519.6879 - val_loss: 2125.5813\n",
      "Epoch 1734/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 494.3926 - val_loss: 2075.1193\n",
      "Epoch 1735/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 515.6519 - val_loss: 2117.7208\n",
      "Epoch 1736/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 524.5217 - val_loss: 2048.5091\n",
      "Epoch 1737/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 489.4190 - val_loss: 2098.1098\n",
      "Epoch 1738/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 512.4492 - val_loss: 2080.5830\n",
      "Epoch 1739/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 522.6745 - val_loss: 2100.2980\n",
      "Epoch 1740/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 503.0010 - val_loss: 2110.3183\n",
      "Epoch 1741/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 493.7618 - val_loss: 2102.5934\n",
      "Epoch 1742/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 433.6494 - val_loss: 2095.9920\n",
      "Epoch 1743/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4023/4023 [==============================] - 0s 97us/step - loss: 453.0152 - val_loss: 2116.8077\n",
      "Epoch 1744/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 472.2735 - val_loss: 1973.1892\n",
      "Epoch 1745/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 462.8951 - val_loss: 2042.7898\n",
      "Epoch 1746/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 436.0651 - val_loss: 1985.6668\n",
      "Epoch 1747/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 449.2538 - val_loss: 2061.5422\n",
      "Epoch 1748/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 535.5588 - val_loss: 2102.5564\n",
      "Epoch 1749/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 493.7678 - val_loss: 1984.8205\n",
      "Epoch 1750/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 470.2959 - val_loss: 1967.6014\n",
      "Epoch 1751/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 404.0423 - val_loss: 1953.2026\n",
      "Epoch 1752/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 471.4245 - val_loss: 2059.4531\n",
      "Epoch 1753/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 526.1607 - val_loss: 2017.9340\n",
      "Epoch 1754/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 421.7135 - val_loss: 1908.8389\n",
      "Epoch 1755/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 516.6859 - val_loss: 2054.1941\n",
      "Epoch 1756/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 468.2301 - val_loss: 2049.6995\n",
      "Epoch 1757/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 506.3605 - val_loss: 2031.4535\n",
      "Epoch 1758/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 500.7229 - val_loss: 1980.8904\n",
      "Epoch 1759/2000\n",
      "4023/4023 [==============================] - 0s 109us/step - loss: 443.5265 - val_loss: 2029.5658\n",
      "Epoch 1760/2000\n",
      "4023/4023 [==============================] - 0s 110us/step - loss: 424.8580 - val_loss: 1981.4782\n",
      "Epoch 1761/2000\n",
      "4023/4023 [==============================] - 0s 123us/step - loss: 469.3070 - val_loss: 2016.5792\n",
      "Epoch 1762/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 475.0027 - val_loss: 2032.8698\n",
      "Epoch 1763/2000\n",
      "4023/4023 [==============================] - 0s 118us/step - loss: 543.4133 - val_loss: 1995.6251\n",
      "Epoch 1764/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 478.2281 - val_loss: 2022.3393\n",
      "Epoch 1765/2000\n",
      "4023/4023 [==============================] - 0s 120us/step - loss: 455.0048 - val_loss: 1930.3122\n",
      "Epoch 1766/2000\n",
      "4023/4023 [==============================] - 0s 114us/step - loss: 436.1712 - val_loss: 1937.8350\n",
      "Epoch 1767/2000\n",
      "4023/4023 [==============================] - 0s 117us/step - loss: 457.3313 - val_loss: 2024.7299\n",
      "Epoch 1768/2000\n",
      "4023/4023 [==============================] - 0s 112us/step - loss: 509.9350 - val_loss: 1980.7153\n",
      "Epoch 1769/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 450.9008 - val_loss: 1886.4209\n",
      "Epoch 1770/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 444.5627 - val_loss: 2049.2102\n",
      "Epoch 1771/2000\n",
      "4023/4023 [==============================] - 0s 113us/step - loss: 466.3916 - val_loss: 1904.2811\n",
      "Epoch 1772/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 446.2280 - val_loss: 1943.5043\n",
      "Epoch 1773/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 433.7340 - val_loss: 1873.2541\n",
      "Epoch 1774/2000\n",
      "4023/4023 [==============================] - 0s 108us/step - loss: 442.1667 - val_loss: 2000.5035\n",
      "Epoch 1775/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 469.6275 - val_loss: 1985.5344\n",
      "Epoch 1776/2000\n",
      "4023/4023 [==============================] - 0s 117us/step - loss: 493.1782 - val_loss: 1949.4936\n",
      "Epoch 1777/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 500.1796 - val_loss: 1994.2959\n",
      "Epoch 1778/2000\n",
      "4023/4023 [==============================] - 0s 117us/step - loss: 464.6252 - val_loss: 1847.7303\n",
      "Epoch 1779/2000\n",
      "4023/4023 [==============================] - 0s 107us/step - loss: 484.3116 - val_loss: 1896.0605\n",
      "Epoch 1780/2000\n",
      "4023/4023 [==============================] - 0s 107us/step - loss: 509.8490 - val_loss: 1855.6941\n",
      "Epoch 1781/2000\n",
      "4023/4023 [==============================] - 0s 120us/step - loss: 427.0706 - val_loss: 1943.8838\n",
      "Epoch 1782/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 507.4853 - val_loss: 2042.8983\n",
      "Epoch 1783/2000\n",
      "4023/4023 [==============================] - 0s 122us/step - loss: 470.9097 - val_loss: 1885.7218\n",
      "Epoch 1784/2000\n",
      "4023/4023 [==============================] - 0s 115us/step - loss: 457.5480 - val_loss: 1985.1660\n",
      "Epoch 1785/2000\n",
      "4023/4023 [==============================] - 0s 118us/step - loss: 447.2274 - val_loss: 1906.7367\n",
      "Epoch 1786/2000\n",
      "4023/4023 [==============================] - 0s 110us/step - loss: 530.3824 - val_loss: 2062.1681\n",
      "Epoch 1787/2000\n",
      "4023/4023 [==============================] - 0s 105us/step - loss: 509.1412 - val_loss: 1995.6978\n",
      "Epoch 1788/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 462.5626 - val_loss: 2062.8428\n",
      "Epoch 1789/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 463.8523 - val_loss: 1966.6519\n",
      "Epoch 1790/2000\n",
      "4023/4023 [==============================] - 0s 114us/step - loss: 435.7874 - val_loss: 1856.7176\n",
      "Epoch 1791/2000\n",
      "4023/4023 [==============================] - 0s 121us/step - loss: 434.2940 - val_loss: 1903.0569\n",
      "Epoch 1792/2000\n",
      "4023/4023 [==============================] - 0s 109us/step - loss: 486.3609 - val_loss: 1932.3020\n",
      "Epoch 1793/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 496.7058 - val_loss: 2017.9012\n",
      "Epoch 1794/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 474.8393 - val_loss: 1970.9074\n",
      "Epoch 1795/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 433.7841 - val_loss: 1831.6900\n",
      "Epoch 1796/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 439.7743 - val_loss: 1847.8840\n",
      "Epoch 1797/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 519.3438 - val_loss: 1811.4799\n",
      "Epoch 1798/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 405.8127 - val_loss: 1838.8480\n",
      "Epoch 1799/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 473.4323 - val_loss: 1856.9307\n",
      "Epoch 1800/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 484.5835 - val_loss: 1974.9565\n",
      "Epoch 1801/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 474.9188 - val_loss: 1802.0417\n",
      "Epoch 1802/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 443.6393 - val_loss: 1812.1554\n",
      "Epoch 1803/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 440.6472 - val_loss: 1906.3023\n",
      "Epoch 1804/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 472.5312 - val_loss: 1863.7860\n",
      "Epoch 1805/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 449.0789 - val_loss: 1852.9681\n",
      "Epoch 1806/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 411.5823 - val_loss: 1852.7849\n",
      "Epoch 1807/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 396.2142 - val_loss: 1850.1927\n",
      "Epoch 1808/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 423.5564 - val_loss: 1838.1734\n",
      "Epoch 1809/2000\n",
      "4023/4023 [==============================] - 2s 412us/step - loss: 423.1670 - val_loss: 1793.8430\n",
      "Epoch 1810/2000\n",
      "4023/4023 [==============================] - 2s 393us/step - loss: 433.9898 - val_loss: 1779.9194\n",
      "Epoch 1811/2000\n",
      "4023/4023 [==============================] - 2s 402us/step - loss: 404.4742 - val_loss: 1870.0938\n",
      "Epoch 1812/2000\n",
      "4023/4023 [==============================] - 2s 399us/step - loss: 479.5093 - val_loss: 1831.7333\n",
      "Epoch 1813/2000\n",
      "4023/4023 [==============================] - 2s 426us/step - loss: 445.6160 - val_loss: 1915.3924\n",
      "Epoch 1814/2000\n",
      "4023/4023 [==============================] - 2s 453us/step - loss: 444.4387 - val_loss: 1759.7593\n",
      "Epoch 1815/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4023/4023 [==============================] - 2s 537us/step - loss: 434.3476 - val_loss: 1758.0795\n",
      "Epoch 1816/2000\n",
      "4023/4023 [==============================] - 2s 508us/step - loss: 456.0544 - val_loss: 1776.9360\n",
      "Epoch 1817/2000\n",
      "4023/4023 [==============================] - 2s 434us/step - loss: 437.2320 - val_loss: 1787.4174\n",
      "Epoch 1818/2000\n",
      "4023/4023 [==============================] - 2s 420us/step - loss: 402.2371 - val_loss: 1815.7503\n",
      "Epoch 1819/2000\n",
      "4023/4023 [==============================] - 1s 363us/step - loss: 420.1544 - val_loss: 1970.1445\n",
      "Epoch 1820/2000\n",
      "4023/4023 [==============================] - 1s 350us/step - loss: 483.1443 - val_loss: 1933.4368\n",
      "Epoch 1821/2000\n",
      "4023/4023 [==============================] - 1s 302us/step - loss: 427.3329 - val_loss: 1747.4862\n",
      "Epoch 1822/2000\n",
      "4023/4023 [==============================] - 1s 262us/step - loss: 461.5740 - val_loss: 1843.2610\n",
      "Epoch 1823/2000\n",
      "4023/4023 [==============================] - 1s 237us/step - loss: 476.3004 - val_loss: 1791.5564\n",
      "Epoch 1824/2000\n",
      "4023/4023 [==============================] - 1s 234us/step - loss: 442.1744 - val_loss: 1780.7770\n",
      "Epoch 1825/2000\n",
      "4023/4023 [==============================] - 1s 227us/step - loss: 407.1514 - val_loss: 1721.0891\n",
      "Epoch 1826/2000\n",
      "4023/4023 [==============================] - 1s 233us/step - loss: 391.5538 - val_loss: 1743.3747\n",
      "Epoch 1827/2000\n",
      "4023/4023 [==============================] - 1s 211us/step - loss: 359.0310 - val_loss: 1843.6089\n",
      "Epoch 1828/2000\n",
      "4023/4023 [==============================] - 1s 217us/step - loss: 413.9676 - val_loss: 1739.8017\n",
      "Epoch 1829/2000\n",
      "4023/4023 [==============================] - 1s 192us/step - loss: 369.8125 - val_loss: 1723.8375\n",
      "Epoch 1830/2000\n",
      "4023/4023 [==============================] - 1s 189us/step - loss: 429.8669 - val_loss: 1834.7019\n",
      "Epoch 1831/2000\n",
      "4023/4023 [==============================] - 1s 184us/step - loss: 395.4015 - val_loss: 1828.3429\n",
      "Epoch 1832/2000\n",
      "4023/4023 [==============================] - 1s 168us/step - loss: 422.1583 - val_loss: 1729.5754\n",
      "Epoch 1833/2000\n",
      "4023/4023 [==============================] - 1s 160us/step - loss: 411.7233 - val_loss: 1899.7167\n",
      "Epoch 1834/2000\n",
      "4023/4023 [==============================] - 1s 166us/step - loss: 463.8446 - val_loss: 1796.1371\n",
      "Epoch 1835/2000\n",
      "4023/4023 [==============================] - 1s 154us/step - loss: 375.5102 - val_loss: 1757.1880\n",
      "Epoch 1836/2000\n",
      "4023/4023 [==============================] - 1s 174us/step - loss: 407.4907 - val_loss: 1780.1094\n",
      "Epoch 1837/2000\n",
      "4023/4023 [==============================] - 1s 160us/step - loss: 367.7231 - val_loss: 1887.4619\n",
      "Epoch 1838/2000\n",
      "4023/4023 [==============================] - 1s 165us/step - loss: 457.5678 - val_loss: 1771.4299\n",
      "Epoch 1839/2000\n",
      "4023/4023 [==============================] - 1s 156us/step - loss: 452.2273 - val_loss: 1698.8300\n",
      "Epoch 1840/2000\n",
      "4023/4023 [==============================] - 1s 148us/step - loss: 396.3480 - val_loss: 1795.4487\n",
      "Epoch 1841/2000\n",
      "4023/4023 [==============================] - 1s 151us/step - loss: 403.3983 - val_loss: 1680.6854\n",
      "Epoch 1842/2000\n",
      "4023/4023 [==============================] - 1s 147us/step - loss: 406.1193 - val_loss: 1876.3231\n",
      "Epoch 1843/2000\n",
      "4023/4023 [==============================] - 1s 146us/step - loss: 417.5164 - val_loss: 1803.7745\n",
      "Epoch 1844/2000\n",
      "4023/4023 [==============================] - 1s 138us/step - loss: 440.3694 - val_loss: 1707.5589\n",
      "Epoch 1845/2000\n",
      "4023/4023 [==============================] - 1s 137us/step - loss: 365.0391 - val_loss: 1762.5449\n",
      "Epoch 1846/2000\n",
      "4023/4023 [==============================] - 1s 139us/step - loss: 425.6388 - val_loss: 1702.0408\n",
      "Epoch 1847/2000\n",
      "4023/4023 [==============================] - 1s 135us/step - loss: 377.3413 - val_loss: 1725.4454\n",
      "Epoch 1848/2000\n",
      "4023/4023 [==============================] - 1s 143us/step - loss: 364.1263 - val_loss: 1815.4320\n",
      "Epoch 1849/2000\n",
      "4023/4023 [==============================] - 0s 123us/step - loss: 402.2519 - val_loss: 1689.0491\n",
      "Epoch 1850/2000\n",
      "4023/4023 [==============================] - 0s 123us/step - loss: 349.4044 - val_loss: 1684.5919\n",
      "Epoch 1851/2000\n",
      "4023/4023 [==============================] - 0s 120us/step - loss: 384.4985 - val_loss: 1657.7387\n",
      "Epoch 1852/2000\n",
      "4023/4023 [==============================] - 0s 120us/step - loss: 372.5455 - val_loss: 1670.6129\n",
      "Epoch 1853/2000\n",
      "4023/4023 [==============================] - 0s 119us/step - loss: 349.5665 - val_loss: 1828.8894\n",
      "Epoch 1854/2000\n",
      "4023/4023 [==============================] - 0s 120us/step - loss: 428.5689 - val_loss: 1773.0525\n",
      "Epoch 1855/2000\n",
      "4023/4023 [==============================] - 0s 116us/step - loss: 471.3151 - val_loss: 1840.6457\n",
      "Epoch 1856/2000\n",
      "4023/4023 [==============================] - 0s 117us/step - loss: 446.7067 - val_loss: 1668.2262\n",
      "Epoch 1857/2000\n",
      "4023/4023 [==============================] - 0s 114us/step - loss: 453.6501 - val_loss: 1822.5319\n",
      "Epoch 1858/2000\n",
      "4023/4023 [==============================] - 0s 115us/step - loss: 467.1937 - val_loss: 1648.8289\n",
      "Epoch 1859/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 423.8170 - val_loss: 1675.5410\n",
      "Epoch 1860/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 355.4413 - val_loss: 1643.0186\n",
      "Epoch 1861/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 377.6839 - val_loss: 1660.1125\n",
      "Epoch 1862/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 365.6505 - val_loss: 1651.5931\n",
      "Epoch 1863/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 362.1200 - val_loss: 1652.4377\n",
      "Epoch 1864/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 343.7082 - val_loss: 1638.8573\n",
      "Epoch 1865/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 346.0723 - val_loss: 1612.5318\n",
      "Epoch 1866/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 363.8419 - val_loss: 1629.2787\n",
      "Epoch 1867/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 358.7197 - val_loss: 1630.4696\n",
      "Epoch 1868/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 332.0484 - val_loss: 1631.5318\n",
      "Epoch 1869/2000\n",
      "4023/4023 [==============================] - 0s 105us/step - loss: 352.6603 - val_loss: 1618.1876\n",
      "Epoch 1870/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 329.3421 - val_loss: 1628.1425\n",
      "Epoch 1871/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 344.8288 - val_loss: 1595.1305\n",
      "Epoch 1872/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 373.3437 - val_loss: 1583.4907\n",
      "Epoch 1873/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 369.9638 - val_loss: 1600.2124\n",
      "Epoch 1874/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 379.6384 - val_loss: 1708.0856\n",
      "Epoch 1875/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 410.0779 - val_loss: 1711.7538\n",
      "Epoch 1876/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 391.2421 - val_loss: 1627.8273\n",
      "Epoch 1877/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 374.4955 - val_loss: 1637.9107\n",
      "Epoch 1878/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 405.6601 - val_loss: 1638.5050\n",
      "Epoch 1879/2000\n",
      "4023/4023 [==============================] - 0s 112us/step - loss: 389.0064 - val_loss: 1657.8354\n",
      "Epoch 1880/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 373.8111 - val_loss: 1727.8669\n",
      "Epoch 1881/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 413.6171 - val_loss: 1697.3664\n",
      "Epoch 1882/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 374.6321 - val_loss: 1555.5464\n",
      "Epoch 1883/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 396.4379 - val_loss: 1580.7356\n",
      "Epoch 1884/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 407.7749 - val_loss: 1583.4425\n",
      "Epoch 1885/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 376.1162 - val_loss: 1569.8123\n",
      "Epoch 1886/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 424.6677 - val_loss: 1752.0077\n",
      "Epoch 1887/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4023/4023 [==============================] - 0s 92us/step - loss: 436.2211 - val_loss: 1612.3469\n",
      "Epoch 1888/2000\n",
      "4023/4023 [==============================] - 0s 107us/step - loss: 379.5796 - val_loss: 1651.7972\n",
      "Epoch 1889/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 409.8080 - val_loss: 1684.0283\n",
      "Epoch 1890/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 417.6898 - val_loss: 1586.2746\n",
      "Epoch 1891/2000\n",
      "4023/4023 [==============================] - 1s 173us/step - loss: 401.5913 - val_loss: 1623.8346\n",
      "Epoch 1892/2000\n",
      "4023/4023 [==============================] - 1s 132us/step - loss: 374.7484 - val_loss: 1673.9911\n",
      "Epoch 1893/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 379.1349 - val_loss: 1675.0099\n",
      "Epoch 1894/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 388.0177 - val_loss: 1626.9511\n",
      "Epoch 1895/2000\n",
      "4023/4023 [==============================] - 0s 111us/step - loss: 397.0487 - val_loss: 1625.1162\n",
      "Epoch 1896/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 336.3850 - val_loss: 1667.5711\n",
      "Epoch 1897/2000\n",
      "4023/4023 [==============================] - 0s 117us/step - loss: 442.6212 - val_loss: 1782.2365\n",
      "Epoch 1898/2000\n",
      "4023/4023 [==============================] - 0s 112us/step - loss: 409.3189 - val_loss: 1605.9137\n",
      "Epoch 1899/2000\n",
      "4023/4023 [==============================] - 0s 104us/step - loss: 361.7895 - val_loss: 1882.3982\n",
      "Epoch 1900/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 520.3446 - val_loss: 1621.0876\n",
      "Epoch 1901/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 355.7274 - val_loss: 1661.9968\n",
      "Epoch 1902/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 366.3115 - val_loss: 1615.9785\n",
      "Epoch 1903/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 398.5002 - val_loss: 1891.7668\n",
      "Epoch 1904/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 503.5451 - val_loss: 1687.1063\n",
      "Epoch 1905/2000\n",
      "4023/4023 [==============================] - 0s 107us/step - loss: 360.2521 - val_loss: 1708.9847\n",
      "Epoch 1906/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 379.1008 - val_loss: 1730.8991\n",
      "Epoch 1907/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 494.9666 - val_loss: 1708.6204\n",
      "Epoch 1908/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 386.9910 - val_loss: 1666.4457\n",
      "Epoch 1909/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 391.7363 - val_loss: 1618.1269\n",
      "Epoch 1910/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 367.8668 - val_loss: 1622.6438\n",
      "Epoch 1911/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 413.0214 - val_loss: 1634.8328\n",
      "Epoch 1912/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 342.9116 - val_loss: 1664.6478\n",
      "Epoch 1913/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 395.7619 - val_loss: 1604.4031\n",
      "Epoch 1914/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 386.7628 - val_loss: 1881.6540\n",
      "Epoch 1915/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 514.7665 - val_loss: 1608.7724\n",
      "Epoch 1916/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 366.6323 - val_loss: 1624.4770\n",
      "Epoch 1917/2000\n",
      "4023/4023 [==============================] - 0s 97us/step - loss: 349.7572 - val_loss: 1661.2506\n",
      "Epoch 1918/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 368.7675 - val_loss: 1625.7298\n",
      "Epoch 1919/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 410.8512 - val_loss: 1624.3458\n",
      "Epoch 1920/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 339.3370 - val_loss: 1577.1691\n",
      "Epoch 1921/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 355.6008 - val_loss: 1585.5319\n",
      "Epoch 1922/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 358.0382 - val_loss: 1566.8005\n",
      "Epoch 1923/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 330.5811 - val_loss: 1574.1538\n",
      "Epoch 1924/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 383.6124 - val_loss: 1561.0195\n",
      "Epoch 1925/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 339.9649 - val_loss: 1557.6781\n",
      "Epoch 1926/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 346.9638 - val_loss: 1597.6095\n",
      "Epoch 1927/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 359.2566 - val_loss: 1580.7471\n",
      "Epoch 1928/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 349.8078 - val_loss: 1599.0831\n",
      "Epoch 1929/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 356.4385 - val_loss: 1548.1617\n",
      "Epoch 1930/2000\n",
      "4023/4023 [==============================] - 0s 94us/step - loss: 375.6699 - val_loss: 1573.2278\n",
      "Epoch 1931/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 347.5294 - val_loss: 1595.4028\n",
      "Epoch 1932/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 433.1677 - val_loss: 1668.9126\n",
      "Epoch 1933/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 412.7978 - val_loss: 1552.7977\n",
      "Epoch 1934/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 337.5957 - val_loss: 1574.9619\n",
      "Epoch 1935/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 429.5685 - val_loss: 1555.6282\n",
      "Epoch 1936/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 333.4642 - val_loss: 1559.4233\n",
      "Epoch 1937/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 350.8895 - val_loss: 1660.2632\n",
      "Epoch 1938/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 429.0135 - val_loss: 1603.5083\n",
      "Epoch 1939/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 424.6786 - val_loss: 1585.8161\n",
      "Epoch 1940/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 384.9363 - val_loss: 1574.9077\n",
      "Epoch 1941/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 307.1639 - val_loss: 1559.2586\n",
      "Epoch 1942/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 330.1065 - val_loss: 1508.5287\n",
      "Epoch 1943/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 345.5602 - val_loss: 1630.4853\n",
      "Epoch 1944/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 387.0487 - val_loss: 1529.9617\n",
      "Epoch 1945/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 386.3434 - val_loss: 1739.8177\n",
      "Epoch 1946/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 450.1857 - val_loss: 1507.7330\n",
      "Epoch 1947/2000\n",
      "4023/4023 [==============================] - 0s 106us/step - loss: 354.7285 - val_loss: 1517.2212\n",
      "Epoch 1948/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 335.2161 - val_loss: 1547.4268\n",
      "Epoch 1949/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 398.1502 - val_loss: 1533.8271\n",
      "Epoch 1950/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 336.5937 - val_loss: 1522.1340\n",
      "Epoch 1951/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 345.4579 - val_loss: 1490.9546\n",
      "Epoch 1952/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 340.9353 - val_loss: 1682.8655\n",
      "Epoch 1953/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 396.7486 - val_loss: 1534.9900\n",
      "Epoch 1954/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 327.9872 - val_loss: 1774.9964\n",
      "Epoch 1955/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 410.8100 - val_loss: 1513.3537\n",
      "Epoch 1956/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 325.9244 - val_loss: 1506.2900\n",
      "Epoch 1957/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 295.5623 - val_loss: 1518.5667\n",
      "Epoch 1958/2000\n",
      "4023/4023 [==============================] - 0s 95us/step - loss: 343.1784 - val_loss: 1510.6481\n",
      "Epoch 1959/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4023/4023 [==============================] - 0s 97us/step - loss: 343.6358 - val_loss: 1518.0737\n",
      "Epoch 1960/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 401.2253 - val_loss: 1509.7411\n",
      "Epoch 1961/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 351.2520 - val_loss: 1496.7008\n",
      "Epoch 1962/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 318.0951 - val_loss: 1498.9848\n",
      "Epoch 1963/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 305.5580 - val_loss: 1505.6971\n",
      "Epoch 1964/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 318.0582 - val_loss: 1581.7207\n",
      "Epoch 1965/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 396.2090 - val_loss: 1544.5208\n",
      "Epoch 1966/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 381.0194 - val_loss: 1508.9902\n",
      "Epoch 1967/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 355.7058 - val_loss: 1558.1547\n",
      "Epoch 1968/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 355.0020 - val_loss: 1638.1339\n",
      "Epoch 1969/2000\n",
      "4023/4023 [==============================] - 0s 92us/step - loss: 442.0760 - val_loss: 1514.3353\n",
      "Epoch 1970/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 313.1334 - val_loss: 1508.7612\n",
      "Epoch 1971/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 353.0795 - val_loss: 1480.9175\n",
      "Epoch 1972/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 331.6666 - val_loss: 1509.7724\n",
      "Epoch 1973/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 346.2912 - val_loss: 1607.0640\n",
      "Epoch 1974/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 394.2145 - val_loss: 1608.9625\n",
      "Epoch 1975/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 396.8820 - val_loss: 1501.0108\n",
      "Epoch 1976/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 320.3209 - val_loss: 1480.4729\n",
      "Epoch 1977/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 340.6675 - val_loss: 1667.6266\n",
      "Epoch 1978/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 512.9804 - val_loss: 1539.6091\n",
      "Epoch 1979/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 387.4480 - val_loss: 1453.4650\n",
      "Epoch 1980/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 351.5018 - val_loss: 1498.5174\n",
      "Epoch 1981/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 364.1315 - val_loss: 1516.3613\n",
      "Epoch 1982/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 358.8173 - val_loss: 1539.3882\n",
      "Epoch 1983/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 387.6350 - val_loss: 1505.0206\n",
      "Epoch 1984/2000\n",
      "4023/4023 [==============================] - 0s 93us/step - loss: 373.4084 - val_loss: 1516.3785\n",
      "Epoch 1985/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 366.7788 - val_loss: 1496.5168\n",
      "Epoch 1986/2000\n",
      "4023/4023 [==============================] - 0s 91us/step - loss: 335.0171 - val_loss: 1553.0429\n",
      "Epoch 1987/2000\n",
      "4023/4023 [==============================] - 0s 89us/step - loss: 365.8390 - val_loss: 1516.6183\n",
      "Epoch 1988/2000\n",
      "4023/4023 [==============================] - 0s 90us/step - loss: 357.9027 - val_loss: 1510.9873\n",
      "Epoch 1989/2000\n",
      "4023/4023 [==============================] - 0s 101us/step - loss: 338.8829 - val_loss: 1467.7830\n",
      "Epoch 1990/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 314.9268 - val_loss: 1571.3678\n",
      "Epoch 1991/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 382.6899 - val_loss: 1740.0847\n",
      "Epoch 1992/2000\n",
      "4023/4023 [==============================] - 0s 102us/step - loss: 418.6306 - val_loss: 1465.9630\n",
      "Epoch 1993/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 317.9278 - val_loss: 1464.3300\n",
      "Epoch 1994/2000\n",
      "4023/4023 [==============================] - 0s 96us/step - loss: 379.3824 - val_loss: 1480.4598\n",
      "Epoch 1995/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 330.6898 - val_loss: 1485.0933\n",
      "Epoch 1996/2000\n",
      "4023/4023 [==============================] - 0s 99us/step - loss: 330.4923 - val_loss: 1478.4744\n",
      "Epoch 1997/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 357.5564 - val_loss: 1462.0777\n",
      "Epoch 1998/2000\n",
      "4023/4023 [==============================] - 0s 100us/step - loss: 312.5238 - val_loss: 1498.6027\n",
      "Epoch 1999/2000\n",
      "4023/4023 [==============================] - 0s 103us/step - loss: 340.0437 - val_loss: 1933.4678\n",
      "Epoch 2000/2000\n",
      "4023/4023 [==============================] - 0s 98us/step - loss: 462.2834 - val_loss: 1810.1708\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAETCAYAAAAs4pGmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsnXd4W+X1xz+S5b3jEceOs5M3CSRkE8gOM4xSyigtBQottPwClJYyCmGEFTZtCKOEEUbZO4Ews/feyc0etuPEe1u2LP3+0IhkS7JkS5YTn8/z5Inue99777HG/d73nPOeV2exWBAEQRCExuhDbYAgCILQPhGBEARBENwiAiEIgiC4RQRCEARBcIsIhCAIguAWEQhBEATBLSIQghAAlFLzlFJ/bKbPRKXUNl/bBSHUiEAIgiAIbjGE2gBBaGuUUhOBGcBhQAFVwFPAHbbtzzVN+7ut7y229gbgGHCbpmm7lVKZwDtAJnAISHc6/wDgP0AKEAbM1DTtLR9tSwReBoYAFmA+cL+maSal1HTgcqAOKAL+qGnaUU/tLXt3BOEEMoIQOiojgac0TRsClAP/Ai4GhgFTlVKZSqnJwD3AJE3TzgA+AL5SSumw3sRXaZp2GlYB6Q+glDIAnwH3aZo2HJgA/FMpNdpHu2ZivckPAkYAZ9iOzwbuBEZqmjYC+BE401N7a94YQbAjAiF0VA5omrbR9nofsFDTtDpN0wqxCkYn4ELgY03TCgA0TZsDZAE9gHOBObb2vcAC27n6Ab2Bt5RSm4DFQDQw1Ee7pgCzNE2zaJpmBF6zteUCm4ENSqnngE2apn3lpV0QWo0IhNBRMTbarnfTJwyrm8cZHRBua9c5tZucjinTNG2I/R8wGnjbR7v0ja6pB8I1TTNjHY38EesI40Wl1DOe2n28liB4RQRCEDzzPXCNUioNQCl1I9ab8F7bvlts7d2ASbZjNKBGKfUH275sYBsw3Mdr/gDcppTSKaUibdf4SSl1hu08OzVNmwG8CIz01N66P1sQrIhACIIHNE37CesNd4FSajtwA3CJ7al9KjBQKbUTeBPYZDumDrgM+LNSagvWmMCDmqYt9/Gyd2ANeG+1/dOAJzRN2wx8AqxTSq0DbgL+4am99X+9IIBOyn0LgiAI7pARhCAIguAWEQhBEATBLSIQgiAIgltEIARBEAS3nFKlNgoKKloccU9OjqGkpDqQ5gQEscs/xC7/ELv841S1Ky0tXueuXUYQNgyGsFCb4Baxyz/ELv8Qu/yjo9klAiEIgiC4RQRCEARBcIsIhCAIguAWEQhBEATBLSIQgiAIgltEIARBEAS3iEAIgiAIbhGBAJbnrmZD3rZQmyEIgtCuEIEAvtz3HR9v+ybUZgiCIGA0Gpk717dVY7/7bi7Lli0Omi0iEEC43kBtfeMVKAVBENqe4uIinwXioosuZezYCUGz5ZSqxdRSIsIiqDWJQAiC4MonC/aydtdxx3ZYmI6GhtYtsjayfzpXT+7jcf+7777FwYMHGDduJCNGjKKmpob77nuQ77//ll27dlBdXU2PHj25//6HefPN/5KSksLgwQN5+eXXCA83cPRoHpMnn8cNN/ypVXaCCAQAkWERFBurQm2GIAgC119/E/v27eXMM8+ioqKCO+/8J1VVlcTHx/Pvf7+C2WzmuuuupqDguMtxx44dZc6cD6mvr+fXv75QBCJQRIZFUmsyYrFY0OncFjUUBKEDcvXkPi5P+2lp8RQUVLTZ9bt16w5AZGQUJSUlPPzw/cTExFBTU4PJZHLp26tXHwwGAwaDgcjIqIBcXwQC6wjCYrFQbzYRERYeanMEQejA6HR6LBYzAHq99YF11arlHD9+jEcfnUFJSQlLlizEYrE0Oi7wtohAYB1BABgbjCIQgiCElOTkZOrrTRiNJ+KiAwacxpw5b3LLLX8kIiKCzMwsCgsLgm6LCAQQGx4NQFV9FfERcSG2RhCEjkxkZCRz5nzg0paSksobb7zbpO/gwUMAq+urV6+BjvZvvvkhILYEVSCUUv8CfgVEAK8Ai4E5gAXYBkzVNM2slLoZ+AtgAh7XNG2eUioaeB9IByqAGzRNC4pkJkYkAFBmrCAjtnMwLiEIgnDSEbR5EEqpicDZwBhgApANvABM0zRtHKADLlNKZQB32PpdAMxQSkUCtwJbbX3fBaYFy9aESJtA1JUH6xKCIAgnHcGcKHcBsBX4EpgLzAOGYx1FAMwHzgVGAcs1TTNqmlYG7AUGA2OB7xv1DQpJdoEwikAIgiDYCaaLKRXoDlwC9AS+AfSaptlD7xVAIpAAlDkd567d3uaV5OSYFq3N2j0sA4C6sFrS0uL9Pj7YtEebQOzyF7HLP8Qu/wiGXcEUiCJgl6ZpdYCmlKrF6mayEw+UAuW2197a7W1eKSmpbpGhZqNVVPJLC9s0x9kX2jrv2lfELv8Qu/xD7PKP1trlSVyC6WJaBlyolNIppTKBWOAXW2wCYAqwFFgDjFNKRSmlEoEBWAPYy4GLGvUNCvHhcejQUWZsfx+8IAhCqAiaQGiaNg/YiFUA5gJTgbuA6UqplVgzmz7TNC0fmIlVABYAD2iaVgu8CpymlFoG3AJMD5atYfowEqPiKTWWNd9ZEAQhiPhTzdXO2rVr2bt3T8BtCWqaq6Zp97hpblJ6UNO02cDsRm3VwFVBMq0JaTGd2F9yBLPFjF4nRW4FQQgN9mqul176a5+P+fzzzxkzZhJ9+vQNqC0yUc5GWmwKe4oPUmYsJzkqKdTmCILQDvhi7zw2Ht/q2A7T62gwt66a69D0QfymzyUe99urub711uvs37+XsjKrZ+POO++md+8+PPHEI+Tm5lBXV8fvfvcHsrKyWbp0KVu2bKVHj15kZGS0yj5nRCBspMelAlBUWyICIQhCyLBXc62trWX48FFcfvmVHDlymCefnM7zz89kw4Z1vPHGe+h0OtasWUX//gMYN24cY8ZMCqg4gAiEg7SYFACKaorpk9QzxNYIgtAe+E2fS1ye9tsyi2n//r1s2LCOX375EYCKigpiYmL5+9/v4ZlnnqC6uorzz58SVBtEIGykxVoFori2JMSWCILQkbFXc+3evQfnnz+Q88+/kJKSYubO/YrCwkI0bSczZjyH0Wjkiisu5oILLkKn0zkqwAYSEQgb6bGdAKuLSRAEIVTYq7lWV1ezcOFPfPPNF1RXV3HTTbeQkpJCcXERN974e6KjY7jmmj9gMBg444wzeO21WXTpkkWPHoHzgIhA2EiNPeFiEgRBCBXuqrk6c/fd9zdpu+aaazjnnIsDbovkc9qICAsnMSJeRhCCIAg2RCCc6BTViRJjKQ3mhlCbIgiCEHJEIJxIiU7GbDFL2W9BEAREIFxIibIFqmvEzSQIgiAC4URKVDIARbUSqBYEQRCBcKJTtF0gZAQhCIIgAuGEfQRRLC4mQRAEEQhnkqOS0aETF5MgCAIiEC6E6w0kRiZIuQ1BEAREIJrQKSqZEmOZzIUQBKHDIwLRiJQo61wIWV1OEISOjghEI1KipWifIAgCiEA0wTEXQor2CYLQwRGBaERqtLWqa0FNUYgtEQRBCC0iEI3oHJMOQH718RBbIgiCEFpEIBqREBFHtCGa/CoRCEEQOjYiEI3Q6XRkxKRTUFMoqa6CIHRoRCDckBGbjtlipqCmMNSmCIIghAwRCDdkxNriEOJmEgShAxPUNamVUhsB+4yzA8ATwBzAAmwDpmqaZlZK3Qz8BTABj2uaNk8pFQ28D6QDFcANmqYVBNNeO51j0gAJVAuC0LEJ2ghCKRUFoGnaRNu/G4EXgGmapo0DdMBlSqkM4A5gDHABMEMpFQncCmy19X0XmBYsWxuTEdMZkBGEIAgdm2COIM4AYpRSP9qucz8wHFhs2z8fOB9oAJZrmmYEjEqpvcBgYCzwjFPfB5u7YHJyDAZDWIsNTkuLByDFHEv4GgOFdYWOtlDSHmxwh9jlH2KXf4hd/hEMu4IpENXAc8AbQF+sN3mdpmkW2/4KIBFI4IQbylO7vc0rJSXVLTY2LS2egoIKx3Z6TBq5ZfkcO16GXhe6UE1ju9oLYpd/iF3+IXb5R2vt8iQuwbzz7Qbe1zTNomnabqAI6Oy0Px4oBcptr72129vajIyYdOrM9VK0TxCEDkswBeIm4HkApVQm1hHBj0qpibb9U4ClwBpgnFIqSimVCAzAGsBeDlzUqG+b0VkymQRB6OAEUyDeBJKUUsuAj7EKxt+A6UqplUAE8JmmafnATKwCsAB4QNO0WuBV4DTb8bcA04NoaxMypOSGIAgdnKDFIDRNqwN+72bXBDd9ZwOzG7VVA1cFx7rmkbkQgiB0dGSinAfSo1PRoROBEAShwyIC4YHwsHBSojtxTFxMgiB0UEQgvNAlNp3K+ioq6ipDbYogCEKbIwLhhazYLgDkVOaF2BJBEIS2RwTCC1nxmQDkVh4NsSWCIAhtjwiEF7LirCMIEQhBEDoiIhBeSItOIVwfLgIhCEKHRATCC3qdnsy4DPKrjmMym0JtjiAIQpsiAtEMXeO60GBp4Fh1myxFIQiC0G4QgWiGrDhroDqnQjKZBEHoWIhANIMEqgVB6KiIQDRDVlwGIAIhCELHQwSiGaIN0aREJYtACILQ4RCB8IHMuC5U1FdSZmx/K0kJgiAECxEIH+jqiENIoFoQhI6DCIQPZMdnAXC4IjfElgiCILQdIhA+0D0hG4DD5UdCbIkgCELbIQLhA4kRCSRExHOoIifUpgiCILQZIhA+oNPp6J7QlVJjmQSqBUHoMIhA+Ei3+K4AHK4QN5MgCB0DEQgfcQhEubiZBEHoGIhA+Ig9UH1QAtWCIHQQRCB8JD4ijtToFA6UH8ZsMYfaHEEQhKAjAuEHvRN7UGOqIb/qeKhNEQRBCDqGYJ5cKZUOrAfOA0zAHMACbAOmappmVkrdDPzFtv9xTdPmKaWigfeBdKACuEHTtJAvyNArsTur89ezv+wgmbYifoIgCKcqQRtBKKXCgf8CNbamF4BpmqaNA3TAZUqpDOAOYAxwATBDKRUJ3ApstfV9F5gWLDv9oVdiDwD2lx0KrSGCIAhtQDBdTM8BrwH2AkbDgcW21/OBc4FRwHJN04yappUBe4HBwFjg+0Z9Q05GbDrRhigOiEAIgtABCIqLSSn1R6BA07QflFL/sjXrNE2z2F5XAIlAAlDmdKi7dntbsyQnx2AwhLXY7rS0+Gb79Evtyeb8nUQm6EiIjGvxtfzBF7tCgdjlH2KXf4hd/hEMu4IVg7gJsCilzgWGYHUTpTvtjwdKgXLba2/t9rZmKSmpbrHBaWnxFBQ0P0s6KyqLzexk3f7tDEod2OLrBdqutkbs8g+xyz/ELv9orV2exCUoLiZN08ZrmjZB07SJwCbgemC+UmqircsUYCmwBhinlIpSSiUCA7AGsJcDFzXqGzS+Wrqfn1b75jbqmdgdgANlh4NpkiAIQshpyzTXu4DpSqmVQATwmaZp+cBMrAKwAHhA07Ra4FXgNKXUMuAWYHowDftx7RHmLT/gU9+eid0I04Wxo2hXME0SBEEIOUFNcwWwjSLsTHCzfzYwu1FbNXBVcC07gSFMT73Jt8lv0YZouidks7/sINX11cSExwTZOkEQhNAgE+WAcIOeelODz/27xmUC8NmeucEySRAEIeSIQADhYXrq6n0vn3F25igAVuevD5ZJgiAIIUcEAjAYfHcxAWTGdna8lrpMgiCcqohAYB1B+ONiCtOH0cuWzXS06liwzBIEQQgpIhCAwaDzawQBMLrLCACZVS0IwimLCATWEUSD2YLZbGm+s42eCdYRhNRlEgThVEUEAmsMAqC+wfdRREZsOvHhcWwq2Eq92RQs0wRBEEKGCATWEQTgl5tJr9OjOvXB2FDHvUsfCZJlgiAIoUMEAogMtxb4q6v3PVANJ+ZDGBvqAm6TIAhCqBGBAOJjIgAor/bvRn9G2unBMEcQBKFdIAIBJMZZBaKs0j+BSI9JJTPWurLcofIjAbdLEAQhlIhAAImxNoGo8t9V1CMhG4B3dnwUUJsEQRBCjU/F+pRSo7Cu8jYLmAcMBa7TNO17rweeJDgEotLo97FnZY5ixdG1HKsuwGwxo9eJ5gqCcGrg691sJtZ1Gq4EqoFhwGPBMqqtSWjFCKJnQjfH6zJjecBsEgRBCDW+CoRe07QfgYuBzzVNO0IblApvKxLjIoGWCYROp2NKD+uS2T8eWhRIswRBEEKKrwJRrZS6C5gMzFNK3YF1rehTgvjocPS6lgkEQN+kXgAsyV0RSLMEQRBCiq8CcS0QC1yhaVoJkAX8PmhWtTF6vY6k+MgWxSAAVKc+jtfHqwsCZZYgCEJI8VUgCoCvNE1boZT6ve242uCZ1fakJEZTUmHEbPG9HpMz9uJ9b277XyDNEgRBCBm+CsT7wLW2bKbpQDkwJ1hGhYK05GhMDRYqWuhmGpt5JmAdQeRU5EnAWhCEkx5fBaKnpmn3AlcAb2ia9hjQuZljTirSkqxrSxdXtMzN1DOxO93iu1JnrmfG2n9z//LHA2meIAhCm+OrQBiUUqnA5cC3SqkMIDp4ZrU9qUnWP6eorOWes3O7jXfZliqvgiCczPgqEM8Cq4FvNU3bBizhFJoHAVYXE0BxecsFonFtptzKvFbZJAiCEEp8EghN0z4ABgBvKqWGAAM1Tfs4qJa1MWm2EURLXUwABr2BqLBIx/Yrm99qtV2CIAihwieBUEqNAHYD7wBvA4eVUmcG07C2xi4QRa0YQQD8fditjtdV9dWtOpcgCEIo8XU29H+A32qathpAKTUaeAkYFSzD2prEuEgMYfpWxSAAusZnumxbLBZ0Ol2rzikIghAKfBWIOLs4AGiatkopFeXtAKVUGDAbUEADcCOgw5oea8Fa22mqpmlmpdTNwF8AE/C4pmnzlFLRWNNr07HO2r5B07SgzULT63WkJkZR2EqBAHhp0lPcvvA+AIprS0mJTm7SZ03+Bg6UHeK36vJWX08QBCEY+BqkLlZKXWbfUEpdDhQ1c8ylAJqmjQEeAl6w/Zumado4rGJxmS0j6g5gDHABMEMpFQncCmy19X0XmObzX9VCUpOiqKypp8bYuuwj54quD62c4XbFuXd2fMSS3JXUmk6p+YaCIJxC+CoQtwD3K6UKlVKFwL+wPvF7RNO0r2zHAXQHjgHDgcW2tvnAuVjdVMs1TTNqmlYG7AUGYy0v/n2jvkHFHocoKK1p9blGdh7meP31vvke+7Vs3rYgCELw8epiUkot5MQ9rBo4gFVUqoDXsBbv84imaSal1DtY509cCVyiaZr9fBVAIpAAlDkd5q7d3uaV5OQYDIaw5rp5pGdWEgs35FJngbS0+BafB+DOcX/k2s82ALA4ZzlTx/zBbb+UlFhiI2K8nqu1tgQLscs/xC7/ELv8Ixh2NReDeKS1F9A07Qal1L1Y51E4T66LB0qxlu2Ib6bd3uaVkpKWZw2lpcUTbbAGk/cdLqFPRuvf7Ildx7AoZzkAa/Zup2dityZ9jheUExfR4NWugoL2Vzj3ZLZr4/GtfHfgJ/4x/FaiDW0z3/Nkfr9CgdjlH621y5O4eBUITdMWe9vvDaXUdUBXTdNmYB19mIF1SqmJmqYtAqYAC4E1wBO2oHck1vkW24DlwEW2/VOApS21xVcC6WICuKrfZQ6BeG79LG4YeA3RhigGpQ509GmwmANyLcF33tj2HgCbjm/jrMyRIbZGENovwVwf8wtgqFJqCfADcCcwFZiulFoJRACfaZqWj3XFuqXAAuABTdNqgVeB05RSy7DGMqYH0VYAUhOtAhGITCY7tw+52fH6nR0f8dqWOS77zRbPowchuJgRcRYEbwRtVThN06qAq93smuCm72ysKbHObdXAVcGxzj0xUQbiosPJLagK2Dn7d+rLpOyxLDyyzNG2Im+N4/Wz62YxKXss53WfGLBrCr5haWFpd0HoKARzBHFS0iMjnqLy2lanujpzRZ9LXbb/t+szx+uyunK+2vddwK4l+E5L1/4QhI6CCEQjUhOt8/9aW3LDGZ1Oxz+G/V/AzicEBnExCYJ3RCAakWIXiADGIQB6J/XgufGewygltc0maQkBRlxMguAdEYhGpCRYBSKQgWo70YZobhz4O7f7ciuPBvx6gncskkEmCF4RgWhERop10lp+UXAqsY7IGIpB3zQ3wLk8hzvMFrPbkh1CyzHLPHZB8IoIRCO6pMSiA3ILK4N2jUt6nt+kLb/qmNdjXt38Nv9YPI06EYmAIS4mQfBO0NJcT1Yiw8NIS4omJ4Cpro05p9t4MmLTabCYmb31XQA+3zuPKEM0Z3uYuLWjWAOg1FhGekxa0GzrSJjFxSQIXpERhBuy0mKprKmnvCo4T+t6nZ5BqQMZknY6v+lziaP9f7s+5dFVz3p9sjU21Pt0jfK6CqYuuIfVR9e32t5TFYu4mATBKyIQbshKiwUgtyB4biY7E7uOcdk+Vl3AbQvv9bgana8upnd3WFeEfXfnKbUybECZu/+HUJsgCO0aEQg3ZKXGAZBTGDw3k50wfRizJj3dpP2epY+wtXBHk3ZfBaKiLvjiJgjCqY0IhBuyUu0jiOALBOBxSdLGdZsAdhbv9umc4l/3jI4T73eDWWphCYInRCDckJESQ5heF9RMpsbcN/JvTDvzLqb0cF0X6Vil6yqrvxxZ4lP2Tb/k3gG171TCWZAldVgQPCMC4QZDmJ7OnWLIK6xqs1TI7PgsusR25qKergJx+7cP8fb2D1zaqk3NlyNPi051vJbRhCvOc06+3vcdhytyQmiN0BwyygsdIhAeyEqNpcbYQEmFsU2vq9fpOa/bRJe2dcc2uWx7CmA7U1h7YsnwWlMtNaYaXt/6LofL5Waod3IxLctbzdNrZwbkvGXGCqrrA7OWiGDl+4MLuGPRv5qdJyQEBxEID9gzmYI5H8ITv+p9IVlxXTzuz6nMa/YczuXFq+prWJKzks0F23h63UxKjWVejjz18RTzaS33L3+Mu5c+HJRzd1Tm7rcuS7+1cGeILemYiEB4wBGobsM4hB29Ts/9o/7uUSTe3Pa+1+Mbu8UOlB/C5LQw0XPrXm69kScxzZU1CSbHqgsoM5aH7PonKzLrPTSIQHggK82a6tpWmUzuuHfEHS06rnHM4Z0dH7m4VUqMHbtyrEEXugICj656lvuXPx6y65+sSGn20CAC4YH0pGgMYfqQCkSYPox3r/i3z/3tT1kNbpYxnXfgx4DZdbKTEt0p1Ca0O8rrKph/4Jd2m9UV6ESLvaUHOFR+xLGdW3mU3SX7AnqNUwERCA/o9ToyU2PIK6rCbA7d8DbKEMlVfS8D4G9Db3G0P7b6eb4/uMCx/c2+77lt4b3UmGowSdaHVxIi4pu0VdaH7kGgPfDejk+Yd+AHfnD6TrUnAuliyq08yosbXuWZdS852p5c8yL/2fjfgF3jVEEEwgtZqXHUm8wUlIU2M2Vi9hhenvwM/ZL70DOhO2Ct/jp3//eOJ6sfDll/2Ecq8tyOIPzltS1zApbd056orq9xzEaPNkQ52u9d6nkxJ385GdMyC2oKASiqLQ6xJa7YJzXaS7Ovy9/IR9qXLRKMRUeWM3vre3y591tHW+ORSVFN+/r7Q40IhBdO1GRqP0+XU3qe47LtrvRGIARia+GOU3J+wN1LH2ZXyR4AosKiXPYty13leF1UU8LS3JUtuhEty1vttr29BlqX5a6ioMaaFt3eRp/2jDP7e/f2jg9ZmrvSJdXbbDHzyua3+HrffGZvfZf/7bSu+b6zeDdFNcXsLN7N1AX38Omer9lUsNWlGsHtC++j3qkA5kMrn+LepdOpNdVSWVfFnpL9bfFntluk3LcXTpTcqGRYv/ZRYntAp34u2//e8BpJUUkubYH8kVsslqClhbY1jZ/sz8ocyXcHfnJsf6h9wdis0QA8tHIGAD0SupEdn+XXdY5VH3fb3h4nLOaW5/Oh9oVje1PBVj7SvuQadXkIrTqBXqfHbDE3ee9qG4zEYf19lhnL2V60i+1Fuxz7Vx5d63O13jsXP+CyXVlfxV1LHnJsPzT6bjp30BL7MoLwgmME0QZF+3xFr9Nzdb9fO7aPVOY1KepnH1X0SuzR6usFYjTSHlh1dB13LPqXS9s52eP4vzNucmlrLCJmi9mnJ3/nPotzVpBT0XSuSntcwc6dO2xp7kqqfZiM2RaE68MBa0kU5/e4xuR9SeBAlnLfVrjTZZTRkRCB8EJKQhSREWHtysUEMD7rLI/79pbud/iR06JTPPbz1U9e5+GH0R6fhr3xzb7vm7TpdHpOS+nv0lZR7zrv5Z0dH/PgihnNikTjG9L3h5oGe53XwG4v7iZDmHsnwnFbTCLUdLKNjpfkruC2hfc62g87ZSD58l38de+LeG78dE5PGeC3DV/sncedix9g6oJ72Fmwx+/jT2ZEILyg0+nITovjaFE1xvr28ySt0+l4dtx0zu8+qcm+bw/85KgCuzrf82JBC3OWedznTJ25aYzjfzs/5ZpPpnoUj7amqKaYZbmrvN4oMuMymrTZHWfjnAT3geVPsChnuWP7WPVxSoylbCrY5tWGxjd8dwLgbF97GZnV1rsvJdNeZi6nRrlPSf5A+9zxuqEZgbhz6F85r/tEog3R3HrGjR77nZ7Sn/9MfJIr+/7KY5+HF7zQpG1/2UFKak/NuUVBiUEopcKBt4AeQCTwOLADmANYgG3AVE3TzEqpm4G/ACbgcU3T5imlooH3gXSgArhB07SCxtdpC3plJrA3t4xD+RX0y05q/oA2IiY8mst6T+HHQws99pnS4xzmH/zF7b4v937L8PQzSIpM5LUtc+iT1JPzuk9s0s9dEHzF0bUAlBpL28Xyp3P3/8DaYxvR6/Rcln6O2z6x4TFN2sJ0YQBcoy4nLjyW+Qd/BuDT3V836fvGtvd4efIzHm1oPIIwNjS98ZqdRKPebMKgD20IcHfJXv6z8XW3+/a0kzkB7j43O/9c8jC/6XOxw5WaGp1CYU2RS5/eiT3pm9yr2ev8fdit9EnqCcCk7LF8tucbj32nLriHq/v9mh1FGr0Su/ONrRyIt+/HyUqwRhB/AIo0TRsHTAFmAS8A02xtOuAypVQGcAcwBrgAmKGUigRuBbYY4/ARAAAgAElEQVTa+r4LTAuSnc3SKzMBgH257bN+0fPjH/W4r3+nfkw/616P+1/d8jabC7axrWgnX+37DpPZRJmxnPd2fOLo03iU4PwUbDI3YLFYeGnjbK9CFWzWHtsI4DLxqTH2J/qM2M6A9YYQpg9z7B/dZbjP1ztYfpiPta+oN5uanN+Ou3U7nGcDL81Z6fP1goUncQDYV3aQ3MqjPLbqOV7b8nYbWuWKt7hNjamG/+36zPGdHNipHy9PfoaHR99NuN7Apb0u5OZB1zU5Lt1W6fgP/a9i1qSneXnyMw5xsHNh98le7fpk91dsK9rpEAewxipONYL1CPMp8JnTtgkYDiy2bc8HzgcagOWaphkBo1JqLzAYGAs849T3wSDZ2Sx9shIB2JfXPuvnRBmieGrsQxyuyOWVzW+67KtrqCPVSxwit/Ios7e959j+26L7HVkjdmptT8L2GbbOdaCeWPMCp6X0Z1fJHnaV7HHr8rJjbKijxlRDUmSif3+gH9SZPbu87G6I24f8GaCJHd7eJzv2jK7XNs+hor6SjNh0JnQ9m5LaUmY1eu+bXN/cwIrcNY7tRTnLOL+H5/erPfDkmhcByK8+zud75nJ5n4ub1LGqqKtkUc5yLuw+mfCw8IDb4Euspso2ydFuW3pMGv+e+KTH/rcNuZn1xzYxKmOYxwy9i3qe54gjnd1lpGPU7I1Xt7zN42ffT3JUEqXGMmLDYwkP8SixtQTFek3TKgGUUvFYhWIa8JymafZPuwJIBBIA50dzd+32tmZJTo7BYAhrvqMH0tKazrBNTY2jU0IUB46Wk5oaF5KUT3d2uewnnl50YWB2T26bd2KwNbzXAOIiYgkPC/c5C6OxH98SWUdaWjxXf3yr2/7OqYXe7PzzV3dTbqzko6teRq8PzsC1QV/P9uO7GZjW1/E5Ha04zqfb5lFjsWbldElPJi4i1u3xSVEJlNZ6fhCITtQTHxnniMtUWMpIS4vnme//47YcdXJKDAbbKOXz7d/x9f75jn2ZiRnNfq7tiQVHljKqxyCGZQ5yaX976fusy9sChgZuGv7bgF4zLS2eyMjmb1G6aGs8Jy4myqf3NI14+nfr1my/6864gvc2f8656my2Fu+kwth84c5pK57k5Use54EFTzCsy+ncN35qs8cEArPFHJTvU9DkTSmVDXwJvKJp2gdKKWcHXTxQCpTbXntrt7c1S0lJy1Pz0tLiKSiocLuvZ5d41msF7NpXQGpidIuvEWi7GqMjgufHP8Zz62dx3YCrqSkzU0MF1w/4Le/s+AiTk0vEV1Ye2ESvyD4+9XW202KxYGwwEmWbrVxu+3HlHy8NypMmwPq8razP2wpYRwgPj76bvy92Lb9dUlRNjcF9UPMfQ/+Pp9fN9Ljexrr9Ozg9dYDD6VFbU8/enDwOleU6+sSFxzrKduw+coRHVlnXG++f6rrC386CPT5/rm3NA6P+wTPrZrq40AA2Ht5FdngPl7Ydx61ZPfsLjwT077F/72tqrWL88Oh7WJKzgtSYFMqM5RyvLmRTgfWzziu0ZlwZaxsCasPolDM5Y/xgovXRTBt1F7M2zuaID6X2v95qjfttOLqtTT7jN7a+x9ainfx7whMtfoD1JC5BeZRTSnUGfgTu1TTtLVvzRqXURNvrKcBSYA0wTikVpZRKBAZgDWAvBy5q1Ddk9M60uZly26ebyZkoQyTTzryL7gnZjrZh6YP5z8Qnubrfr7nxtN/7db7V+eubzB/wxMqj68ipyMNkNvH53rncteQh8irzXfoEei5A4/PbKTWWcdTNU723Ut8p0Z2YMcazN/PVLW8zdcE9jsD9wpxlPLH6eZc+PRO7M7rLCACHOADsKmwa9P358OJ2l/3SPT6bzLgMXpjwOJmxrplfPxxawK5i1zRP++qG9V7ce63BHvyPCAvnyn6/YmLXMVzWewo3D7qOKT3OsdlgnRMRjDLu0QbrA2FceCy3O9VC84ZzPO7JNS9y1+KHmv2cdxbt5rUtb5NbedTvRb02FmzFZDa5lPQPFMEaQdwPJAMPKqXsv7i/ATOVUhHATuAzTdMalFIzsQqAHnhA07RapdSrwDtKqWVAHeDfXS3A9M6yBarzyjhzYOdQmtIqJnQ9G6DJEqaB4v2d1uB29/hsDlVYA8a7S/e5pJiaPXyJj1cXsuroOi7qea7P2T31DfU8saZp2qGduft/aNLW3E0kTB9G94RsR8D7n8NvIzkqkQeWP+G2f5XJdbShR0eEPqI50wFrJtmXe79tN9kv08+611HIUK/Tc+/IO9hauJN1xzY5ntb3lR2kf6e+TY4Nxs0JTsQgdDR9MrZPott4fIvN5uC6f2PDY7jzrD8R25Do9XvnTG7lUcDqenp63MPEGmLcPuXP2vwGcCK9+NlxjxDjJYPLHfUNdQGPeQQrBvE3rILQmAlu+s4GZjdqqwauCoZtLaF753jC9LqTYgThC4+edR+f75nL5sLtQTm/XRwADpfnuKxg12B27975z8b/UmosIzkqiXG2cheNKaopIdoQ6fjhfLnvO692uMsk8uUpc1LXsczZ8SHX9r+SnonN+6qdqTPXU2Is8euYlrKv9CClxlKGdx7S6nPFGmKaBOoNegND0weRGt3JIRDfHfgJPfomNcFa4r70htls5p4ljzgE2N1NtdBWWM++wmJbLAR1drcRFBRU8Oy46X6vHmgvCPnc+EddCkW6o8ZU67dA1Jnr8e+I5pGJcj4QER5Gt85xHD5WQb2pfUxwag0p0Z248fRr2+Raq/PXuzx9u5vUZLaYHSJS42VN54dWzuAep6qrWslev+1x9yTamJEZQ3lizAOc1WWko+2Kvpf6dP4yYzn9kr3HbBq7blrKCxte4a3tH1BZ17KZ/s7vxdDOgz32a2zvvAM/8MDyJ5i64B5HW6AForq+xmV05u5za1yiXa9reYKKv8SER/PihCe4qMe5PD3OP6H4385PXbbdT/D0fzRU3xDYzwBEIHymd2YiDWYLh/LbfgnSYBCuN7i4Nq4bcHWbFCRz52L61qlgXkV9JWXGCodgFNYUcc+SRxyVZS1YHG6Hijr/AoDjs872OYiXFJno0ndy9jhmTToRU7hl0A1ujyuvq2BS17HEGjw/y/VJan7ilj+0dC2LmHCrf/3Kvr/iatuaI+5wni9ip/G65o0D2u6wWCw+l2hpPP8mzM3o4Jxu45vtE0wiwsK5uNf5xIXH8tCZ//T5uINO83UazA3sLzvkppdrrG5LwXZmbXrD6/ucEp3ssw2+cnIn6bYhvbISYL01DtGna/By+duaa/tfSZQhimHpgxndZQT1DfXkVeXzw8EFTOl5Hk+tdb+i3Z+H/4431n/o9/Xc3SDW5W90vF5wZCkLjlhzEp4a+xDPrH2JKlO1y9oUi3NXMLHrGJIjkzxmHDUmEH5+nU7HE2MeIL/qOP079aVfch92NxrFVNZXWfuNncadi+4nXB/eJIB7VpcRLMld0SpbnOcHlBrLyIhN9/scZouZ7olZTMoe22zf24b8mVmb3vC4v0ts87G5lze/yc7i3ZyTPZ6Le51PZJjnWE3jOS32YLEzjSe3hXKt8c6x6bw8+RlyK4865o94wl5mpcxY7nH5WeeZ+T8dWsRXNnfqruLdDEod6Nj340FrQDw6PCoof7+MIHykjyOTqX3OqG4pZ2eOYlj6CfdCeFg43ROyuWXwDWTHZ3qccDS515gW3XTduZg8Vd68b9mjTYLAYJ2FvO7YJno4xQfcuSC8FTVsKUmRiY4grfMKf/aaQfYgYbjewH8mPskLEx7jJqfMscfPvp9uCV3dnvt4dQFPr53JETeVYBtT6LSwTUuTDhosZrejA3cM6NSPh0bf7XF/p6jmn17tMaFfjixh8ZHlXvvWmayZYqnRKTw55kGfRn6hFAg7WXFduGfE7V77NJgb2HB8i9e1yZ0fpL7yEmuzz63Rt8Al5QsygvCRlMQoEmMj2JdXfkqtkdAc4XoD16jLsVgsfLz7K0e7fQKYQRfmVwbLezs/YVDKAM7vMYm5+77HZGnwO/E1v/o4b2//gPiIuBN2hoXTPb4re0r3O2yKj4jjufHTfYo7tJazMkexr+wAF/U419Fmz8Yamj6YO+IjyTJ0c0zSy4zNIK/KNUX3m33fc7gih3d2fMi0M+/yer0wp8mGLXUxmS1mv9wy3lyQy/NWU1BTxN+G3oLFYqHKVE1ORR49ErIdc2GcaVw1tzF2V8rg1IEkRnqeADYu6yyW5lrLlrQHgQDonpDNzIkzmL3tPQ6UHWry+VSZql0qErijwWLmSEUeUWGRbvcfrsghOfJEbbhg/e0iED6i0+no0zWR9VoBBaU1pCcHOl+g/WKvdpoRm96kfs9fB9/oSNHzhf1lB9lfdhCdTue2JLY/VNSduMl0jklj6pA/Y4gzc6yglF8OL2Fy9ji3N6dAkhyZRImxlNSoZC7s4b5+j16nZ2z3US6TpkZmDOXrffNd+tkFpSVrDxgb6ogMi8BsMbPu2CYGp55GlMH9zcWO2WJG7+MIws69I+/wuBTt7pK97CzezXs7PqGszprx1zepF3cO+2uTvs1Vs7XPNYnQe59U6fxeHSg75EjlDjVh+jD+OviPANy/7DHK/IyX/XRokdtqzHUN9aw+up53d35MjJPbLVgC0T4k9yRB2aq57sk5tdxMvtInqRdd4zK5tNcFjrb+nfrSM8G/VFDwPmxuCRO7jiFcbyA9NoWM2HSuHXBl0MUB4M5hf+VXvS5kaLrnLCB3uPtB2/P6fZl01jiWs822aNTKvLW8s+Mj3tzu/QnVHjD2N7DbLb4rMyfO8Lh/1qY3HOIAsKfU/ZKdzq7GMmMFtaZaXt/yDjuKNOBERo6hGYG4ou8ljtf2SXvtjSfHPshz4x+lV2J3n4/xVKr/Q+0L3t35MeD698oIoh3Qo4t1wtz+o+WMGdQlxNa0PXqdnn+NutOlTafT8c8Rt3GkIo9oQxQr89ZQUV/J8rw1Hs4SeJ4f/2ibiIE7UqM7cYGHkYM37BPSAN7Z8RFr8jcwPP0MAOoaZapYLBa+O/ATA1KU4ybTOJbz1vYPKK+rdNw07DdaT9gFJqwFdbHC9GE8MeYBnl03C5Xch72lBxyLVPmK2bZg1eGKHJ5eO5NwvYF6s4nNhdt5efIzjhFEuIcFjezEhMfQKSqZ4toSDG2Y5uov0YYo7ho+lbzKfJ8n2bmjxoMIBsvlLSMIP+iREY8hTM/CDbmYGk6uFdWCTXZ8JqnRnbi094X8vv+VnN99Ev2Tm864DQahEofWMMI2uS3GEM2a/A0ArD++GWg6gsiryue7gz/z/PqXHSsBussG+2zPNyQ4xWUAluWucrvYkV1gWjp3ICkykSfGPMD1A3/L3SNucxG8xhyvLmiyvoQ9xrDIFqxunL5Za7JWEQ5vZgQBOJIsnMvLtFcy4zJ40E1K7NldRvLEmAe4fcjNLTqvjCDaAYYwPcnxERSU1nIov4LeWadOumuguaz3FMCagllSW8qa/A0syQ39GgjtBfsP2p1bxGQ2salgG0PSTgdc11++Y9G/OK/bREZmDHV7XpPTUrJztn/oWCujccaZfT5KIOYOxEfEMWPsgy4T55yZvurZJm3rjm0iNjyGIxW5TfY5n8eXlfcu6XUBKrkPqpkJiu2FjNh0nhv/KMvzVvPl3m/58+nXMTTdWiU3KTKRWwZdz+tb3/XrnMEqMyIjCD/5zXhrVU7tSPsqstZeSYpMpGdid36rLuflyc8wa9LTnNdtIhH6cHq0IHbRUZi99V2Hm2VJjuuciZ8OL+KorUhh43UtnFdCs4sDWBdTqnUSmhMupsC5Zf50+h987mvBwqKc5U0yuRrjbWa9nXC9gYEpKqB/S7CJNkRxbrcJzJr0tEMc7JyRdjovTXqKvw+7law431zZwVrjXEYQftK/uzXfe+ehEi4a7XvQSbCi0+n4dZ+L+HUfa7He2Vvfc9T5Aeid2IN9ZQd9Otfv1G+Ia+RSOZX4+2LPCym+vcM6SbHUWMaz4x7h7qWPeD3XM+teAuDJMdNIjExwuJjCAui3H5Y+mKGTnsaCBb1O73UimK+0ZfmMUOApdqDX6emT1JO/Df0LT6+d2WyMp6DavxiQr4hA+ElibARZqbHsOVKKqcGMIUwGYa3h5kHXsS5/I2/v+JDOMen8Y/j/uew3W8yYzA1EhIVTUF3EOzs+4kD5Ic7JHs9YD0X9ThaiDVEu7qOW4k9Rt/uXP86lvS5gVMYw4MR8lkCh0+kc804SIxP4x7D/Y2vhDn46vAiAWwffyOmpAzy6oxpzZpdhAbXvZCM2PIZHz76P6voawsPCCdcb2FywrYkLanijhZwChS5YQ5NQUFBQ0eI/xp+FeT74aTc/r8/h3t8PRXULfP2TltrVlgTSLrPFzO6SffRK7E6El/ILbW1XIHFnV73ZxJvb3nOUeLYza9LT3LbQ81rizlzU41wu7nU+NaZanl33EseqC/yya1z3UVzT+0q/jmkJJrMJHTqHG6iirpKPtC9cAugvTnicnMo8DpXnkFObw7V9rm43k9/stJfv1/HqAmpNRrLjszA21JHVuRNFRS2bMAmQlhbvdigjAmHDnw9+055CZn6+hd5ZCTxw3YiWXjLgdrUlYpd/eLKr1lTLopzlTMoeR2FNEbUmI72TenCo/Agf7PrcUcraEzee9ntHRpQzB8oO89z6WT7ZFqr1KCwWC5X1VViwEK4PdymBfbJ9jqGmtXZ5EghxMbWA03paRw37c8upMZqI9mHdXEFwR5QhigttK6M5ByS7J2Tzr1F3UlRTzEMrn3I55tr+V/K/XZ8BeJyk2DOxG3cMuYWyunJGdh5KvdnEwiNL+Wb/90H6S/xHp9O5lEsR2h9yZ2sB4YYwLhiVzQ9rjrA3t4xBvVKaP0gQWkBKdCdmTpzBkcpcOsekU2uqJTkqySEQKdGdPB6rOp1I+4wIC+eCHpMZmzWae5wC2s9dMA2Cs1qocArQvhx8JxFD+qQCsHbn8RBbIpzqhOnD6JHQjWhDFMlR1nIvl/a6gPO6TfT7XLHhMY501D/0v4puSVmBNFU4xZARRAvpm51EYmwEG/cUYDb3R6/vGNVdhfaB3S3VEuzpqB2lIrHQcmQE0UL0Oh1n9EmlqtbEmp3HQm2OIPiFiIPgCyIQrWDKmdYA4S8bckJsiSAIQuARgWgFnTvF0DszgQN5FVTXSqRPEIRTCxGIVjKkbypmi4Xb/r001KYIgiAEFBGIVnLO8BPrC1fWyChCEAKNlNYPHSIQrSQqwsCwfta1en9edyTE1gjCqcXG3QXc8uwi1mv+lRARAkNQ01yVUmcCT2uaNlEp1QeYA1iAbcBUTdPMSqmbgb8AJuBxTdPmKaWigfeBdKACuEHT2u835KzTMtiwu4Bvlh/kkrN7oNfrmD13ByNUOsOV+4XejxyvZPWOY/xmfC9JkRUED/xke+j6ae1hj78lIXgEbQShlLoHeAOwF1h5AZimado4QAdcppTKAO4AxgAXADOUUpHArcBWW993Ac91j9sBQ/umOl7Pmb+LHNvN/+Uvt3o85uG31vDdqkNs3lvYFiYKwkmJPR331KkYd3IRzBHEPuA3wHu27eHAYtvr+cD5QAOwXNM0I2BUSu0FBgNjgWec+j7oywWTk2MwGFpevjgtzfOyic0x/eazeHj2SlZsy+dS26JCvpzTEBnebJ/W2BVMxC7/ELv8Iy0tnvBw6+/ZYAhrN3a2FzsaEwy7giYQmqZ9rpTq4dSk0zTN/iBQASQCCUCZUx937fa2ZikpqW6xva2thpidEu14/a9XljteN3fO8vIar33sdh0+VsGSzXlcc07fdrEGxala1TJYiF3+YbfLZLIuOVpXb2oXdrb396s1x7ujLe80zqkI8UApUG577a3d3tbuue9a/xc3Wb3zGG9+u6PZTI3H3lnHgg25rN4hs7aFjoNE50JLWwrERqXURNvrKcBSYA0wTikVpZRKBAZgDWAvBy5q1Lfd0y87idN7eq6u6Y5t+4tZvjWfn9d5n43dYLYOvmrrml/EXfCO2Wzh8LEKzKfQWiinLPaSIPJRhYS2FIi7gOlKqZVABPCZpmn5wEysArAAeEDTtFrgVeA0pdQy4BZgehva2SruuHKwy/bxkmoazM3ncX+ycK9P5/dngae5yw/w41pJvW3MvBUHeeTttSze5H0xHiH02EcQZhGIkBDUNFdN0w4Co22vdwMT3PSZDcxu1FYNXBVM24KFIUzPC7eN4a5Zy7EA9/13Faf36sQ/rm666ldjSiqMJMdHeu3jz+/ky6UHADh/ZLYfR536bLRljm3eW8ikoVLuul3j8DGJQoSC0Ec7T0GS4iK57w8n4hHb9hez/UAxS7fkUV5d5/G4OlPz7iPxirSecIP1a79lX1GILRGaQ2dTCPnehwYRiCDRt2sSd/32xKjh+Y838fZ3u3hsztrWnVh+KU0oqTByKN/3DI4Iw4mv/bFWZL4JwUdCEKFFBCKInNazE0/9ZbRLW1G5kZueWuC2v6mh+Z9Ba34oDWYzHy/Yw+Fj7S9NrzXc9fJyps9Zi9lHR3W4U5rwcx9uZP7qQwGxY9WOfL5b5f1c368+zIc/7wnI9ToSzg8AdfW+JWrUGE2SiNBKRCCCTHpyDLf8aqBPfV/5cis3PbXA69Owr99355ulPYX2q6UH+GHNEaa/3cpRTDvF1wyvcKcRRFG5kU8X7gvI9V//ZgefLfJ+rk8W7nWUj2gv/LI+h427C9i0p5D3ftT8SoQAKCqrDVpBvfjocMfrOfN3cv/rq/jr84s5Xlrj9ThjfQNTX1zCcx9uDIpdHQURiDZg9MAM3rh3Ev+8xnug+miR1d0xe94Oj32WbvEt8yavqMrxut5kpqTCyLcrrU+3p+ozVUWN5/iOM+GtmG3vC/7eYEPN/37azUtfbGXm51tYuCGXgmZuvs4Ul9dy96sreP6jTUGxLTXpxATUJZuPkl9s/Y3sPFjs9bgKW6xv1+GTYgpVu0UEoo3Q63QM7NGJF28b42ibcmY3Rp/WmdGndXbpm1dYxb8/3ewyPE6IjQCsIpJbUNns9TbtOVHjacW2fEorjX7Zu+1AEZ8u2hvQm12wh/tLNx/1qV9EeHC/9g1uXF01RhPzndxPFouFTXsKOR7iGIi7z9ed/Z6wP4hoR4JzI/b0/fts0T7KqupcSux/u/Ig67XjUh48gAQ1zVVoSmJcJG/dN9mlzWKxsGq76wzpLfuKuHPmMl6519o3MTaC8irrU1FuYRVZaXFer/PFkv2O1//7aTcXjurml50vfLwZgAlDskh3eorzF1ODGYsFNu4p4LWvt/PAdcPpneVT5RS/MdY1cPtzCxnWN5UdB4u59nxF5+Ro9ueV07droqPwW7ibUiXrdh1nRP/0Fl33WHG1y42q3mRuUg7l8wV7+NTJ/VRaWcfMz7cANPk+tCW+xm080eBD3MyZypp6lm7JY9LQLKIiPN9+GswW7n99lWPE0JiqWhN/f2kZYH3/jPUNfL7Y+p2PjTIw7YYRjr75xdVkdIrBYrFw5HglXdPj0Mua3D4hAtEO0Ol0vHXfZF75civrnKqaV9bUc/0jP/Dc/51NZMQJt8hrX29nZP90vxae/37N4RbZVufGr19aaaS4up7YcD3hYXqv5cpv/89SGhrMxMdYR0CLN+UFTCBMDWYWbMh1bC/fdpTaugYOHi0H4L9fb2dQr07MX32YKyb04uKzejB/1SF+Xt901vorX23juvP7cdbpGURFGKiqrafGaKK0so7CshpGD8zwaMe/Xl/VxK7GFJfXumxXG01+/a3Bot6NrQ/MXs2060fQKzOh2eN3HCzx+VqmBjOvfrWNnYdKKC4zcu35/Tz2ramt9ygOjbFYLMz8bItju6rWxH+/3u7Yvr/R5zNxSCZ9s5PI6BRDzy4JHCup5rE56+iVmcDtVwx2xKgOH6tg+8FiLhzVza/f2qmECEQ74uZLT+M3E2rJK6xi1hcnSoX/85UVTfqWVBjplBDl2DZbLJSUG0lJjGrS1xesGU57GXN6F7pnnCiPZXQzN+PuV1Y43BDdO8fz8I0jPZ7XWGcvtmb9P5Dunbe+3ckqp9pUjYPUOQWVDhfF54v3O54wPfHej7t578fdPHjDCJ79cKPL+Qb1SiE26kTAtKC0hpSEKLfieCi/gtN7pbi0RUe5/tRyjjfvJnTH/FWHSEuKZkT/dA4fqyC/uJpRAzo3f6AHamrdC9Xj767j/j8Mp0/XRCwWC3UmM5HhTWM3/gTcX/xkMzsPWQXlSDNuUn/cXF8s2e84r52DXhI9Fm3KY5GbWfTbDhTzl+cW8fifzyQhNoJHbMkcneKjOHNgZ8qr64itredYcTVHi6oZYivzb/+OnYoiIgLRjgg36MnoFENGpxhev3sitzy7yGPff76ygoTYCB69aRQ6HfxtpnW4fffvhjKge3Kz1/phzWHOG5mNDtiXW05JpZGf1+Xw87ocZv5tnKNfTaMn3araepcf76FjFRwtqsJigczUWI/XqzNZn1Qj3NxkWoovhQtzC6s87hvSJ5VNbtbjmLv8YBOxWbwpj4tGdwdg16ESnvlwIxee2Y2rJ/VpcvyBo+UkxEYQptc5XIHfLHEVp/9+s73JcRXVdfzzlRVcMaG3x9nvdjfVW/dNdtzAenZJIK2FbsD35u/0uO/J99e7bA/vl8afLx3oIhSDe6ewZV8R8TEnxPNQfgW5hZWcfXoXl+Odb+LGZjLO/HF92ZMvAsW0N1a7bP/3m+1uP6+7fzeU/KIq3vtxNyNUGlGRBq49t5/LaN9XFm3MJbegivNGZbNeO84FI7t5HZlv2VdIbHQ4vTOD4661IwLRTjGE6Xnrvsks3HzU44+4vKqOO19axhUTejnadhwsRmUnNXv+jxfs5fPF+wgL0zf5sd7xnxO1EffllvPF4v2kJkVz9aTeLFif2/hUPDDb+oN6455JNP22d2EAABElSURBVJjN1NQ1EBUe5nLzrbcLhMH9CGLnoRKe/XAj/7xmCAN7WAseGusbKK0w0rlTjNtjzhzY2WUE4S8XndUdi8XCsZIaF3eGO9H4bNE+aowm0pOjHWmx368+TJItecCZL5cecJQ58Se+sCenjHqTmY9+2eNWIJxvms4B/+OlNXzw027OHZnNaT18LxY5d/kBfvLD9bh+dwHbXlrGi7eN4ZvlB0mOi3Q8LDgL1HTbZNCE2AhO75ni9ly1TnMZDhwtJz052mWE1ngE0blTDMecPqNH/zSKD3/e02Tk0JY865RCa3cNF5fXMuXM7pRWGtHrdZx12gnXpNliwWQyu31IevcHDYBfNljdnykJUV5Hhv/+tG3iVyIQ7Zyrz+1HhB7e/HanoxCgs78VcHGdbN1X5JLB5A1TgwVTg/cnua+XWW90B/MrWLfruNe+f35moeP1gO7Jbn+8ntay+GKx9aY7b8VBh0DMeG89h49Xcs/vhpKaFMWPa4+QnRbHuDMygRMpkGMHdSEyIoxf3MQWvNGrSwJ/u+oMwOqyu+vl5V77u3tS/WiB9yKLny7aS1qib0/3UV6ePIvKatGOnHg/a51Gdsu2HGXzviI27yvilksHMtrpprRi21GKymq5dEzPJue0i5gzt/1mkIt7szHGugYee2edIyU7wybehaU1FJXV4uxleeHjzdx4UX/GDc50cx6r/YePVfDYO+volh7HIzeNcux3juMYwvQ8cfOZHMgrZ9fhEupNZrqmxXHWaRku37Fxg7sQGx3O96utonf7bwYxpG+qwz1WYzQRFRHGn54+8T21o9PBZWN78pWb98QfdhwscYnLLFifw768cqZdP4LPFu1l1+FSrr9QMXGI9xpgFdUnkh5yC6uoq2+gZxdrTKisyjWde9OeQpZ9tY2/XDLQZY5PIBCBOAkYM6gLYwadGK6/ce8kZs/d4dbFcriRb9vuAmhrPD3ZfbFkP0P7pvL54v38/ty+VNTUc+R4pSNoGxamp7TSSF5hleNveabRZKdunePpnhGPyTYqmTQsi55dEqgxmlixLZ+rJvYmITaC6EgD//1mO/27JbN1v/U9+OOU/ow/o+kNKzk+kqmXn868FYc4FMCZ5vNXNf+EXlffQJ3JzEYvwn73q65xKOf8fufvwetzdzgE4lhxNW/Ms44+e2Ym0DUtjqQ498Ug/3TxAIb2TSUmKpw3753E6x6+X3Bivg7gGHmVV9c3sRHg7e92MW5wJjsazVsw1jewZV+h40m48ffWPloa2COZO64YjF6no3dWokuCw9mDMnjruxOj64LSGm68aADrteMUlNaSEBeBTqdzuMSiI623u+vO78d7P+7m0rN7sHhzHndcMdgRkO/fLZnZc7dTVH4iLfyt+yZ7rH7QHPvyrAkTj7+7ztH24c97GH9GJpU19bw5b6fju+lMg9nCY++s44At4QLg5b+PZ9ehEl5yEvBjxdV8vfwAh/IrKJ7Qi87J7kfbLUV3sk3q8UZBQUWL/5iTcaWouvoG3vpuJ6UVRv540YAm2RqXj+vJxWf1cDzZ33zJQIb1S+PWFxa7O12bo9MFprTUY38+kyxb/MPT+2X/gd/12yGc5mXNjgazma37i9lxoJizB2Xw6Jx1Hvu2lB4Z8Vx3geKxdzyf+817J1nTZQ16zGZLk3hUdKShSXzIzrO3nk1tnYkH31zTZN+rd02g3mTmX/9dSZUtQD2gezJ3/25ok777csvYk1PGqAHpLN+Wz5dLvAf5PZEUF0FkeBjHSrxPwMvoFNMkc2nikEyuv7C/x2NmvL+ePTnWxSfTk6N56i9nUVZVx77cMob1S3N7jMViodiHhA7tcAkR4WH07JLAxwv28MOawMyAH9o3lfiYCJZsDmy5+eenjmm2GrQn0tLi3QY8RCBsnIwC0Zh3vt/lssbBP357Bqf3TKGypp66+gZH1tNPa48wd8VBl9x9T0RFhLktYfHuIxdw/SM/+PhXBJdX/zHBERj09H699e1Olm09you3jSHRw1O0J+pNDRjC9FgsVtfHzkMldOscz8KNOcREhjOgezI6HY6g8dC+qV5HA1NGd+OqiX2459UVFJbVuu2TkhDp8hQbSK49rx//+2k3YI0T/Pv2sT4dtyenlOc+2uSIJ7UFyfGRPD91jMf9xroGxwPPgzeMcLhhgsHB/HI+WbiPWy4dSGJshEvWUmVNPR/+vJuVtvlMEeF66urbdsLerDvHEeMUx/EHEYhmOBUEAqxZR6t3HCMzNZZ+XoLV/9/evQdHVV8BHP9mk5DwCIFKAyOCcQSPMaAIAoqPhmpVRAfb+mA0vhjx1Xc7GrQ61o5OrdPicxSpOkiVaqtStVOE+hZUMr4KCh6oiCDyCK88yIskt3/87sZF7ia7kL27mPOZYYZ7d5M9+9vf3pP7u7/fuZ7n0by7jepdTRT06sGn63bQr08eVTsbmOXPIZ8wYhDTJpe0T+ObftfrAJxfdjiXnjOiPS7P86iY9U7cg12qxV6oi9de0fe7LzNMEtHmeTyxUBl5+EEcO/y7bN3ZwA2z3gHcbJchBxeS63ksXbmZE0cOIjsS4YUln+/3mHdXSPZC52P/Xsm26sZQLhAPP6SQG8vHdPict5Z9RVG/nsjQzmfv7a9Ev4+e5wVe60il2deX7fP96i1BdOLbkiC6QvWuZhZVrmPKSYftMeNiW3UjubkR+vbqsVdcnuexq7GFxcs2MnH0YPJys5kx6x227Gygb8wq8HgO6pvPeWWHB04n7EwiCSId2to8srLc/PiguFpa2zqcypyIRyom0tbm8dQrq/dYNJiMfZ0Js6GqjnufWUb56UdQcmh/Hn5hBR+squKO6eNZumIzLyxZu9fP9C/IY8bFo6nwk2dnKi46NpQDf6KS6V87apvIzYnQp2cutfXN3DXvQ0YNH9A+2eHC7w/jjHFDWb5mG3f//b+cPaGYuvrmwDUaiXi0YuI+r8WwBNGJTDqwxDqQ46ra2cDCynVcMHEYWVlZbNhaxx1z36dfnzwmjh7M6WOH0Ly7leVrtjOuxK0M/8976/nby6sZV1LEqOEDeH7xWq6cXMIHq6so7J1Hr7wceuXnsGHrLhoaWzi+dCBDB369sO9AbC/P81i7qZaGphZ65+eyeUd9+1lc1Ngji1i/pY7S4u+0T4U8oXQQ08/Zs1JwS2sbv5/zHl/6C9FuumQM9Y27mffyarYEXAOYfOJh/PjkvWc47S/P81hYuZ6q6gaq65oZNWwAY0uKiGRlkZsTwfM8aup3t5fLuO7cESxevpFrppRy3cw3GTKwD7ddMa6TVwlfV/SvltY21m+p63A4bEdtE/369ODtjzexYOk6pp46rL38DbiFm9POOpLZL65g5Rc7uGzyUXxvZPzV/p2xBNGJA/HAkk4WV3KSjWv1lzvJyY5Q39RCTV0zJ4xI/Mvf5nksrFzH+JKBe6y2B3fgefylTzl/4jAGD+id9vZa81UN/Qvy9ri4WlPfzOBBhdTWJF5VNizpbK+1m2oo6tcz8DrD/sYVL0HYNFdjMtDwQzpf7BhPJCuLSeMPDXysf0Eev/TXfmSCoHpPfXv1ID8vh8xL8+lVPCh1F+DjsXLfxhhjAlmCMMYYE8gShDHGmECWIIwxxgSyBGGMMSZQxs5iEpEI8CBwDNAEXKmqHZfONMYY02Uy+QziXCBfVU8AZgB/TnM8xhjTrWTsQjkRmQlUqupT/vYGVe2wiHpLS6uXk5OaWjvGGPMtdsAtlOsLVMdst4pIjqrGvdt7Tk72t++msMYYkyaZPMRUAxTEbEc6Sg7GGGO6ViYniCXAWQAicjwQ/z6IxhhjulwmDzHNB34gIm/jxseuSHM8xhjTrWTsRWpjjDHplclDTMYYY9LIEoQxxphAliCMMcYEyuSL1CmXCeU8RCQXeAwoBvKA24EvgReB1f7THlLVp0VkOnA10ALcrqr/SnFsH/L1WpTPgTuAOYAHfAz8RFXbwoxLRC4HLvc384FRwATS2F4iMh74o6qWicgwEmwjEekJPAEUAbXAZapalaK4RgH3A624vn6pqm4WkfuAE/3XB5gCNIcY12gS/OxCbq+ngOht/IqBd1V1apjtFefYsIIQ+1d3P4PIhHIe5cA2VT0ZmAQ8AIwGZqpqmf/vaREZBPwc1znPAP4gInlxf+t+EpF8gJgYrgBmAjf7sWYBU8KOS1XnRGMC3vdfO23tJSI3AI/gkhUk10bXAsv9584Fbk5hXPcCP/Pb7Tmgwt8/Gjgjpu2qQ44rmc8utLhUdarfVj8EdgK/iok3rPYKOjaE2r+6e4I4CXgJQFXfBY5LQwz/AG6J2W4BxgCTReRNEXlURAqAccASVW3yO+X/gKNTGNcxQC8RWSQir/prUcYAb/iPLwBOS0NcAIjIcUCpqs4mve31GfCjmO1k2qi9/8U8N1VxTVXVj/z/5wCN/hn0cGC2iCwRkWn+42HGlcxnF2ZcUbcB96vqxjS0V7xjQ2j9q7sniMByHmEGoKp1qlrrfzGewWX5SuB6VT0FWAPcGhBrLVCYwtDqgT/h/iK5BngSyFLV6Lzo6OuHHVfUTbgvL6SxvVT1WWB3zK5k2ih2f5fG9824VHUjgIhMAH4K3A30xg07lQNnAteJyNFhxkVyn12YcSEiRcCpuCEdCLm94hwbQu1f3T1BZEQ5DxEZArwG/FVV5wHzVfV9/+H5wLHsHWsB7tQ3VVYBT6iqp6qrgG3AwIDXDzsuRKQfcKSqvubvyoT2imoLeM14scTuD6PdLgRmAZP9seh64F5VrVfVWuBV3JljmHEl89mF2l7AecA8VW31t0Nvr4BjQ6j9q7sniLSX8xCRgcAioEJVH/N3LxSRcf7/T8WNtVcCJ4tIvogUAiW4i1SpMg3/moyIHIz7a2SRiJT5j08C3kpDXACnAC/HbGdCe0V9mEQbtfe/mOemhIiU484cylR1jb/7CGCxiGT7F0RPAj4IMy6S++zCjAvckMyCmO1Q2yvOsSHU/tWtZzGRGeU8bgL6A7eISHS88dfAPSLSDGwCrlLVGn8GxVu4xP5bVW1MYVyPAnNEZDFuxsQ0YCvwFxHpAawEnlHV1pDjAhDccETUtcADaW6vqN+QYBuJyEPA434bNwMXpSIgEckG7gPWAc+JCMAbqnqriDwJvIsbXpmrqp+IyOdhxOVL+LMLq71i7NHPVHVlyO0VdGz4BXBfWP3LSm0YY4wJ1N2HmIwxxsRhCcIYY0wgSxDGGGMCWYIwxhgTyBKEMcaYQJYgjMkAInK5iMxJdxzGxLIEYYwxJpCtgzAmCSIyA7gAyAYWAg8BzwOfAqXAF0C5qm4XkbNxJZojuAVXV/sltk/DrVKP+M+/CFco7kpcQbahwCuqOj3M92bMN9kZhDEJEpEzcdU0x+JqBg0GLgZGAg+qailudevv/EJvDwPnqurRuLIHD/hlmJ/E1eYfiSvvcpn/EkNxiaIEmCQipaG9OWMCdPdSG8Yk4zRgPK5eEEBP3B9Zq1T1dX/f48A8XA2dSlVd6++fDdyISyYboqW3VfVGaL8R0puqut3f/gwYkNq3Y0zHLEEYk7hs4B5VnQntVWUPAZ6OeU4EN0z0zbPzLNz3bTeuthX+7yjk64qbsZWEPf9njEkbG2IyJnGvApeISB//viH/xN1kSsTd0hNcwccFwFLgeBEp9vdfhSvbrECRiBzl778Bd78NYzKOJQhjEqSqLwLP4g7+HwMf4e7utR24TUQ+wd3/93ZV3YxLCvP9/WXANX5F2XJgrogsA44C7gz7vRiTCJvFZMx+8M8QXlfV4jSHYkyXszMIY4wxgewMwhhjTCA7gzDGGBPIEoQxxphAliCMMcYEsgRhjDEmkCUIY4wxgf4PyCuErDwyJOkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#stacked LSTMs\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(t_period))\n",
    "model.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adam(lr=0.003))\n",
    "# fit model\n",
    "es = keras.callbacks.EarlyStopping(monitor='mean_squared_error', mode='min', verbose=1, patience=5)\n",
    "\n",
    "history = model.fit(train_X, train_y, epochs=2000, batch_size=100, validation_data=(test_X, test_y), callbacks=[es], verbose=1, shuffle=False)\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 2.827\n",
      "Test Standardized RMSE: 42.546\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.median(np.abs((y_true - y_pred) / np.mean(y_true))) * 100\n",
    "\n",
    "def standardized_rmse(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    " \n",
    "yhat = model.predict(test_X, verbose=0)\n",
    "mape = mean_absolute_percentage_error(test_y, yhat)\n",
    "rmse = standardized_rmse(test_y, yhat)\n",
    "print('MAPE: %.3f' % mape)\n",
    "print('Test Standardized RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_period=7\n",
    "multi_time_data_dict={}\n",
    "\n",
    "# for every type (perspective x product), shift data to prepare features and labels\n",
    "for key in data_dict.keys():\n",
    "    temp=shift_series(data_dict[key],t_period,1)\n",
    "    label_index=['unblendedcost(t-'+str(t_period-i-1)+')' for i in range(t_period-1)]+['unblendedcost(t)']\n",
    "    label_data=temp.loc[:,label_index]\n",
    "    label_data.columns=['y(t-'+str(t_period-i-1)+')' for i in range(t_period-1)]+['y(t)']\n",
    "    temp.drop(temp.iloc[:,-ori_column_num:], axis=1, inplace=True)\n",
    "    temp=pd.concat([temp,label_data],axis=1)\n",
    "    #print(reframed_data.columns)\n",
    "    multi_time_data_dict[key]=temp\n",
    "    #print(key,len(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73, 7, 236) (73, 7) (48, 7, 236) (48, 7)\n",
      "(147, 7, 236) (147, 7) (96, 7, 236) (96, 7)\n",
      "(217, 7, 236) (217, 7) (141, 7, 236) (141, 7)\n",
      "(291, 7, 236) (291, 7) (189, 7, 236) (189, 7)\n",
      "(365, 7, 236) (365, 7) (237, 7, 236) (237, 7)\n",
      "(439, 7, 236) (439, 7) (285, 7, 236) (285, 7)\n",
      "(502, 7, 236) (502, 7) (326, 7, 236) (326, 7)\n",
      "(576, 7, 236) (576, 7) (374, 7, 236) (374, 7)\n",
      "(644, 7, 236) (644, 7) (419, 7, 236) (419, 7)\n",
      "(684, 7, 236) (684, 7) (444, 7, 236) (444, 7)\n",
      "(758, 7, 236) (758, 7) (492, 7, 236) (492, 7)\n",
      "(831, 7, 236) (831, 7) (539, 7, 236) (539, 7)\n",
      "(905, 7, 236) (905, 7) (587, 7, 236) (587, 7)\n",
      "(978, 7, 236) (978, 7) (634, 7, 236) (634, 7)\n",
      "(980, 7, 236) (980, 7) (634, 7, 236) (634, 7)\n",
      "(1043, 7, 236) (1043, 7) (675, 7, 236) (675, 7)\n",
      "(1117, 7, 236) (1117, 7) (723, 7, 236) (723, 7)\n",
      "(1191, 7, 236) (1191, 7) (771, 7, 236) (771, 7)\n",
      "(1265, 7, 236) (1265, 7) (819, 7, 236) (819, 7)\n",
      "(1339, 7, 236) (1339, 7) (867, 7, 236) (867, 7)\n",
      "(1340, 7, 236) (1340, 7) (867, 7, 236) (867, 7)\n",
      "(1414, 7, 236) (1414, 7) (915, 7, 236) (915, 7)\n",
      "(1488, 7, 236) (1488, 7) (963, 7, 236) (963, 7)\n",
      "(1562, 7, 236) (1562, 7) (1011, 7, 236) (1011, 7)\n",
      "(1636, 7, 236) (1636, 7) (1059, 7, 236) (1059, 7)\n",
      "(1710, 7, 236) (1710, 7) (1107, 7, 236) (1107, 7)\n",
      "(1784, 7, 236) (1784, 7) (1155, 7, 236) (1155, 7)\n",
      "(1815, 7, 236) (1815, 7) (1174, 7, 236) (1174, 7)\n",
      "(1889, 7, 236) (1889, 7) (1222, 7, 236) (1222, 7)\n",
      "(1962, 7, 236) (1962, 7) (1269, 7, 236) (1269, 7)\n",
      "(2036, 7, 236) (2036, 7) (1317, 7, 236) (1317, 7)\n",
      "(2064, 7, 236) (2064, 7) (1334, 7, 236) (1334, 7)\n",
      "(2138, 7, 236) (2138, 7) (1382, 7, 236) (1382, 7)\n",
      "(2212, 7, 236) (2212, 7) (1430, 7, 236) (1430, 7)\n",
      "(2286, 7, 236) (2286, 7) (1478, 7, 236) (1478, 7)\n",
      "(2360, 7, 236) (2360, 7) (1526, 7, 236) (1526, 7)\n",
      "(2434, 7, 236) (2434, 7) (1574, 7, 236) (1574, 7)\n",
      "(2505, 7, 236) (2505, 7) (1621, 7, 236) (1621, 7)\n",
      "(2579, 7, 236) (2579, 7) (1669, 7, 236) (1669, 7)\n",
      "(2653, 7, 236) (2653, 7) (1717, 7, 236) (1717, 7)\n",
      "(2725, 7, 236) (2725, 7) (1764, 7, 236) (1764, 7)\n",
      "(2778, 7, 236) (2778, 7) (1799, 7, 236) (1799, 7)\n",
      "(2852, 7, 236) (2852, 7) (1847, 7, 236) (1847, 7)\n",
      "(2926, 7, 236) (2926, 7) (1895, 7, 236) (1895, 7)\n",
      "(3000, 7, 236) (3000, 7) (1943, 7, 236) (1943, 7)\n",
      "(3074, 7, 236) (3074, 7) (1991, 7, 236) (1991, 7)\n",
      "(3148, 7, 236) (3148, 7) (2039, 7, 236) (2039, 7)\n",
      "(3222, 7, 236) (3222, 7) (2087, 7, 236) (2087, 7)\n",
      "(3296, 7, 236) (3296, 7) (2135, 7, 236) (2135, 7)\n",
      "(3370, 7, 236) (3370, 7) (2183, 7, 236) (2183, 7)\n",
      "(3444, 7, 236) (3444, 7) (2231, 7, 236) (2231, 7)\n",
      "(3518, 7, 236) (3518, 7) (2279, 7, 236) (2279, 7)\n",
      "(3547, 7, 236) (3547, 7) (2298, 7, 236) (2298, 7)\n",
      "(3574, 7, 236) (3574, 7) (2315, 7, 236) (2315, 7)\n",
      "(3626, 7, 236) (3626, 7) (2348, 7, 236) (2348, 7)\n",
      "(3627, 7, 236) (3627, 7) (2348, 7, 236) (2348, 7)\n",
      "(3631, 7, 236) (3631, 7) (2350, 7, 236) (2350, 7)\n",
      "(3671, 7, 236) (3671, 7) (2376, 7, 236) (2376, 7)\n",
      "(3711, 7, 236) (3711, 7) (2402, 7, 236) (2402, 7)\n",
      "(3751, 7, 236) (3751, 7) (2428, 7, 236) (2428, 7)\n",
      "(3791, 7, 236) (3791, 7) (2453, 7, 236) (2453, 7)\n",
      "(3792, 7, 236) (3792, 7) (2453, 7, 236) (2453, 7)\n",
      "(3820, 7, 236) (3820, 7) (2470, 7, 236) (2470, 7)\n",
      "(3821, 7, 236) (3821, 7) (2470, 7, 236) (2470, 7)\n",
      "(3822, 7, 236) (3822, 7) (2470, 7, 236) (2470, 7)\n",
      "(3823, 7, 236) (3823, 7) (2470, 7, 236) (2470, 7)\n",
      "(3824, 7, 236) (3824, 7) (2470, 7, 236) (2470, 7)\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for key in multi_time_data_dict.keys():\n",
    "    values=multi_time_data_dict[key].values\n",
    "    train_index = int(values.shape[0]*0.6)+1\n",
    "    train = values[:train_index, :]\n",
    "    test = values[train_index:, :]\n",
    "    sub_train_X, sub_train_y = train[:, :-t_period], train[:, -t_period:]\n",
    "    sub_test_X, sub_test_y = test[:, :-t_period], test[:, -t_period:]\n",
    "    sub_train_X = sub_train_X.reshape((sub_train_X.shape[0], t_period, ori_column_num))\n",
    "    sub_test_X = sub_test_X.reshape((sub_test_X.shape[0], t_period, ori_column_num))\n",
    "    if i==0:\n",
    "        train_X,train_y,test_X,test_y=sub_train_X,sub_train_y,sub_test_X,sub_test_y\n",
    "    else:\n",
    "        train_X=np.concatenate([train_X,sub_train_X])\n",
    "        train_y=np.concatenate([train_y,sub_train_y])\n",
    "        test_X=np.concatenate([test_X,sub_test_X])\n",
    "        test_y=np.concatenate([test_y,sub_test_y])\n",
    "    i+=1\n",
    "    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1012 16:53:06.866786 14676 deprecation_wrapper.py:119] From C:\\Users\\leung\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1012 16:53:06.937597 14676 deprecation_wrapper.py:119] From C:\\Users\\leung\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1012 16:53:06.951558 14676 deprecation_wrapper.py:119] From C:\\Users\\leung\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1012 16:53:07.283671 14676 deprecation_wrapper.py:119] From C:\\Users\\leung\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1012 16:53:07.317579 14676 deprecation.py:506] From C:\\Users\\leung\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1012 16:53:07.700556 14676 deprecation_wrapper.py:119] From C:\\Users\\leung\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1012 16:53:08.225152 14676 deprecation.py:323] From C:\\Users\\leung\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1012 16:53:09.909641 14676 deprecation_wrapper.py:119] From C:\\Users\\leung\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3824 samples, validate on 2470 samples\n",
      "Epoch 1/2000\n",
      "3824/3824 [==============================] - 3s 902us/step - loss: 4064.5431 - val_loss: 6591.2957\n",
      "Epoch 2/2000\n",
      " 700/3824 [====>.........................] - ETA: 0s - loss: 0.3556"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leung\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:842: RuntimeWarning: Early stopping conditioned on metric `mean_squared_error` which is not available. Available metrics are: val_loss,loss\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824/3824 [==============================] - 1s 362us/step - loss: 3927.3768 - val_loss: 6499.2521\n",
      "Epoch 3/2000\n",
      "3824/3824 [==============================] - 1s 326us/step - loss: 3845.4030 - val_loss: 6425.1193\n",
      "Epoch 4/2000\n",
      "3824/3824 [==============================] - 1s 348us/step - loss: 3775.1014 - val_loss: 6357.8954\n",
      "Epoch 5/2000\n",
      "3824/3824 [==============================] - 1s 343us/step - loss: 3708.5746 - val_loss: 6294.7834\n",
      "Epoch 6/2000\n",
      "3824/3824 [==============================] - 1s 347us/step - loss: 3646.9190 - val_loss: 6234.3321\n",
      "Epoch 7/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 3585.7377 - val_loss: 6177.3103\n",
      "Epoch 8/2000\n",
      "3824/3824 [==============================] - 1s 338us/step - loss: 3532.9481 - val_loss: 6121.1782\n",
      "Epoch 9/2000\n",
      "3824/3824 [==============================] - 1s 357us/step - loss: 3476.8510 - val_loss: 6070.3005\n",
      "Epoch 10/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 3423.5860 - val_loss: 6012.5231\n",
      "Epoch 11/2000\n",
      "3824/3824 [==============================] - 1s 328us/step - loss: 3373.1766 - val_loss: 5962.2446\n",
      "Epoch 12/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 3321.7055 - val_loss: 5914.5896\n",
      "Epoch 13/2000\n",
      "3824/3824 [==============================] - 1s 353us/step - loss: 3277.9157 - val_loss: 5862.7548\n",
      "Epoch 14/2000\n",
      "3824/3824 [==============================] - 1s 386us/step - loss: 3232.1646 - val_loss: 5814.7306\n",
      "Epoch 15/2000\n",
      "3824/3824 [==============================] - 1s 348us/step - loss: 3188.9647 - val_loss: 5768.6746\n",
      "Epoch 16/2000\n",
      "3824/3824 [==============================] - 1s 360us/step - loss: 3143.9484 - val_loss: 5721.9932\n",
      "Epoch 17/2000\n",
      "3824/3824 [==============================] - 1s 344us/step - loss: 3099.2265 - val_loss: 5678.5374\n",
      "Epoch 18/2000\n",
      "3824/3824 [==============================] - 1s 358us/step - loss: 3050.0863 - val_loss: 5634.5374\n",
      "Epoch 19/2000\n",
      "3824/3824 [==============================] - 1s 347us/step - loss: 3017.7520 - val_loss: 5591.1570\n",
      "Epoch 20/2000\n",
      "3824/3824 [==============================] - 1s 352us/step - loss: 2971.7723 - val_loss: 5547.4519\n",
      "Epoch 21/2000\n",
      "3824/3824 [==============================] - 1s 336us/step - loss: 2934.8722 - val_loss: 5506.0712\n",
      "Epoch 22/2000\n",
      "3824/3824 [==============================] - 1s 346us/step - loss: 2895.4183 - val_loss: 5463.6150\n",
      "Epoch 23/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 2868.4468 - val_loss: 5422.0034\n",
      "Epoch 24/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 2836.8854 - val_loss: 5397.0890\n",
      "Epoch 25/2000\n",
      "3824/3824 [==============================] - 1s 355us/step - loss: 2797.5784 - val_loss: 5348.6830\n",
      "Epoch 26/2000\n",
      "3824/3824 [==============================] - 1s 345us/step - loss: 2744.8695 - val_loss: 5305.7636\n",
      "Epoch 27/2000\n",
      "3824/3824 [==============================] - 1s 345us/step - loss: 2707.1571 - val_loss: 5266.9287\n",
      "Epoch 28/2000\n",
      "3824/3824 [==============================] - 2s 403us/step - loss: 2671.9892 - val_loss: 5224.6061\n",
      "Epoch 29/2000\n",
      "3824/3824 [==============================] - 2s 408us/step - loss: 2630.0566 - val_loss: 5189.4580\n",
      "Epoch 30/2000\n",
      "3824/3824 [==============================] - 2s 396us/step - loss: 2606.6530 - val_loss: 5149.1231\n",
      "Epoch 31/2000\n",
      "3824/3824 [==============================] - 1s 389us/step - loss: 2565.4067 - val_loss: 5111.3075\n",
      "Epoch 32/2000\n",
      "3824/3824 [==============================] - 2s 403us/step - loss: 2527.0232 - val_loss: 5073.5995\n",
      "Epoch 33/2000\n",
      "3824/3824 [==============================] - 1s 369us/step - loss: 2506.9094 - val_loss: 5035.8993\n",
      "Epoch 34/2000\n",
      "3824/3824 [==============================] - 1s 374us/step - loss: 2468.7897 - val_loss: 5000.2657\n",
      "Epoch 35/2000\n",
      "3824/3824 [==============================] - 1s 352us/step - loss: 2430.6029 - val_loss: 4962.4807\n",
      "Epoch 36/2000\n",
      "3824/3824 [==============================] - 1s 342us/step - loss: 2401.2140 - val_loss: 4928.5099\n",
      "Epoch 37/2000\n",
      "3824/3824 [==============================] - 1s 378us/step - loss: 2359.5943 - val_loss: 4892.1719\n",
      "Epoch 38/2000\n",
      "3824/3824 [==============================] - 1s 387us/step - loss: 2330.2325 - val_loss: 4858.2845\n",
      "Epoch 39/2000\n",
      "3824/3824 [==============================] - 1s 366us/step - loss: 2297.2959 - val_loss: 4823.3698\n",
      "Epoch 40/2000\n",
      "3824/3824 [==============================] - 1s 383us/step - loss: 2271.8817 - val_loss: 4791.0305\n",
      "Epoch 41/2000\n",
      "3824/3824 [==============================] - 1s 365us/step - loss: 2239.5422 - val_loss: 4756.4270\n",
      "Epoch 42/2000\n",
      "3824/3824 [==============================] - 1s 388us/step - loss: 2213.0922 - val_loss: 4726.3181\n",
      "Epoch 43/2000\n",
      "3824/3824 [==============================] - 2s 424us/step - loss: 2181.6565 - val_loss: 4690.6358\n",
      "Epoch 44/2000\n",
      "3824/3824 [==============================] - 1s 391us/step - loss: 2156.5752 - val_loss: 4663.4223\n",
      "Epoch 45/2000\n",
      "3824/3824 [==============================] - 1s 365us/step - loss: 2122.2686 - val_loss: 4633.5466\n",
      "Epoch 46/2000\n",
      "3824/3824 [==============================] - 1s 354us/step - loss: 2151.9934 - val_loss: 4597.9539\n",
      "Epoch 47/2000\n",
      "3824/3824 [==============================] - 1s 358us/step - loss: 2079.8888 - val_loss: 4610.6570\n",
      "Epoch 48/2000\n",
      "3824/3824 [==============================] - 1s 349us/step - loss: 2056.9780 - val_loss: 4552.6902\n",
      "Epoch 49/2000\n",
      "3824/3824 [==============================] - 1s 373us/step - loss: 2030.6028 - val_loss: 4507.9736\n",
      "Epoch 50/2000\n",
      "3824/3824 [==============================] - 1s 366us/step - loss: 1997.6528 - val_loss: 4480.3499\n",
      "Epoch 51/2000\n",
      "3824/3824 [==============================] - 1s 352us/step - loss: 1967.4113 - val_loss: 4447.1576\n",
      "Epoch 52/2000\n",
      "3824/3824 [==============================] - 1s 365us/step - loss: 1937.7298 - val_loss: 4423.6771\n",
      "Epoch 53/2000\n",
      "3824/3824 [==============================] - 2s 403us/step - loss: 1926.1575 - val_loss: 4388.3459\n",
      "Epoch 54/2000\n",
      "3824/3824 [==============================] - 1s 368us/step - loss: 1895.8127 - val_loss: 4363.9971\n",
      "Epoch 55/2000\n",
      "3824/3824 [==============================] - 1s 366us/step - loss: 1871.5499 - val_loss: 4332.0623\n",
      "Epoch 56/2000\n",
      "3824/3824 [==============================] - 1s 360us/step - loss: 1838.8463 - val_loss: 4307.2582\n",
      "Epoch 57/2000\n",
      "3824/3824 [==============================] - 1s 379us/step - loss: 1805.8719 - val_loss: 4276.7980\n",
      "Epoch 58/2000\n",
      "3824/3824 [==============================] - 2s 408us/step - loss: 1791.3075 - val_loss: 4270.5335\n",
      "Epoch 59/2000\n",
      "3824/3824 [==============================] - 1s 354us/step - loss: 1783.1995 - val_loss: 4229.5184\n",
      "Epoch 60/2000\n",
      "3824/3824 [==============================] - 1s 359us/step - loss: 1746.4216 - val_loss: 4214.0419\n",
      "Epoch 61/2000\n",
      "3824/3824 [==============================] - 1s 375us/step - loss: 1742.4408 - val_loss: 4175.7860\n",
      "Epoch 62/2000\n",
      "3824/3824 [==============================] - 1s 380us/step - loss: 1712.5517 - val_loss: 4146.0435\n",
      "Epoch 63/2000\n",
      "3824/3824 [==============================] - 1s 380us/step - loss: 1680.7904 - val_loss: 4121.6985\n",
      "Epoch 64/2000\n",
      "3824/3824 [==============================] - 1s 365us/step - loss: 1666.3559 - val_loss: 4093.9354\n",
      "Epoch 65/2000\n",
      "3824/3824 [==============================] - 2s 401us/step - loss: 1645.1510 - val_loss: 4071.7629\n",
      "Epoch 66/2000\n",
      "3824/3824 [==============================] - 1s 365us/step - loss: 1621.2043 - val_loss: 4045.2454\n",
      "Epoch 67/2000\n",
      "3824/3824 [==============================] - 1s 363us/step - loss: 1606.2120 - val_loss: 4020.4999\n",
      "Epoch 68/2000\n",
      "3824/3824 [==============================] - 1s 365us/step - loss: 1582.0812 - val_loss: 3998.2328\n",
      "Epoch 69/2000\n",
      "3824/3824 [==============================] - 1s 377us/step - loss: 1559.6894 - val_loss: 3974.0708\n",
      "Epoch 70/2000\n",
      "3824/3824 [==============================] - 1s 374us/step - loss: 1539.9378 - val_loss: 3953.2018\n",
      "Epoch 71/2000\n",
      "3824/3824 [==============================] - 1s 377us/step - loss: 1527.5555 - val_loss: 3931.7178\n",
      "Epoch 72/2000\n",
      "3824/3824 [==============================] - 1s 377us/step - loss: 1505.4390 - val_loss: 3906.7850\n",
      "Epoch 73/2000\n",
      "3824/3824 [==============================] - 1s 373us/step - loss: 1489.3908 - val_loss: 3888.1409\n",
      "Epoch 74/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824/3824 [==============================] - 2s 393us/step - loss: 1481.0505 - val_loss: 3862.9326\n",
      "Epoch 75/2000\n",
      "3824/3824 [==============================] - 1s 380us/step - loss: 1450.8072 - val_loss: 3844.4104\n",
      "Epoch 76/2000\n",
      "3824/3824 [==============================] - 1s 357us/step - loss: 1441.0926 - val_loss: 3817.1546\n",
      "Epoch 77/2000\n",
      "3824/3824 [==============================] - 1s 378us/step - loss: 1423.7353 - val_loss: 3804.0364\n",
      "Epoch 78/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 1408.4374 - val_loss: 3775.2750\n",
      "Epoch 79/2000\n",
      "3824/3824 [==============================] - 1s 349us/step - loss: 1382.7640 - val_loss: 3758.0762\n",
      "Epoch 80/2000\n",
      "3824/3824 [==============================] - 1s 342us/step - loss: 1359.0496 - val_loss: 3735.7558\n",
      "Epoch 81/2000\n",
      "3824/3824 [==============================] - 1s 342us/step - loss: 1348.5985 - val_loss: 3719.4913\n",
      "Epoch 82/2000\n",
      "3824/3824 [==============================] - 1s 344us/step - loss: 1336.2724 - val_loss: 3694.3704\n",
      "Epoch 83/2000\n",
      "3824/3824 [==============================] - 1s 338us/step - loss: 1307.6667 - val_loss: 3700.8555\n",
      "Epoch 84/2000\n",
      "3824/3824 [==============================] - 1s 364us/step - loss: 1305.2154 - val_loss: 3660.9251\n",
      "Epoch 85/2000\n",
      "3824/3824 [==============================] - 2s 393us/step - loss: 1324.8863 - val_loss: 3680.4298\n",
      "Epoch 86/2000\n",
      "3824/3824 [==============================] - 1s 364us/step - loss: 1296.8578 - val_loss: 3620.4576\n",
      "Epoch 87/2000\n",
      "3824/3824 [==============================] - 1s 365us/step - loss: 1258.0324 - val_loss: 3600.2512\n",
      "Epoch 88/2000\n",
      "3824/3824 [==============================] - 1s 365us/step - loss: 1251.9553 - val_loss: 3580.1846\n",
      "Epoch 89/2000\n",
      "3824/3824 [==============================] - 1s 350us/step - loss: 1235.3865 - val_loss: 3570.0381\n",
      "Epoch 90/2000\n",
      "3824/3824 [==============================] - 1s 348us/step - loss: 1224.7855 - val_loss: 3542.5054\n",
      "Epoch 91/2000\n",
      "3824/3824 [==============================] - 1s 348us/step - loss: 1210.7858 - val_loss: 3528.4956\n",
      "Epoch 92/2000\n",
      "3824/3824 [==============================] - 1s 363us/step - loss: 1189.9192 - val_loss: 3509.4531\n",
      "Epoch 93/2000\n",
      "3824/3824 [==============================] - 1s 346us/step - loss: 1184.0020 - val_loss: 3493.2449\n",
      "Epoch 94/2000\n",
      "3824/3824 [==============================] - 1s 348us/step - loss: 1168.2893 - val_loss: 3480.7188\n",
      "Epoch 95/2000\n",
      "3824/3824 [==============================] - 1s 351us/step - loss: 1156.3633 - val_loss: 3586.4328\n",
      "Epoch 96/2000\n",
      "3824/3824 [==============================] - 1s 357us/step - loss: 1215.0536 - val_loss: 3522.4675\n",
      "Epoch 97/2000\n",
      "3824/3824 [==============================] - 1s 369us/step - loss: 1189.3242 - val_loss: 3509.8247\n",
      "Epoch 98/2000\n",
      "3824/3824 [==============================] - 1s 365us/step - loss: 1178.6336 - val_loss: 3489.8871\n",
      "Epoch 99/2000\n",
      "3824/3824 [==============================] - 1s 371us/step - loss: 1167.7601 - val_loss: 3475.4926\n",
      "Epoch 100/2000\n",
      "3824/3824 [==============================] - 1s 359us/step - loss: 1158.0093 - val_loss: 3453.5542\n",
      "Epoch 101/2000\n",
      "3824/3824 [==============================] - 1s 349us/step - loss: 1157.4039 - val_loss: 3459.8399\n",
      "Epoch 102/2000\n",
      "3824/3824 [==============================] - 1s 352us/step - loss: 1156.6141 - val_loss: 3417.8091\n",
      "Epoch 103/2000\n",
      "3824/3824 [==============================] - 1s 352us/step - loss: 1134.5016 - val_loss: 3371.5300\n",
      "Epoch 104/2000\n",
      "3824/3824 [==============================] - 1s 352us/step - loss: 1094.0061 - val_loss: 3437.2016\n",
      "Epoch 105/2000\n",
      "3824/3824 [==============================] - 1s 345us/step - loss: 1114.3832 - val_loss: 3422.7640\n",
      "Epoch 106/2000\n",
      "3824/3824 [==============================] - 1s 340us/step - loss: 1084.8613 - val_loss: 3570.2656\n",
      "Epoch 107/2000\n",
      "3824/3824 [==============================] - 1s 353us/step - loss: 1203.2825 - val_loss: 3482.1152\n",
      "Epoch 108/2000\n",
      "3824/3824 [==============================] - 1s 352us/step - loss: 1161.7799 - val_loss: 3461.3143\n",
      "Epoch 109/2000\n",
      "3824/3824 [==============================] - 1s 353us/step - loss: 1149.5431 - val_loss: 3462.0593\n",
      "Epoch 110/2000\n",
      "3824/3824 [==============================] - 1s 361us/step - loss: 1144.9772 - val_loss: 3338.1886\n",
      "Epoch 111/2000\n",
      "3824/3824 [==============================] - 1s 352us/step - loss: 1055.1401 - val_loss: 3326.9144\n",
      "Epoch 112/2000\n",
      "3824/3824 [==============================] - 1s 352us/step - loss: 1060.3071 - val_loss: 3311.4107\n",
      "Epoch 113/2000\n",
      "3824/3824 [==============================] - 1s 346us/step - loss: 1062.8196 - val_loss: 3301.9080\n",
      "Epoch 114/2000\n",
      "3824/3824 [==============================] - 1s 355us/step - loss: 1055.2853 - val_loss: 3279.1235\n",
      "Epoch 115/2000\n",
      "3824/3824 [==============================] - 1s 346us/step - loss: 1025.4116 - val_loss: 3274.3301\n",
      "Epoch 116/2000\n",
      "3824/3824 [==============================] - 1s 343us/step - loss: 1014.1190 - val_loss: 3246.3107\n",
      "Epoch 117/2000\n",
      "3824/3824 [==============================] - 1s 347us/step - loss: 1011.8800 - val_loss: 3247.1560\n",
      "Epoch 118/2000\n",
      "3824/3824 [==============================] - 1s 342us/step - loss: 998.1866 - val_loss: 3224.7485\n",
      "Epoch 119/2000\n",
      "3824/3824 [==============================] - 1s 349us/step - loss: 986.7370 - val_loss: 3213.3266\n",
      "Epoch 120/2000\n",
      "3824/3824 [==============================] - 1s 363us/step - loss: 983.4019 - val_loss: 3198.4248\n",
      "Epoch 121/2000\n",
      "3824/3824 [==============================] - 1s 366us/step - loss: 972.0371 - val_loss: 3183.7958\n",
      "Epoch 122/2000\n",
      "3824/3824 [==============================] - 1s 371us/step - loss: 965.3659 - val_loss: 3174.4705\n",
      "Epoch 123/2000\n",
      "3824/3824 [==============================] - 1s 356us/step - loss: 951.8167 - val_loss: 3158.8431\n",
      "Epoch 124/2000\n",
      "3824/3824 [==============================] - 1s 369us/step - loss: 939.8313 - val_loss: 3148.3819\n",
      "Epoch 125/2000\n",
      "3824/3824 [==============================] - 1s 347us/step - loss: 948.2222 - val_loss: 3129.1476\n",
      "Epoch 126/2000\n",
      "3824/3824 [==============================] - 1s 351us/step - loss: 930.0259 - val_loss: 3112.5990\n",
      "Epoch 127/2000\n",
      "3824/3824 [==============================] - 1s 358us/step - loss: 913.9440 - val_loss: 3101.3504\n",
      "Epoch 128/2000\n",
      "3824/3824 [==============================] - 1s 355us/step - loss: 912.0529 - val_loss: 3086.7985\n",
      "Epoch 129/2000\n",
      "3824/3824 [==============================] - 1s 362us/step - loss: 912.3465 - val_loss: 3084.3900\n",
      "Epoch 130/2000\n",
      "3824/3824 [==============================] - 1s 350us/step - loss: 898.4205 - val_loss: 3085.4250\n",
      "Epoch 131/2000\n",
      "3824/3824 [==============================] - 1s 368us/step - loss: 914.0068 - val_loss: 3059.5241\n",
      "Epoch 132/2000\n",
      "3824/3824 [==============================] - 1s 359us/step - loss: 899.3692 - val_loss: 3051.6616\n",
      "Epoch 133/2000\n",
      "3824/3824 [==============================] - 1s 366us/step - loss: 875.0183 - val_loss: 3038.9319\n",
      "Epoch 134/2000\n",
      "3824/3824 [==============================] - 1s 371us/step - loss: 877.5776 - val_loss: 3022.0927\n",
      "Epoch 135/2000\n",
      "3824/3824 [==============================] - 1s 384us/step - loss: 873.5459 - val_loss: 3001.9117\n",
      "Epoch 136/2000\n",
      "3824/3824 [==============================] - 1s 375us/step - loss: 859.8362 - val_loss: 2985.2535\n",
      "Epoch 137/2000\n",
      "3824/3824 [==============================] - 1s 371us/step - loss: 847.7600 - val_loss: 2976.8542\n",
      "Epoch 138/2000\n",
      "3824/3824 [==============================] - 1s 374us/step - loss: 842.0065 - val_loss: 2982.5684\n",
      "Epoch 139/2000\n",
      "3824/3824 [==============================] - 1s 358us/step - loss: 842.0027 - val_loss: 2974.7121\n",
      "Epoch 140/2000\n",
      "3824/3824 [==============================] - 1s 361us/step - loss: 842.1619 - val_loss: 2942.9391\n",
      "Epoch 141/2000\n",
      "3824/3824 [==============================] - 1s 363us/step - loss: 819.6720 - val_loss: 2948.3355\n",
      "Epoch 142/2000\n",
      "3824/3824 [==============================] - 1s 377us/step - loss: 827.5436 - val_loss: 2918.1650\n",
      "Epoch 143/2000\n",
      "3824/3824 [==============================] - 2s 393us/step - loss: 819.3766 - val_loss: 2901.9958\n",
      "Epoch 144/2000\n",
      "3824/3824 [==============================] - 1s 378us/step - loss: 815.2706 - val_loss: 2886.7918\n",
      "Epoch 145/2000\n",
      "3824/3824 [==============================] - 1s 380us/step - loss: 806.7907 - val_loss: 2888.8614\n",
      "Epoch 146/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824/3824 [==============================] - 1s 369us/step - loss: 805.2595 - val_loss: 2898.8493\n",
      "Epoch 147/2000\n",
      "3824/3824 [==============================] - 1s 370us/step - loss: 833.3744 - val_loss: 2893.7531\n",
      "Epoch 148/2000\n",
      "3824/3824 [==============================] - 1s 356us/step - loss: 799.9238 - val_loss: 2903.9001\n",
      "Epoch 149/2000\n",
      "3824/3824 [==============================] - 1s 356us/step - loss: 800.1667 - val_loss: 2957.5457\n",
      "Epoch 150/2000\n",
      "3824/3824 [==============================] - 1s 365us/step - loss: 835.4056 - val_loss: 2936.7766\n",
      "Epoch 151/2000\n",
      "3824/3824 [==============================] - 1s 380us/step - loss: 841.1839 - val_loss: 2925.3143\n",
      "Epoch 152/2000\n",
      "3824/3824 [==============================] - 1s 365us/step - loss: 804.9657 - val_loss: 2815.7214\n",
      "Epoch 153/2000\n",
      "3824/3824 [==============================] - 2s 403us/step - loss: 783.0561 - val_loss: 2797.5436\n",
      "Epoch 154/2000\n",
      "3824/3824 [==============================] - 1s 371us/step - loss: 760.9909 - val_loss: 2787.4473\n",
      "Epoch 155/2000\n",
      "3824/3824 [==============================] - 1s 378us/step - loss: 738.6607 - val_loss: 2793.8754\n",
      "Epoch 156/2000\n",
      "3824/3824 [==============================] - 2s 394us/step - loss: 736.3188 - val_loss: 2805.4133\n",
      "Epoch 157/2000\n",
      "3824/3824 [==============================] - 2s 398us/step - loss: 814.4136 - val_loss: 2839.8448\n",
      "Epoch 158/2000\n",
      "3824/3824 [==============================] - 1s 338us/step - loss: 802.2562 - val_loss: 2799.4434\n",
      "Epoch 159/2000\n",
      "3824/3824 [==============================] - 1s 351us/step - loss: 742.6622 - val_loss: 2877.8819\n",
      "Epoch 160/2000\n",
      "3824/3824 [==============================] - 1s 377us/step - loss: 795.1561 - val_loss: 2770.9095\n",
      "Epoch 161/2000\n",
      "3824/3824 [==============================] - 2s 406us/step - loss: 745.7029 - val_loss: 2766.0595\n",
      "Epoch 162/2000\n",
      "3824/3824 [==============================] - 1s 353us/step - loss: 728.0934 - val_loss: 2726.9665\n",
      "Epoch 163/2000\n",
      "3824/3824 [==============================] - 2s 472us/step - loss: 718.6101 - val_loss: 2732.1468\n",
      "Epoch 164/2000\n",
      "3824/3824 [==============================] - 2s 413us/step - loss: 718.3685 - val_loss: 2741.2392\n",
      "Epoch 165/2000\n",
      "3824/3824 [==============================] - 1s 377us/step - loss: 747.8170 - val_loss: 2703.9775\n",
      "Epoch 166/2000\n",
      "3824/3824 [==============================] - 1s 365us/step - loss: 723.7820 - val_loss: 2654.3794\n",
      "Epoch 167/2000\n",
      "3824/3824 [==============================] - 1s 362us/step - loss: 679.8207 - val_loss: 2688.4388\n",
      "Epoch 168/2000\n",
      "3824/3824 [==============================] - 1s 362us/step - loss: 698.4440 - val_loss: 2643.9423\n",
      "Epoch 169/2000\n",
      "3824/3824 [==============================] - 1s 355us/step - loss: 680.0437 - val_loss: 2647.2659\n",
      "Epoch 170/2000\n",
      "3824/3824 [==============================] - 1s 340us/step - loss: 688.4173 - val_loss: 2631.4151\n",
      "Epoch 171/2000\n",
      "3824/3824 [==============================] - 1s 351us/step - loss: 674.7801 - val_loss: 2695.0929\n",
      "Epoch 172/2000\n",
      "3824/3824 [==============================] - 1s 341us/step - loss: 675.4009 - val_loss: 2614.4580\n",
      "Epoch 173/2000\n",
      "3824/3824 [==============================] - 1s 349us/step - loss: 665.7320 - val_loss: 2631.2065\n",
      "Epoch 174/2000\n",
      "3824/3824 [==============================] - 1s 338us/step - loss: 671.3998 - val_loss: 2592.9706\n",
      "Epoch 175/2000\n",
      "3824/3824 [==============================] - 1s 342us/step - loss: 665.5945 - val_loss: 2585.9005\n",
      "Epoch 176/2000\n",
      "3824/3824 [==============================] - 1s 358us/step - loss: 660.9724 - val_loss: 2589.9719\n",
      "Epoch 177/2000\n",
      "3824/3824 [==============================] - 1s 357us/step - loss: 691.7681 - val_loss: 2587.9497\n",
      "Epoch 178/2000\n",
      "3824/3824 [==============================] - 1s 359us/step - loss: 661.9499 - val_loss: 2554.9839\n",
      "Epoch 179/2000\n",
      "3824/3824 [==============================] - 1s 363us/step - loss: 645.0837 - val_loss: 2555.0375\n",
      "Epoch 180/2000\n",
      "3824/3824 [==============================] - 1s 361us/step - loss: 637.8399 - val_loss: 2545.8502\n",
      "Epoch 181/2000\n",
      "3824/3824 [==============================] - 1s 346us/step - loss: 633.5775 - val_loss: 2605.1402\n",
      "Epoch 182/2000\n",
      "3824/3824 [==============================] - 1s 338us/step - loss: 675.0462 - val_loss: 2569.3223\n",
      "Epoch 183/2000\n",
      "3824/3824 [==============================] - 1s 346us/step - loss: 643.0532 - val_loss: 2645.9462\n",
      "Epoch 184/2000\n",
      "3824/3824 [==============================] - 1s 364us/step - loss: 712.2980 - val_loss: 2593.1271\n",
      "Epoch 185/2000\n",
      "3824/3824 [==============================] - 1s 351us/step - loss: 690.0031 - val_loss: 2577.9048\n",
      "Epoch 186/2000\n",
      "3824/3824 [==============================] - 1s 363us/step - loss: 687.5446 - val_loss: 2543.5916\n",
      "Epoch 187/2000\n",
      "3824/3824 [==============================] - 1s 348us/step - loss: 638.0898 - val_loss: 2524.1105\n",
      "Epoch 188/2000\n",
      "3824/3824 [==============================] - 1s 361us/step - loss: 644.3028 - val_loss: 2528.7684\n",
      "Epoch 189/2000\n",
      "3824/3824 [==============================] - 1s 355us/step - loss: 645.2068 - val_loss: 2543.9382\n",
      "Epoch 190/2000\n",
      "3824/3824 [==============================] - 1s 361us/step - loss: 632.8861 - val_loss: 2502.6587\n",
      "Epoch 191/2000\n",
      "3824/3824 [==============================] - 1s 360us/step - loss: 618.2321 - val_loss: 2497.7267\n",
      "Epoch 192/2000\n",
      "3824/3824 [==============================] - 1s 359us/step - loss: 617.4151 - val_loss: 2484.3743\n",
      "Epoch 193/2000\n",
      "3824/3824 [==============================] - 1s 353us/step - loss: 615.4689 - val_loss: 2534.5544\n",
      "Epoch 194/2000\n",
      "3824/3824 [==============================] - 1s 349us/step - loss: 635.9807 - val_loss: 2468.7236\n",
      "Epoch 195/2000\n",
      "3824/3824 [==============================] - 1s 356us/step - loss: 601.2898 - val_loss: 2462.7340\n",
      "Epoch 196/2000\n",
      "3824/3824 [==============================] - 1s 359us/step - loss: 597.2399 - val_loss: 2449.9536\n",
      "Epoch 197/2000\n",
      "3824/3824 [==============================] - 1s 359us/step - loss: 592.9505 - val_loss: 2444.3343\n",
      "Epoch 198/2000\n",
      "3824/3824 [==============================] - 1s 354us/step - loss: 593.9340 - val_loss: 2442.1564\n",
      "Epoch 199/2000\n",
      "3824/3824 [==============================] - 1s 359us/step - loss: 586.3868 - val_loss: 2447.3800\n",
      "Epoch 200/2000\n",
      "3824/3824 [==============================] - 1s 376us/step - loss: 615.4680 - val_loss: 2417.3721\n",
      "Epoch 201/2000\n",
      "3824/3824 [==============================] - 2s 419us/step - loss: 573.7968 - val_loss: 2419.3299\n",
      "Epoch 202/2000\n",
      "3824/3824 [==============================] - 1s 383us/step - loss: 588.0517 - val_loss: 2426.6300\n",
      "Epoch 203/2000\n",
      "3824/3824 [==============================] - 1s 388us/step - loss: 603.1776 - val_loss: 2430.7366\n",
      "Epoch 204/2000\n",
      "3824/3824 [==============================] - 1s 368us/step - loss: 632.5071 - val_loss: 2394.6599\n",
      "Epoch 205/2000\n",
      "3824/3824 [==============================] - 1s 357us/step - loss: 577.5849 - val_loss: 2402.0175\n",
      "Epoch 206/2000\n",
      "3824/3824 [==============================] - 1s 349us/step - loss: 589.4481 - val_loss: 2368.4176\n",
      "Epoch 207/2000\n",
      "3824/3824 [==============================] - 1s 352us/step - loss: 562.5103 - val_loss: 2375.3206\n",
      "Epoch 208/2000\n",
      "3824/3824 [==============================] - 2s 397us/step - loss: 578.7396 - val_loss: 2358.8611\n",
      "Epoch 209/2000\n",
      "3824/3824 [==============================] - 1s 348us/step - loss: 566.7253 - val_loss: 2375.8022\n",
      "Epoch 210/2000\n",
      "3824/3824 [==============================] - 1s 348us/step - loss: 580.1267 - val_loss: 2359.0059\n",
      "Epoch 211/2000\n",
      "3824/3824 [==============================] - 1s 362us/step - loss: 593.8734 - val_loss: 2344.3782\n",
      "Epoch 212/2000\n",
      "3824/3824 [==============================] - 1s 354us/step - loss: 560.5030 - val_loss: 2326.2007\n",
      "Epoch 213/2000\n",
      "3824/3824 [==============================] - 1s 369us/step - loss: 550.0374 - val_loss: 2325.7479\n",
      "Epoch 214/2000\n",
      "3824/3824 [==============================] - 1s 379us/step - loss: 553.1486 - val_loss: 2325.0392\n",
      "Epoch 215/2000\n",
      "3824/3824 [==============================] - 1s 389us/step - loss: 539.1082 - val_loss: 2300.8371\n",
      "Epoch 216/2000\n",
      "3824/3824 [==============================] - 1s 352us/step - loss: 552.8921 - val_loss: 2304.4784\n",
      "Epoch 217/2000\n",
      "3824/3824 [==============================] - 1s 341us/step - loss: 528.5483 - val_loss: 2300.9097\n",
      "Epoch 218/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824/3824 [==============================] - 1s 339us/step - loss: 534.7608 - val_loss: 2296.9996\n",
      "Epoch 219/2000\n",
      "3824/3824 [==============================] - 1s 342us/step - loss: 535.4405 - val_loss: 2281.2650\n",
      "Epoch 220/2000\n",
      "3824/3824 [==============================] - 1s 346us/step - loss: 538.1083 - val_loss: 2306.5102\n",
      "Epoch 221/2000\n",
      "3824/3824 [==============================] - 1s 359us/step - loss: 561.2788 - val_loss: 2287.2757\n",
      "Epoch 222/2000\n",
      "3824/3824 [==============================] - 1s 325us/step - loss: 556.5725 - val_loss: 2251.3119\n",
      "Epoch 223/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 528.1507 - val_loss: 2242.2579\n",
      "Epoch 224/2000\n",
      "3824/3824 [==============================] - 1s 359us/step - loss: 517.6743 - val_loss: 2237.1496\n",
      "Epoch 225/2000\n",
      "3824/3824 [==============================] - 1s 363us/step - loss: 511.7799 - val_loss: 2241.4673\n",
      "Epoch 226/2000\n",
      "3824/3824 [==============================] - 1s 359us/step - loss: 515.0403 - val_loss: 2220.2784\n",
      "Epoch 227/2000\n",
      "3824/3824 [==============================] - 1s 364us/step - loss: 516.7243 - val_loss: 2174.4512\n",
      "Epoch 228/2000\n",
      "3824/3824 [==============================] - 1s 340us/step - loss: 498.4472 - val_loss: 2170.8471\n",
      "Epoch 229/2000\n",
      "3824/3824 [==============================] - 1s 321us/step - loss: 498.9774 - val_loss: 2154.6373\n",
      "Epoch 230/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 463.9024 - val_loss: 2141.6159\n",
      "Epoch 231/2000\n",
      "3824/3824 [==============================] - 1s 325us/step - loss: 483.6613 - val_loss: 2134.8657\n",
      "Epoch 232/2000\n",
      "3824/3824 [==============================] - 1s 326us/step - loss: 493.4685 - val_loss: 2130.8609\n",
      "Epoch 233/2000\n",
      "3824/3824 [==============================] - 1s 315us/step - loss: 481.6791 - val_loss: 2129.4773\n",
      "Epoch 234/2000\n",
      "3824/3824 [==============================] - 1s 318us/step - loss: 488.5938 - val_loss: 2115.9583\n",
      "Epoch 235/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 476.2051 - val_loss: 2098.7186\n",
      "Epoch 236/2000\n",
      "3824/3824 [==============================] - 1s 340us/step - loss: 451.7420 - val_loss: 2097.4886\n",
      "Epoch 237/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 464.8967 - val_loss: 2088.4794\n",
      "Epoch 238/2000\n",
      "3824/3824 [==============================] - 1s 344us/step - loss: 457.3858 - val_loss: 2093.6178\n",
      "Epoch 239/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 470.2228 - val_loss: 2111.3809\n",
      "Epoch 240/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 524.2166 - val_loss: 2287.3042\n",
      "Epoch 241/2000\n",
      "3824/3824 [==============================] - 1s 330us/step - loss: 548.5661 - val_loss: 2683.9301\n",
      "Epoch 242/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 792.5954 - val_loss: 2544.3797\n",
      "Epoch 243/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 621.0626 - val_loss: 2309.5632\n",
      "Epoch 244/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 516.6134 - val_loss: 2363.7709\n",
      "Epoch 245/2000\n",
      "3824/3824 [==============================] - 1s 326us/step - loss: 562.1596 - val_loss: 2337.7709\n",
      "Epoch 246/2000\n",
      "3824/3824 [==============================] - 1s 340us/step - loss: 553.2842 - val_loss: 2340.0569\n",
      "Epoch 247/2000\n",
      "3824/3824 [==============================] - 1s 354us/step - loss: 543.6854 - val_loss: 2307.2563\n",
      "Epoch 248/2000\n",
      "3824/3824 [==============================] - 1s 338us/step - loss: 539.4511 - val_loss: 2311.0408\n",
      "Epoch 249/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 550.0958 - val_loss: 2189.4605\n",
      "Epoch 250/2000\n",
      "3824/3824 [==============================] - 1s 343us/step - loss: 511.6534 - val_loss: 2186.1276\n",
      "Epoch 251/2000\n",
      "3824/3824 [==============================] - 1s 345us/step - loss: 475.5860 - val_loss: 2180.2726\n",
      "Epoch 252/2000\n",
      "3824/3824 [==============================] - 1s 336us/step - loss: 505.3285 - val_loss: 2164.8295\n",
      "Epoch 253/2000\n",
      "3824/3824 [==============================] - 1s 343us/step - loss: 504.9388 - val_loss: 2157.8434\n",
      "Epoch 254/2000\n",
      "3824/3824 [==============================] - 1s 341us/step - loss: 485.3809 - val_loss: 2158.3666\n",
      "Epoch 255/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 486.5517 - val_loss: 2144.4511\n",
      "Epoch 256/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 491.8481 - val_loss: 2145.5932\n",
      "Epoch 257/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 476.2491 - val_loss: 2164.1667\n",
      "Epoch 258/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 536.6582 - val_loss: 2242.9929\n",
      "Epoch 259/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 571.6998 - val_loss: 2197.4659\n",
      "Epoch 260/2000\n",
      "3824/3824 [==============================] - 1s 343us/step - loss: 541.9464 - val_loss: 2201.1062\n",
      "Epoch 261/2000\n",
      "3824/3824 [==============================] - 1s 343us/step - loss: 519.2893 - val_loss: 2174.7928\n",
      "Epoch 262/2000\n",
      "3824/3824 [==============================] - 1s 343us/step - loss: 618.0791 - val_loss: 2129.0987\n",
      "Epoch 263/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 494.2563 - val_loss: 2128.9289\n",
      "Epoch 264/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 480.6324 - val_loss: 2102.0925\n",
      "Epoch 265/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 469.2287 - val_loss: 2090.6395\n",
      "Epoch 266/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 460.6196 - val_loss: 2083.8112\n",
      "Epoch 267/2000\n",
      "3824/3824 [==============================] - 1s 328us/step - loss: 477.1349 - val_loss: 2079.2939\n",
      "Epoch 268/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 460.0167 - val_loss: 2056.0381\n",
      "Epoch 269/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 452.5800 - val_loss: 2064.9727\n",
      "Epoch 270/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 452.3596 - val_loss: 2047.3437\n",
      "Epoch 271/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 462.4226 - val_loss: 2048.0477\n",
      "Epoch 272/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 441.4284 - val_loss: 2048.5661\n",
      "Epoch 273/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 461.2037 - val_loss: 2033.4811\n",
      "Epoch 274/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 440.2295 - val_loss: 2024.9205\n",
      "Epoch 275/2000\n",
      "3824/3824 [==============================] - 1s 340us/step - loss: 464.0505 - val_loss: 2064.6013\n",
      "Epoch 276/2000\n",
      "3824/3824 [==============================] - 1s 345us/step - loss: 480.8009 - val_loss: 2018.8275\n",
      "Epoch 277/2000\n",
      "3824/3824 [==============================] - 1s 346us/step - loss: 452.3563 - val_loss: 2056.8181\n",
      "Epoch 278/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 474.1837 - val_loss: 2008.1656\n",
      "Epoch 279/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 445.6006 - val_loss: 2003.4683\n",
      "Epoch 280/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 436.5991 - val_loss: 1994.2983\n",
      "Epoch 281/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 438.4443 - val_loss: 2018.3861\n",
      "Epoch 282/2000\n",
      "3824/3824 [==============================] - 1s 336us/step - loss: 446.8028 - val_loss: 2177.1984\n",
      "Epoch 283/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 555.6365 - val_loss: 2075.0266\n",
      "Epoch 284/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 486.8239 - val_loss: 2043.0238\n",
      "Epoch 285/2000\n",
      "3824/3824 [==============================] - 1s 346us/step - loss: 499.9416 - val_loss: 2005.7046\n",
      "Epoch 286/2000\n",
      "3824/3824 [==============================] - 1s 346us/step - loss: 450.0835 - val_loss: 1998.3284\n",
      "Epoch 287/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 436.7750 - val_loss: 1970.2838\n",
      "Epoch 288/2000\n",
      "3824/3824 [==============================] - 1s 347us/step - loss: 406.8962 - val_loss: 1942.7050\n",
      "Epoch 289/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 432.8791 - val_loss: 1962.9729\n",
      "Epoch 290/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824/3824 [==============================] - 1s 325us/step - loss: 416.1779 - val_loss: 1933.7971\n",
      "Epoch 291/2000\n",
      "3824/3824 [==============================] - 1s 326us/step - loss: 438.1968 - val_loss: 1947.7735\n",
      "Epoch 292/2000\n",
      "3824/3824 [==============================] - 1s 326us/step - loss: 422.6309 - val_loss: 1917.0526\n",
      "Epoch 293/2000\n",
      "3824/3824 [==============================] - 1s 328us/step - loss: 434.6071 - val_loss: 1928.8235\n",
      "Epoch 294/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 415.5761 - val_loss: 1906.8324\n",
      "Epoch 295/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 432.9912 - val_loss: 1933.8413\n",
      "Epoch 296/2000\n",
      "3824/3824 [==============================] - 1s 320us/step - loss: 432.9620 - val_loss: 1904.9546\n",
      "Epoch 297/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 430.4910 - val_loss: 1959.3132\n",
      "Epoch 298/2000\n",
      "3824/3824 [==============================] - 1s 338us/step - loss: 425.0542 - val_loss: 1880.6134\n",
      "Epoch 299/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 400.8622 - val_loss: 1870.3831\n",
      "Epoch 300/2000\n",
      "3824/3824 [==============================] - 1s 341us/step - loss: 394.7419 - val_loss: 1842.7783\n",
      "Epoch 301/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 409.6717 - val_loss: 1914.9757\n",
      "Epoch 302/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 450.0742 - val_loss: 1917.8397\n",
      "Epoch 303/2000\n",
      "3824/3824 [==============================] - 1s 319us/step - loss: 469.7065 - val_loss: 1853.6639\n",
      "Epoch 304/2000\n",
      "3824/3824 [==============================] - 1s 321us/step - loss: 416.9538 - val_loss: 1874.3773\n",
      "Epoch 305/2000\n",
      "3824/3824 [==============================] - 1s 317us/step - loss: 435.5466 - val_loss: 1830.3353\n",
      "Epoch 306/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 398.3632 - val_loss: 1835.9740\n",
      "Epoch 307/2000\n",
      "3824/3824 [==============================] - 1s 317us/step - loss: 379.0801 - val_loss: 1815.6895\n",
      "Epoch 308/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 378.4023 - val_loss: 1834.4234\n",
      "Epoch 309/2000\n",
      "3824/3824 [==============================] - 1s 325us/step - loss: 379.4880 - val_loss: 1806.3252\n",
      "Epoch 310/2000\n",
      "3824/3824 [==============================] - 1s 358us/step - loss: 380.5628 - val_loss: 1814.4129\n",
      "Epoch 311/2000\n",
      "3824/3824 [==============================] - 1s 375us/step - loss: 380.8014 - val_loss: 1799.1550\n",
      "Epoch 312/2000\n",
      "3824/3824 [==============================] - 2s 419us/step - loss: 368.3258 - val_loss: 1808.7868\n",
      "Epoch 313/2000\n",
      "3824/3824 [==============================] - 1s 366us/step - loss: 382.5809 - val_loss: 1901.1693\n",
      "Epoch 314/2000\n",
      "3824/3824 [==============================] - 1s 369us/step - loss: 460.8851 - val_loss: 1881.4287\n",
      "Epoch 315/2000\n",
      "3824/3824 [==============================] - 1s 351us/step - loss: 508.0992 - val_loss: 1845.1106\n",
      "Epoch 316/2000\n",
      "3824/3824 [==============================] - 1s 357us/step - loss: 434.7414 - val_loss: 1857.1161\n",
      "Epoch 317/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 478.0570 - val_loss: 1782.3379\n",
      "Epoch 318/2000\n",
      "3824/3824 [==============================] - 1s 342us/step - loss: 392.1528 - val_loss: 1821.7334\n",
      "Epoch 319/2000\n",
      "3824/3824 [==============================] - 1s 363us/step - loss: 402.7579 - val_loss: 1789.6616\n",
      "Epoch 320/2000\n",
      "3824/3824 [==============================] - 1s 376us/step - loss: 422.9451 - val_loss: 1885.2188\n",
      "Epoch 321/2000\n",
      "3824/3824 [==============================] - 1s 373us/step - loss: 413.2241 - val_loss: 1879.8453\n",
      "Epoch 322/2000\n",
      "3824/3824 [==============================] - 1s 367us/step - loss: 417.1038 - val_loss: 1866.8427\n",
      "Epoch 323/2000\n",
      "3824/3824 [==============================] - 1s 357us/step - loss: 408.6691 - val_loss: 1868.1809\n",
      "Epoch 324/2000\n",
      "3824/3824 [==============================] - 1s 375us/step - loss: 402.2829 - val_loss: 1847.3145\n",
      "Epoch 325/2000\n",
      "3824/3824 [==============================] - 1s 351us/step - loss: 389.3207 - val_loss: 1851.6153\n",
      "Epoch 326/2000\n",
      "3824/3824 [==============================] - 1s 350us/step - loss: 387.0007 - val_loss: 1834.8890\n",
      "Epoch 327/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 401.1158 - val_loss: 1849.7951\n",
      "Epoch 328/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 390.1653 - val_loss: 1832.0798\n",
      "Epoch 329/2000\n",
      "3824/3824 [==============================] - 1s 345us/step - loss: 412.9256 - val_loss: 1832.7945\n",
      "Epoch 330/2000\n",
      "3824/3824 [==============================] - 1s 349us/step - loss: 390.4862 - val_loss: 1828.4068\n",
      "Epoch 331/2000\n",
      "3824/3824 [==============================] - 2s 425us/step - loss: 391.4407 - val_loss: 1832.5202\n",
      "Epoch 332/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 390.2522 - val_loss: 1815.3329\n",
      "Epoch 333/2000\n",
      "3824/3824 [==============================] - 1s 343us/step - loss: 378.9163 - val_loss: 1812.1683\n",
      "Epoch 334/2000\n",
      "3824/3824 [==============================] - 1s 356us/step - loss: 379.6022 - val_loss: 1807.9827\n",
      "Epoch 335/2000\n",
      "3824/3824 [==============================] - 1s 346us/step - loss: 372.1152 - val_loss: 1799.9394\n",
      "Epoch 336/2000\n",
      "3824/3824 [==============================] - 1s 352us/step - loss: 368.5303 - val_loss: 1816.8492\n",
      "Epoch 337/2000\n",
      "3824/3824 [==============================] - 1s 354us/step - loss: 365.5812 - val_loss: 1793.2170\n",
      "Epoch 338/2000\n",
      "3824/3824 [==============================] - 1s 357us/step - loss: 396.3846 - val_loss: 1827.7442\n",
      "Epoch 339/2000\n",
      "3824/3824 [==============================] - 1s 355us/step - loss: 436.9019 - val_loss: 1787.8868\n",
      "Epoch 340/2000\n",
      "3824/3824 [==============================] - 1s 340us/step - loss: 388.4974 - val_loss: 1790.5539\n",
      "Epoch 341/2000\n",
      "3824/3824 [==============================] - 1s 347us/step - loss: 402.6224 - val_loss: 1778.7598\n",
      "Epoch 342/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 379.6673 - val_loss: 1763.2203\n",
      "Epoch 343/2000\n",
      "3824/3824 [==============================] - 1s 345us/step - loss: 353.2775 - val_loss: 1772.4479\n",
      "Epoch 344/2000\n",
      "3824/3824 [==============================] - 1s 344us/step - loss: 366.2224 - val_loss: 1757.4036\n",
      "Epoch 345/2000\n",
      "3824/3824 [==============================] - 1s 348us/step - loss: 353.9288 - val_loss: 1754.1071\n",
      "Epoch 346/2000\n",
      "3824/3824 [==============================] - 1s 363us/step - loss: 352.6980 - val_loss: 1738.4855\n",
      "Epoch 347/2000\n",
      "3824/3824 [==============================] - 1s 349us/step - loss: 359.8744 - val_loss: 1742.0033\n",
      "Epoch 348/2000\n",
      "3824/3824 [==============================] - 1s 347us/step - loss: 357.0073 - val_loss: 1728.2734\n",
      "Epoch 349/2000\n",
      "3824/3824 [==============================] - 1s 350us/step - loss: 359.8482 - val_loss: 1727.7064\n",
      "Epoch 350/2000\n",
      "3824/3824 [==============================] - 1s 344us/step - loss: 364.7064 - val_loss: 1716.7024\n",
      "Epoch 351/2000\n",
      "3824/3824 [==============================] - 1s 341us/step - loss: 354.6786 - val_loss: 1721.6567\n",
      "Epoch 352/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 345.9455 - val_loss: 1707.1070\n",
      "Epoch 353/2000\n",
      "3824/3824 [==============================] - 1s 343us/step - loss: 334.4586 - val_loss: 1712.6117\n",
      "Epoch 354/2000\n",
      "3824/3824 [==============================] - 1s 340us/step - loss: 355.3785 - val_loss: 1701.6794\n",
      "Epoch 355/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 335.6010 - val_loss: 1705.2493\n",
      "Epoch 356/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 331.0201 - val_loss: 1701.2082\n",
      "Epoch 357/2000\n",
      "3824/3824 [==============================] - 1s 341us/step - loss: 356.5269 - val_loss: 1696.0090\n",
      "Epoch 358/2000\n",
      "3824/3824 [==============================] - 1s 353us/step - loss: 373.5968 - val_loss: 1698.6776\n",
      "Epoch 359/2000\n",
      "3824/3824 [==============================] - 1s 349us/step - loss: 385.7461 - val_loss: 1701.0760\n",
      "Epoch 360/2000\n",
      "3824/3824 [==============================] - 1s 348us/step - loss: 374.1122 - val_loss: 1704.0822\n",
      "Epoch 361/2000\n",
      "3824/3824 [==============================] - 1s 357us/step - loss: 377.5897 - val_loss: 1752.7538\n",
      "Epoch 362/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824/3824 [==============================] - 1s 346us/step - loss: 457.5489 - val_loss: 1695.3777\n",
      "Epoch 363/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 400.0326 - val_loss: 1713.2490\n",
      "Epoch 364/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 408.2858 - val_loss: 1701.1474\n",
      "Epoch 365/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 388.7308 - val_loss: 1674.9845\n",
      "Epoch 366/2000\n",
      "3824/3824 [==============================] - 1s 328us/step - loss: 375.0118 - val_loss: 1678.6769\n",
      "Epoch 367/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 390.4001 - val_loss: 1698.7858\n",
      "Epoch 368/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 395.3028 - val_loss: 1662.6944\n",
      "Epoch 369/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 368.4996 - val_loss: 1646.9131\n",
      "Epoch 370/2000\n",
      "3824/3824 [==============================] - 1s 341us/step - loss: 355.0499 - val_loss: 1638.4630\n",
      "Epoch 371/2000\n",
      "3824/3824 [==============================] - 1s 342us/step - loss: 325.0744 - val_loss: 1639.0266\n",
      "Epoch 372/2000\n",
      "3824/3824 [==============================] - 1s 354us/step - loss: 325.0059 - val_loss: 1549.1561\n",
      "Epoch 373/2000\n",
      "3824/3824 [==============================] - 1s 345us/step - loss: 309.7979 - val_loss: 1615.9966\n",
      "Epoch 374/2000\n",
      "3824/3824 [==============================] - 1s 343us/step - loss: 322.7970 - val_loss: 1542.1311\n",
      "Epoch 375/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 310.2831 - val_loss: 1539.4517\n",
      "Epoch 376/2000\n",
      "3824/3824 [==============================] - 1s 330us/step - loss: 305.2328 - val_loss: 1537.6299\n",
      "Epoch 377/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 307.4370 - val_loss: 1535.6968\n",
      "Epoch 378/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 303.1833 - val_loss: 1528.2467\n",
      "Epoch 379/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 309.6528 - val_loss: 1530.9536\n",
      "Epoch 380/2000\n",
      "3824/3824 [==============================] - 1s 330us/step - loss: 304.0927 - val_loss: 1531.8636\n",
      "Epoch 381/2000\n",
      "3824/3824 [==============================] - 1s 328us/step - loss: 370.3530 - val_loss: 1814.8001\n",
      "Epoch 382/2000\n",
      "3824/3824 [==============================] - 1s 336us/step - loss: 503.6087 - val_loss: 1556.9993\n",
      "Epoch 383/2000\n",
      "3824/3824 [==============================] - 1s 347us/step - loss: 359.1819 - val_loss: 1622.1262\n",
      "Epoch 384/2000\n",
      "3824/3824 [==============================] - 1s 349us/step - loss: 367.7179 - val_loss: 1618.3761\n",
      "Epoch 385/2000\n",
      "3824/3824 [==============================] - 1s 354us/step - loss: 378.9757 - val_loss: 1715.6661\n",
      "Epoch 386/2000\n",
      "3824/3824 [==============================] - 1s 343us/step - loss: 549.0961 - val_loss: 1738.5921\n",
      "Epoch 387/2000\n",
      "3824/3824 [==============================] - 1s 342us/step - loss: 554.8959 - val_loss: 1827.9906\n",
      "Epoch 388/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 401.9881 - val_loss: 1797.9088\n",
      "Epoch 389/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 388.6941 - val_loss: 1794.6280\n",
      "Epoch 390/2000\n",
      "3824/3824 [==============================] - 1s 328us/step - loss: 384.8915 - val_loss: 1775.4040\n",
      "Epoch 391/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 358.7579 - val_loss: 1680.4484\n",
      "Epoch 392/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 347.3643 - val_loss: 1677.9632\n",
      "Epoch 393/2000\n",
      "3824/3824 [==============================] - 1s 338us/step - loss: 337.0990 - val_loss: 1641.9543\n",
      "Epoch 394/2000\n",
      "3824/3824 [==============================] - 1s 340us/step - loss: 339.3136 - val_loss: 1636.1349\n",
      "Epoch 395/2000\n",
      "3824/3824 [==============================] - 1s 359us/step - loss: 328.1731 - val_loss: 1626.4642\n",
      "Epoch 396/2000\n",
      "3824/3824 [==============================] - 1s 346us/step - loss: 323.5041 - val_loss: 1635.4913\n",
      "Epoch 397/2000\n",
      "3824/3824 [==============================] - 1s 349us/step - loss: 336.3778 - val_loss: 1629.8444\n",
      "Epoch 398/2000\n",
      "3824/3824 [==============================] - 1s 342us/step - loss: 328.0720 - val_loss: 1645.1655\n",
      "Epoch 399/2000\n",
      "3824/3824 [==============================] - 1s 343us/step - loss: 354.6648 - val_loss: 1632.2045\n",
      "Epoch 400/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 344.5156 - val_loss: 1606.2858\n",
      "Epoch 401/2000\n",
      "3824/3824 [==============================] - 1s 330us/step - loss: 339.1132 - val_loss: 1617.4970\n",
      "Epoch 402/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 345.8728 - val_loss: 1603.5629\n",
      "Epoch 403/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 325.0792 - val_loss: 1599.5944\n",
      "Epoch 404/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 324.3640 - val_loss: 1600.9153\n",
      "Epoch 405/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 319.6452 - val_loss: 1591.5364\n",
      "Epoch 406/2000\n",
      "3824/3824 [==============================] - 1s 341us/step - loss: 314.7224 - val_loss: 1596.4602\n",
      "Epoch 407/2000\n",
      "3824/3824 [==============================] - 1s 358us/step - loss: 324.6190 - val_loss: 1581.2873\n",
      "Epoch 408/2000\n",
      "3824/3824 [==============================] - 1s 357us/step - loss: 314.1888 - val_loss: 1587.4023\n",
      "Epoch 409/2000\n",
      "3824/3824 [==============================] - 1s 341us/step - loss: 325.8897 - val_loss: 1583.5846\n",
      "Epoch 410/2000\n",
      "3824/3824 [==============================] - 1s 349us/step - loss: 311.4261 - val_loss: 1575.4340\n",
      "Epoch 411/2000\n",
      "3824/3824 [==============================] - 1s 349us/step - loss: 317.1134 - val_loss: 1570.8493\n",
      "Epoch 412/2000\n",
      "3824/3824 [==============================] - 1s 365us/step - loss: 304.5050 - val_loss: 1591.4106\n",
      "Epoch 413/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 323.1655 - val_loss: 1605.4368\n",
      "Epoch 414/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 335.5469 - val_loss: 1594.6113\n",
      "Epoch 415/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 368.5531 - val_loss: 1613.0863\n",
      "Epoch 416/2000\n",
      "3824/3824 [==============================] - 1s 344us/step - loss: 405.9867 - val_loss: 1572.5656\n",
      "Epoch 417/2000\n",
      "3824/3824 [==============================] - 1s 336us/step - loss: 327.3674 - val_loss: 1556.0737\n",
      "Epoch 418/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 329.4157 - val_loss: 1562.5836\n",
      "Epoch 419/2000\n",
      "3824/3824 [==============================] - 1s 349us/step - loss: 301.9912 - val_loss: 1547.9894\n",
      "Epoch 420/2000\n",
      "3824/3824 [==============================] - 1s 345us/step - loss: 310.4894 - val_loss: 1562.8218\n",
      "Epoch 421/2000\n",
      "3824/3824 [==============================] - 1s 346us/step - loss: 310.5112 - val_loss: 1544.3222\n",
      "Epoch 422/2000\n",
      "3824/3824 [==============================] - 1s 345us/step - loss: 310.0143 - val_loss: 1538.3427\n",
      "Epoch 423/2000\n",
      "3824/3824 [==============================] - 1s 340us/step - loss: 300.7204 - val_loss: 1530.2212\n",
      "Epoch 424/2000\n",
      "3824/3824 [==============================] - 1s 346us/step - loss: 298.7812 - val_loss: 1518.5265\n",
      "Epoch 425/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 307.5487 - val_loss: 1519.7963\n",
      "Epoch 426/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 300.4038 - val_loss: 1510.7031\n",
      "Epoch 427/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 308.3725 - val_loss: 1516.8920\n",
      "Epoch 428/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 297.2847 - val_loss: 1511.8186\n",
      "Epoch 429/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 291.4663 - val_loss: 1504.2313\n",
      "Epoch 430/2000\n",
      "3824/3824 [==============================] - 1s 336us/step - loss: 288.2409 - val_loss: 1513.9184\n",
      "Epoch 431/2000\n",
      "3824/3824 [==============================] - 1s 357us/step - loss: 300.4569 - val_loss: 1504.3579\n",
      "Epoch 432/2000\n",
      "3824/3824 [==============================] - 1s 345us/step - loss: 312.8740 - val_loss: 1478.2869\n",
      "Epoch 433/2000\n",
      "3824/3824 [==============================] - 1s 347us/step - loss: 309.6692 - val_loss: 1473.5489\n",
      "Epoch 434/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824/3824 [==============================] - 1s 341us/step - loss: 331.2650 - val_loss: 1480.3990\n",
      "Epoch 435/2000\n",
      "3824/3824 [==============================] - 1s 346us/step - loss: 352.3592 - val_loss: 1508.3742\n",
      "Epoch 436/2000\n",
      "3824/3824 [==============================] - 1s 343us/step - loss: 337.7357 - val_loss: 1485.2960\n",
      "Epoch 437/2000\n",
      "3824/3824 [==============================] - 1s 342us/step - loss: 339.5834 - val_loss: 1526.3485\n",
      "Epoch 438/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 375.6922 - val_loss: 1563.1080\n",
      "Epoch 439/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 392.3151 - val_loss: 1554.1215\n",
      "Epoch 440/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 388.7116 - val_loss: 1476.0640\n",
      "Epoch 441/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 348.0418 - val_loss: 1593.6483\n",
      "Epoch 442/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 436.1344 - val_loss: 1485.3614\n",
      "Epoch 443/2000\n",
      "3824/3824 [==============================] - 1s 328us/step - loss: 351.1989 - val_loss: 1454.9350\n",
      "Epoch 444/2000\n",
      "3824/3824 [==============================] - 1s 355us/step - loss: 314.6847 - val_loss: 1445.8134\n",
      "Epoch 445/2000\n",
      "3824/3824 [==============================] - 1s 346us/step - loss: 297.5561 - val_loss: 1441.7775\n",
      "Epoch 446/2000\n",
      "3824/3824 [==============================] - 1s 347us/step - loss: 290.0593 - val_loss: 1433.1378\n",
      "Epoch 447/2000\n",
      "3824/3824 [==============================] - 1s 351us/step - loss: 281.9581 - val_loss: 1433.1388\n",
      "Epoch 448/2000\n",
      "3824/3824 [==============================] - 1s 350us/step - loss: 281.0664 - val_loss: 1424.5549\n",
      "Epoch 449/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 289.6461 - val_loss: 1422.3924\n",
      "Epoch 450/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 273.9078 - val_loss: 1417.6950\n",
      "Epoch 451/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 274.2768 - val_loss: 1417.4718\n",
      "Epoch 452/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 289.5100 - val_loss: 1419.5157\n",
      "Epoch 453/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 276.7123 - val_loss: 1413.8437\n",
      "Epoch 454/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 279.1093 - val_loss: 1431.4291\n",
      "Epoch 455/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 271.0599 - val_loss: 1411.0760\n",
      "Epoch 456/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 283.4052 - val_loss: 1413.7172\n",
      "Epoch 457/2000\n",
      "3824/3824 [==============================] - 1s 354us/step - loss: 276.0096 - val_loss: 1406.1795\n",
      "Epoch 458/2000\n",
      "3824/3824 [==============================] - 1s 346us/step - loss: 277.6174 - val_loss: 1397.5380\n",
      "Epoch 459/2000\n",
      "3824/3824 [==============================] - 1s 345us/step - loss: 266.9682 - val_loss: 1402.7519\n",
      "Epoch 460/2000\n",
      "3824/3824 [==============================] - 1s 347us/step - loss: 277.2263 - val_loss: 1393.4714\n",
      "Epoch 461/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 267.2348 - val_loss: 1397.4278\n",
      "Epoch 462/2000\n",
      "3824/3824 [==============================] - 1s 336us/step - loss: 270.0570 - val_loss: 1389.9851\n",
      "Epoch 463/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 258.8112 - val_loss: 1384.4701\n",
      "Epoch 464/2000\n",
      "3824/3824 [==============================] - 1s 340us/step - loss: 263.9786 - val_loss: 1384.7804\n",
      "Epoch 465/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 268.5153 - val_loss: 1394.3208\n",
      "Epoch 466/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 269.1623 - val_loss: 1382.0460\n",
      "Epoch 467/2000\n",
      "3824/3824 [==============================] - 1s 330us/step - loss: 290.1596 - val_loss: 1378.9613\n",
      "Epoch 468/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 265.0530 - val_loss: 1380.3847\n",
      "Epoch 469/2000\n",
      "3824/3824 [==============================] - 1s 341us/step - loss: 268.3634 - val_loss: 1375.4314\n",
      "Epoch 470/2000\n",
      "3824/3824 [==============================] - 1s 346us/step - loss: 262.4319 - val_loss: 1387.2122\n",
      "Epoch 471/2000\n",
      "3824/3824 [==============================] - 1s 345us/step - loss: 291.7968 - val_loss: 1388.6462\n",
      "Epoch 472/2000\n",
      "3824/3824 [==============================] - 1s 344us/step - loss: 295.9932 - val_loss: 1438.8615\n",
      "Epoch 473/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 337.1068 - val_loss: 1392.1107\n",
      "Epoch 474/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 367.5423 - val_loss: 1456.7774\n",
      "Epoch 475/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 370.4549 - val_loss: 1408.9476\n",
      "Epoch 476/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 333.0951 - val_loss: 1447.0174\n",
      "Epoch 477/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 357.3966 - val_loss: 1421.1535\n",
      "Epoch 478/2000\n",
      "3824/3824 [==============================] - 1s 336us/step - loss: 326.1319 - val_loss: 1484.8534\n",
      "Epoch 479/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 319.0676 - val_loss: 1452.1267\n",
      "Epoch 480/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 298.5581 - val_loss: 1424.8763\n",
      "Epoch 481/2000\n",
      "3824/3824 [==============================] - 1s 349us/step - loss: 285.3155 - val_loss: 1402.3178\n",
      "Epoch 482/2000\n",
      "3824/3824 [==============================] - 2s 399us/step - loss: 271.4346 - val_loss: 1390.9938\n",
      "Epoch 483/2000\n",
      "3824/3824 [==============================] - 1s 369us/step - loss: 271.0694 - val_loss: 1389.4014\n",
      "Epoch 484/2000\n",
      "3824/3824 [==============================] - 1s 362us/step - loss: 265.4062 - val_loss: 1384.1457\n",
      "Epoch 485/2000\n",
      "3824/3824 [==============================] - 1s 354us/step - loss: 276.1726 - val_loss: 1379.5968\n",
      "Epoch 486/2000\n",
      "3824/3824 [==============================] - 1s 344us/step - loss: 263.7769 - val_loss: 1378.0262\n",
      "Epoch 487/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 265.6716 - val_loss: 1381.4660\n",
      "Epoch 488/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 264.6711 - val_loss: 1366.4595\n",
      "Epoch 489/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 276.7661 - val_loss: 1364.8738\n",
      "Epoch 490/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 257.4055 - val_loss: 1388.7285\n",
      "Epoch 491/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 268.6835 - val_loss: 1360.2239\n",
      "Epoch 492/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 276.5440 - val_loss: 1360.3343\n",
      "Epoch 493/2000\n",
      "3824/3824 [==============================] - 1s 358us/step - loss: 264.3091 - val_loss: 1356.3825\n",
      "Epoch 494/2000\n",
      "3824/3824 [==============================] - 1s 343us/step - loss: 269.0780 - val_loss: 1352.8298\n",
      "Epoch 495/2000\n",
      "3824/3824 [==============================] - 1s 346us/step - loss: 263.3808 - val_loss: 1363.8212\n",
      "Epoch 496/2000\n",
      "3824/3824 [==============================] - 1s 348us/step - loss: 273.0331 - val_loss: 1351.6125\n",
      "Epoch 497/2000\n",
      "3824/3824 [==============================] - 1s 345us/step - loss: 280.0748 - val_loss: 1345.8212\n",
      "Epoch 498/2000\n",
      "3824/3824 [==============================] - 1s 356us/step - loss: 253.9856 - val_loss: 1346.5042\n",
      "Epoch 499/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 258.0340 - val_loss: 1339.8911\n",
      "Epoch 500/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 260.0619 - val_loss: 1345.5292\n",
      "Epoch 501/2000\n",
      "3824/3824 [==============================] - 1s 343us/step - loss: 255.2243 - val_loss: 1334.7563\n",
      "Epoch 502/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 249.5495 - val_loss: 1358.5008\n",
      "Epoch 503/2000\n",
      "3824/3824 [==============================] - 1s 343us/step - loss: 281.9035 - val_loss: 1350.9032\n",
      "Epoch 504/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 289.2868 - val_loss: 1373.9562\n",
      "Epoch 505/2000\n",
      "3824/3824 [==============================] - 1s 351us/step - loss: 329.6042 - val_loss: 1485.8603\n",
      "Epoch 506/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824/3824 [==============================] - 1s 345us/step - loss: 365.3122 - val_loss: 1419.9692\n",
      "Epoch 507/2000\n",
      "3824/3824 [==============================] - 1s 343us/step - loss: 348.3919 - val_loss: 1372.0101\n",
      "Epoch 508/2000\n",
      "3824/3824 [==============================] - 1s 340us/step - loss: 354.9394 - val_loss: 1411.9049\n",
      "Epoch 509/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 362.4072 - val_loss: 1349.3902\n",
      "Epoch 510/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 308.4906 - val_loss: 1355.6525\n",
      "Epoch 511/2000\n",
      "3824/3824 [==============================] - 1s 321us/step - loss: 295.8545 - val_loss: 1329.8422\n",
      "Epoch 512/2000\n",
      "3824/3824 [==============================] - 1s 325us/step - loss: 263.6136 - val_loss: 1333.2782\n",
      "Epoch 513/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 263.4024 - val_loss: 1322.5117\n",
      "Epoch 514/2000\n",
      "3824/3824 [==============================] - 1s 317us/step - loss: 267.8557 - val_loss: 1324.3536\n",
      "Epoch 515/2000\n",
      "3824/3824 [==============================] - 1s 321us/step - loss: 246.5979 - val_loss: 1318.1991\n",
      "Epoch 516/2000\n",
      "3824/3824 [==============================] - 1s 385us/step - loss: 257.0817 - val_loss: 1312.2548\n",
      "Epoch 517/2000\n",
      "3824/3824 [==============================] - 2s 411us/step - loss: 259.3844 - val_loss: 1315.7447\n",
      "Epoch 518/2000\n",
      "3824/3824 [==============================] - 2s 509us/step - loss: 244.0023 - val_loss: 1311.2970\n",
      "Epoch 519/2000\n",
      "3824/3824 [==============================] - 2s 489us/step - loss: 251.4931 - val_loss: 1307.4345\n",
      "Epoch 520/2000\n",
      "3824/3824 [==============================] - 2s 468us/step - loss: 254.4472 - val_loss: 1303.6942\n",
      "Epoch 521/2000\n",
      "3824/3824 [==============================] - 2s 457us/step - loss: 252.2204 - val_loss: 1300.8811\n",
      "Epoch 522/2000\n",
      "3824/3824 [==============================] - 2s 522us/step - loss: 249.7100 - val_loss: 1305.5607\n",
      "Epoch 523/2000\n",
      "3824/3824 [==============================] - 2s 452us/step - loss: 257.2402 - val_loss: 1294.4443\n",
      "Epoch 524/2000\n",
      "3824/3824 [==============================] - 2s 477us/step - loss: 244.7032 - val_loss: 1296.3303\n",
      "Epoch 525/2000\n",
      "3824/3824 [==============================] - 2s 506us/step - loss: 249.9874 - val_loss: 1291.0775\n",
      "Epoch 526/2000\n",
      "3824/3824 [==============================] - 2s 491us/step - loss: 246.4247 - val_loss: 1288.3729\n",
      "Epoch 527/2000\n",
      "3824/3824 [==============================] - 1s 377us/step - loss: 244.7189 - val_loss: 1297.9424\n",
      "Epoch 528/2000\n",
      "3824/3824 [==============================] - 2s 395us/step - loss: 269.2084 - val_loss: 1285.1586\n",
      "Epoch 529/2000\n",
      "3824/3824 [==============================] - 2s 460us/step - loss: 310.3249 - val_loss: 1322.2273\n",
      "Epoch 530/2000\n",
      "3824/3824 [==============================] - 2s 528us/step - loss: 291.0775 - val_loss: 1296.7991\n",
      "Epoch 531/2000\n",
      "3824/3824 [==============================] - 2s 584us/step - loss: 333.3651 - val_loss: 1316.2828\n",
      "Epoch 532/2000\n",
      "3824/3824 [==============================] - 2s 441us/step - loss: 320.7384 - val_loss: 1292.5236\n",
      "Epoch 533/2000\n",
      "3824/3824 [==============================] - 2s 456us/step - loss: 274.7360 - val_loss: 1318.0662\n",
      "Epoch 534/2000\n",
      "3824/3824 [==============================] - 1s 341us/step - loss: 262.4470 - val_loss: 1326.1051\n",
      "Epoch 535/2000\n",
      "3824/3824 [==============================] - 1s 353us/step - loss: 248.5084 - val_loss: 1311.2632\n",
      "Epoch 536/2000\n",
      "3824/3824 [==============================] - 1s 347us/step - loss: 252.3998 - val_loss: 1321.8482\n",
      "Epoch 537/2000\n",
      "3824/3824 [==============================] - 1s 340us/step - loss: 264.8582 - val_loss: 1316.2918\n",
      "Epoch 538/2000\n",
      "3824/3824 [==============================] - 1s 353us/step - loss: 258.8099 - val_loss: 1315.3510\n",
      "Epoch 539/2000\n",
      "3824/3824 [==============================] - 1s 348us/step - loss: 258.0843 - val_loss: 1306.6809\n",
      "Epoch 540/2000\n",
      "3824/3824 [==============================] - 1s 375us/step - loss: 252.8785 - val_loss: 1277.3001\n",
      "Epoch 541/2000\n",
      "3824/3824 [==============================] - 1s 341us/step - loss: 248.7459 - val_loss: 1271.3230\n",
      "Epoch 542/2000\n",
      "3824/3824 [==============================] - 1s 338us/step - loss: 285.7976 - val_loss: 1290.6757\n",
      "Epoch 543/2000\n",
      "3824/3824 [==============================] - 1s 347us/step - loss: 296.9037 - val_loss: 1258.9990\n",
      "Epoch 544/2000\n",
      "3824/3824 [==============================] - 1s 365us/step - loss: 298.7272 - val_loss: 1371.8715\n",
      "Epoch 545/2000\n",
      "3824/3824 [==============================] - 1s 366us/step - loss: 411.8442 - val_loss: 1546.0900\n",
      "Epoch 546/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 501.4642 - val_loss: 1297.8484\n",
      "Epoch 547/2000\n",
      "3824/3824 [==============================] - 1s 350us/step - loss: 293.2540 - val_loss: 1259.1437\n",
      "Epoch 548/2000\n",
      "3824/3824 [==============================] - 1s 357us/step - loss: 255.1268 - val_loss: 1258.9083\n",
      "Epoch 549/2000\n",
      "3824/3824 [==============================] - 1s 348us/step - loss: 246.1326 - val_loss: 1243.9642\n",
      "Epoch 550/2000\n",
      "3824/3824 [==============================] - 1s 365us/step - loss: 251.8730 - val_loss: 1245.5342\n",
      "Epoch 551/2000\n",
      "3824/3824 [==============================] - 1s 346us/step - loss: 246.5637 - val_loss: 1236.7777\n",
      "Epoch 552/2000\n",
      "3824/3824 [==============================] - 1s 345us/step - loss: 245.5057 - val_loss: 1245.4240\n",
      "Epoch 553/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 242.5000 - val_loss: 1237.9257\n",
      "Epoch 554/2000\n",
      "3824/3824 [==============================] - 1s 336us/step - loss: 240.9598 - val_loss: 1245.3356\n",
      "Epoch 555/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 240.9207 - val_loss: 1238.8390\n",
      "Epoch 556/2000\n",
      "3824/3824 [==============================] - 1s 336us/step - loss: 252.0691 - val_loss: 1246.5282\n",
      "Epoch 557/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 243.7974 - val_loss: 1229.2617\n",
      "Epoch 558/2000\n",
      "3824/3824 [==============================] - 1s 336us/step - loss: 241.4357 - val_loss: 1238.9689\n",
      "Epoch 559/2000\n",
      "3824/3824 [==============================] - 1s 340us/step - loss: 239.8894 - val_loss: 1239.4820\n",
      "Epoch 560/2000\n",
      "3824/3824 [==============================] - 1s 354us/step - loss: 235.6702 - val_loss: 1233.3275\n",
      "Epoch 561/2000\n",
      "3824/3824 [==============================] - 1s 345us/step - loss: 240.1726 - val_loss: 1226.6355\n",
      "Epoch 562/2000\n",
      "3824/3824 [==============================] - 1s 353us/step - loss: 237.4765 - val_loss: 1218.5689\n",
      "Epoch 563/2000\n",
      "3824/3824 [==============================] - 1s 348us/step - loss: 250.2588 - val_loss: 1221.3042\n",
      "Epoch 564/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 241.1678 - val_loss: 1250.7034\n",
      "Epoch 565/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 272.0607 - val_loss: 1304.7769\n",
      "Epoch 566/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 354.7361 - val_loss: 1393.1102\n",
      "Epoch 567/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 464.1803 - val_loss: 1315.8568\n",
      "Epoch 568/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 270.7214 - val_loss: 1256.7549\n",
      "Epoch 569/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 262.5492 - val_loss: 1224.9165\n",
      "Epoch 570/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 243.6141 - val_loss: 1213.6227\n",
      "Epoch 571/2000\n",
      "3824/3824 [==============================] - 1s 343us/step - loss: 255.7632 - val_loss: 1206.8950\n",
      "Epoch 572/2000\n",
      "3824/3824 [==============================] - 1s 351us/step - loss: 235.4431 - val_loss: 1203.1991\n",
      "Epoch 573/2000\n",
      "3824/3824 [==============================] - 1s 346us/step - loss: 231.2094 - val_loss: 1203.3842\n",
      "Epoch 574/2000\n",
      "3824/3824 [==============================] - 1s 351us/step - loss: 242.3674 - val_loss: 1207.4181\n",
      "Epoch 575/2000\n",
      "3824/3824 [==============================] - 1s 346us/step - loss: 246.5212 - val_loss: 1211.2885\n",
      "Epoch 576/2000\n",
      "3824/3824 [==============================] - 1s 355us/step - loss: 255.1095 - val_loss: 1204.9795\n",
      "Epoch 577/2000\n",
      "3824/3824 [==============================] - 1s 336us/step - loss: 234.4325 - val_loss: 1204.4329\n",
      "Epoch 578/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824/3824 [==============================] - 1s 328us/step - loss: 239.5207 - val_loss: 1220.7442\n",
      "Epoch 579/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 305.4020 - val_loss: 1362.4240\n",
      "Epoch 580/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 345.8606 - val_loss: 1249.7754\n",
      "Epoch 581/2000\n",
      "3824/3824 [==============================] - 1s 330us/step - loss: 263.6455 - val_loss: 1337.0810\n",
      "Epoch 582/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 285.3863 - val_loss: 1233.5077\n",
      "Epoch 583/2000\n",
      "3824/3824 [==============================] - 1s 330us/step - loss: 283.0105 - val_loss: 1232.7252\n",
      "Epoch 584/2000\n",
      "3824/3824 [==============================] - 1s 336us/step - loss: 259.7234 - val_loss: 1204.0655\n",
      "Epoch 585/2000\n",
      "3824/3824 [==============================] - 1s 343us/step - loss: 249.9150 - val_loss: 1280.0204\n",
      "Epoch 586/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 274.3793 - val_loss: 1215.6835\n",
      "Epoch 587/2000\n",
      "3824/3824 [==============================] - 1s 343us/step - loss: 326.4811 - val_loss: 1280.1602\n",
      "Epoch 588/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 302.8257 - val_loss: 1264.2205\n",
      "Epoch 589/2000\n",
      "3824/3824 [==============================] - 1s 345us/step - loss: 307.4893 - val_loss: 1269.7002\n",
      "Epoch 590/2000\n",
      "3824/3824 [==============================] - 1s 349us/step - loss: 260.1013 - val_loss: 1206.6261\n",
      "Epoch 591/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 249.5581 - val_loss: 1156.6269\n",
      "Epoch 592/2000\n",
      "3824/3824 [==============================] - 1s 365us/step - loss: 247.3165 - val_loss: 1167.5750\n",
      "Epoch 593/2000\n",
      "3824/3824 [==============================] - 1s 356us/step - loss: 234.8200 - val_loss: 1155.2816\n",
      "Epoch 594/2000\n",
      "3824/3824 [==============================] - 1s 342us/step - loss: 240.1893 - val_loss: 1169.1142\n",
      "Epoch 595/2000\n",
      "3824/3824 [==============================] - 1s 343us/step - loss: 229.6049 - val_loss: 1152.7185\n",
      "Epoch 596/2000\n",
      "3824/3824 [==============================] - 1s 354us/step - loss: 237.8760 - val_loss: 1159.9177\n",
      "Epoch 597/2000\n",
      "3824/3824 [==============================] - 1s 362us/step - loss: 237.2084 - val_loss: 1151.9355\n",
      "Epoch 598/2000\n",
      "3824/3824 [==============================] - 1s 351us/step - loss: 232.6088 - val_loss: 1156.0894\n",
      "Epoch 599/2000\n",
      "3824/3824 [==============================] - 1s 349us/step - loss: 240.2652 - val_loss: 1147.1623\n",
      "Epoch 600/2000\n",
      "3824/3824 [==============================] - 1s 344us/step - loss: 232.0146 - val_loss: 1161.7584\n",
      "Epoch 601/2000\n",
      "3824/3824 [==============================] - 1s 351us/step - loss: 231.9790 - val_loss: 1166.2060\n",
      "Epoch 602/2000\n",
      "3824/3824 [==============================] - 1s 347us/step - loss: 272.2191 - val_loss: 1173.1811\n",
      "Epoch 603/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 271.3005 - val_loss: 1274.1019\n",
      "Epoch 604/2000\n",
      "3824/3824 [==============================] - 1s 341us/step - loss: 358.4459 - val_loss: 1199.0540\n",
      "Epoch 605/2000\n",
      "3824/3824 [==============================] - 1s 342us/step - loss: 340.0493 - val_loss: 1324.8843\n",
      "Epoch 606/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 370.7931 - val_loss: 1219.8738\n",
      "Epoch 607/2000\n",
      "3824/3824 [==============================] - 1s 346us/step - loss: 297.1248 - val_loss: 1162.2445\n",
      "Epoch 608/2000\n",
      "3824/3824 [==============================] - 1s 366us/step - loss: 250.3272 - val_loss: 1190.3354\n",
      "Epoch 609/2000\n",
      "3824/3824 [==============================] - 1s 353us/step - loss: 286.5477 - val_loss: 1256.2987\n",
      "Epoch 610/2000\n",
      "3824/3824 [==============================] - 1s 345us/step - loss: 325.3071 - val_loss: 1207.1314\n",
      "Epoch 611/2000\n",
      "3824/3824 [==============================] - 1s 350us/step - loss: 271.7312 - val_loss: 1199.5166\n",
      "Epoch 612/2000\n",
      "3824/3824 [==============================] - 1s 343us/step - loss: 243.0441 - val_loss: 1157.6090\n",
      "Epoch 613/2000\n",
      "3824/3824 [==============================] - 1s 349us/step - loss: 232.8434 - val_loss: 1153.7818\n",
      "Epoch 614/2000\n",
      "3824/3824 [==============================] - 1s 349us/step - loss: 234.9180 - val_loss: 1158.9076\n",
      "Epoch 615/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 224.4997 - val_loss: 1153.7809\n",
      "Epoch 616/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 236.1101 - val_loss: 1188.1311\n",
      "Epoch 617/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 230.2972 - val_loss: 1173.1952\n",
      "Epoch 618/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 257.3168 - val_loss: 1146.0641\n",
      "Epoch 619/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 243.1650 - val_loss: 1157.0989\n",
      "Epoch 620/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 233.3616 - val_loss: 1230.3459\n",
      "Epoch 621/2000\n",
      "3824/3824 [==============================] - 1s 343us/step - loss: 276.0685 - val_loss: 1166.8928\n",
      "Epoch 622/2000\n",
      "3824/3824 [==============================] - 1s 344us/step - loss: 252.1122 - val_loss: 1177.3990\n",
      "Epoch 623/2000\n",
      "3824/3824 [==============================] - 1s 352us/step - loss: 289.1527 - val_loss: 1165.6601\n",
      "Epoch 624/2000\n",
      "3824/3824 [==============================] - 1s 344us/step - loss: 284.7115 - val_loss: 1307.6497\n",
      "Epoch 625/2000\n",
      "3824/3824 [==============================] - 1s 348us/step - loss: 423.1515 - val_loss: 1204.2044\n",
      "Epoch 626/2000\n",
      "3824/3824 [==============================] - 1s 330us/step - loss: 310.6812 - val_loss: 1200.1280\n",
      "Epoch 627/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 331.6897 - val_loss: 1153.0665\n",
      "Epoch 628/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 254.5621 - val_loss: 1152.7515\n",
      "Epoch 629/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 259.7311 - val_loss: 1154.2565\n",
      "Epoch 630/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 248.1282 - val_loss: 1150.6551\n",
      "Epoch 631/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 244.1705 - val_loss: 1137.9411\n",
      "Epoch 632/2000\n",
      "3824/3824 [==============================] - 1s 328us/step - loss: 239.6613 - val_loss: 1148.8774\n",
      "Epoch 633/2000\n",
      "3824/3824 [==============================] - 1s 355us/step - loss: 255.3472 - val_loss: 1206.7776\n",
      "Epoch 634/2000\n",
      "3824/3824 [==============================] - 1s 352us/step - loss: 299.5985 - val_loss: 1192.1787\n",
      "Epoch 635/2000\n",
      "3824/3824 [==============================] - 1s 353us/step - loss: 288.4182 - val_loss: 1229.4287\n",
      "Epoch 636/2000\n",
      "3824/3824 [==============================] - 1s 344us/step - loss: 283.1137 - val_loss: 1218.9616\n",
      "Epoch 637/2000\n",
      "3824/3824 [==============================] - 1s 346us/step - loss: 249.7772 - val_loss: 1173.7336\n",
      "Epoch 638/2000\n",
      "3824/3824 [==============================] - 1s 344us/step - loss: 272.1763 - val_loss: 1170.7429\n",
      "Epoch 639/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 235.8215 - val_loss: 1156.7585\n",
      "Epoch 640/2000\n",
      "3824/3824 [==============================] - 1s 343us/step - loss: 258.4159 - val_loss: 1208.8106\n",
      "Epoch 641/2000\n",
      "3824/3824 [==============================] - 1s 369us/step - loss: 288.4302 - val_loss: 1182.9349\n",
      "Epoch 642/2000\n",
      "3824/3824 [==============================] - 1s 352us/step - loss: 320.3039 - val_loss: 1250.8553\n",
      "Epoch 643/2000\n",
      "3824/3824 [==============================] - 1s 338us/step - loss: 382.2658 - val_loss: 1173.4502\n",
      "Epoch 644/2000\n",
      "3824/3824 [==============================] - 1s 342us/step - loss: 288.6697 - val_loss: 1253.1231\n",
      "Epoch 645/2000\n",
      "3824/3824 [==============================] - 1s 347us/step - loss: 342.3371 - val_loss: 1285.9650\n",
      "Epoch 646/2000\n",
      "3824/3824 [==============================] - 1s 346us/step - loss: 257.0372 - val_loss: 1225.1720\n",
      "Epoch 647/2000\n",
      "3824/3824 [==============================] - 1s 342us/step - loss: 266.1169 - val_loss: 1238.5462\n",
      "Epoch 648/2000\n",
      "3824/3824 [==============================] - 1s 348us/step - loss: 262.5418 - val_loss: 1194.4030\n",
      "Epoch 649/2000\n",
      "3824/3824 [==============================] - 1s 355us/step - loss: 242.7727 - val_loss: 1181.9244\n",
      "Epoch 650/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824/3824 [==============================] - 1s 331us/step - loss: 229.4543 - val_loss: 1180.2288\n",
      "Epoch 651/2000\n",
      "3824/3824 [==============================] - 1s 319us/step - loss: 232.1916 - val_loss: 1167.2105\n",
      "Epoch 652/2000\n",
      "3824/3824 [==============================] - 1s 328us/step - loss: 230.8784 - val_loss: 1159.8698\n",
      "Epoch 653/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 230.1577 - val_loss: 1137.8099\n",
      "Epoch 654/2000\n",
      "3824/3824 [==============================] - 1s 325us/step - loss: 229.8482 - val_loss: 1131.8280\n",
      "Epoch 655/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 223.7781 - val_loss: 1138.4571\n",
      "Epoch 656/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 225.2052 - val_loss: 1143.4838\n",
      "Epoch 657/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 225.4093 - val_loss: 1126.5439\n",
      "Epoch 658/2000\n",
      "3824/3824 [==============================] - 1s 345us/step - loss: 219.8042 - val_loss: 1133.1925\n",
      "Epoch 659/2000\n",
      "3824/3824 [==============================] - 1s 336us/step - loss: 215.6265 - val_loss: 1119.1083\n",
      "Epoch 660/2000\n",
      "3824/3824 [==============================] - 1s 342us/step - loss: 226.1774 - val_loss: 1121.5349\n",
      "Epoch 661/2000\n",
      "3824/3824 [==============================] - 1s 338us/step - loss: 223.8285 - val_loss: 1115.1241\n",
      "Epoch 662/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 222.9700 - val_loss: 1120.3317\n",
      "Epoch 663/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 227.7132 - val_loss: 1128.8873\n",
      "Epoch 664/2000\n",
      "3824/3824 [==============================] - 1s 330us/step - loss: 252.3345 - val_loss: 1141.0053\n",
      "Epoch 665/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 309.8291 - val_loss: 1179.6069\n",
      "Epoch 666/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 262.0422 - val_loss: 1146.8078\n",
      "Epoch 667/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 276.7818 - val_loss: 1115.9316\n",
      "Epoch 668/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 244.3193 - val_loss: 1133.0242\n",
      "Epoch 669/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 233.0225 - val_loss: 1120.3233\n",
      "Epoch 670/2000\n",
      "3824/3824 [==============================] - 1s 338us/step - loss: 216.8215 - val_loss: 1116.7046\n",
      "Epoch 671/2000\n",
      "3824/3824 [==============================] - 1s 344us/step - loss: 224.9831 - val_loss: 1111.4792\n",
      "Epoch 672/2000\n",
      "3824/3824 [==============================] - 1s 347us/step - loss: 220.4317 - val_loss: 1104.5255\n",
      "Epoch 673/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 246.4016 - val_loss: 1121.8419\n",
      "Epoch 674/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 224.0974 - val_loss: 1101.6456\n",
      "Epoch 675/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 238.7350 - val_loss: 1121.7934\n",
      "Epoch 676/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 221.6962 - val_loss: 1103.0008\n",
      "Epoch 677/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 226.4829 - val_loss: 1111.9521\n",
      "Epoch 678/2000\n",
      "3824/3824 [==============================] - 1s 353us/step - loss: 226.0900 - val_loss: 1093.4277\n",
      "Epoch 679/2000\n",
      "3824/3824 [==============================] - 1s 369us/step - loss: 219.8928 - val_loss: 1119.9584\n",
      "Epoch 680/2000\n",
      "3824/3824 [==============================] - 1s 360us/step - loss: 222.2863 - val_loss: 1106.6453\n",
      "Epoch 681/2000\n",
      "3824/3824 [==============================] - 1s 364us/step - loss: 215.9459 - val_loss: 1102.0407\n",
      "Epoch 682/2000\n",
      "3824/3824 [==============================] - 1s 370us/step - loss: 214.9770 - val_loss: 1088.6951\n",
      "Epoch 683/2000\n",
      "3824/3824 [==============================] - 1s 362us/step - loss: 215.3672 - val_loss: 1094.9760\n",
      "Epoch 684/2000\n",
      "3824/3824 [==============================] - 1s 352us/step - loss: 216.9877 - val_loss: 1085.9110\n",
      "Epoch 685/2000\n",
      "3824/3824 [==============================] - 1s 360us/step - loss: 218.3339 - val_loss: 1093.6388\n",
      "Epoch 686/2000\n",
      "3824/3824 [==============================] - 1s 363us/step - loss: 212.3850 - val_loss: 1093.5563\n",
      "Epoch 687/2000\n",
      "3824/3824 [==============================] - 1s 367us/step - loss: 223.5204 - val_loss: 1083.9775\n",
      "Epoch 688/2000\n",
      "3824/3824 [==============================] - 1s 347us/step - loss: 223.8098 - val_loss: 1093.3861\n",
      "Epoch 689/2000\n",
      "3824/3824 [==============================] - 1s 362us/step - loss: 213.8956 - val_loss: 1086.4937\n",
      "Epoch 690/2000\n",
      "3824/3824 [==============================] - 1s 348us/step - loss: 212.9194 - val_loss: 1072.7182\n",
      "Epoch 691/2000\n",
      "3824/3824 [==============================] - 1s 342us/step - loss: 211.8313 - val_loss: 1074.4514\n",
      "Epoch 692/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 210.1196 - val_loss: 1092.0747\n",
      "Epoch 693/2000\n",
      "3824/3824 [==============================] - 1s 330us/step - loss: 212.2863 - val_loss: 1079.7852\n",
      "Epoch 694/2000\n",
      "3824/3824 [==============================] - 1s 352us/step - loss: 212.7545 - val_loss: 1089.3090\n",
      "Epoch 695/2000\n",
      "3824/3824 [==============================] - 1s 345us/step - loss: 213.4477 - val_loss: 1068.6000\n",
      "Epoch 696/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 224.3947 - val_loss: 1093.0245\n",
      "Epoch 697/2000\n",
      "3824/3824 [==============================] - 1s 342us/step - loss: 242.9433 - val_loss: 1101.4172\n",
      "Epoch 698/2000\n",
      "3824/3824 [==============================] - 1s 341us/step - loss: 278.1818 - val_loss: 1071.6673\n",
      "Epoch 699/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 226.1204 - val_loss: 1068.4973\n",
      "Epoch 700/2000\n",
      "3824/3824 [==============================] - 1s 344us/step - loss: 218.7450 - val_loss: 1058.5320\n",
      "Epoch 701/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 221.0845 - val_loss: 1062.3084\n",
      "Epoch 702/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 212.4923 - val_loss: 1048.7076\n",
      "Epoch 703/2000\n",
      "3824/3824 [==============================] - 1s 326us/step - loss: 214.8797 - val_loss: 1055.4102\n",
      "Epoch 704/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 206.6306 - val_loss: 1052.7418\n",
      "Epoch 705/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 208.8378 - val_loss: 1055.2233\n",
      "Epoch 706/2000\n",
      "3824/3824 [==============================] - 1s 338us/step - loss: 222.4914 - val_loss: 1070.9177\n",
      "Epoch 707/2000\n",
      "3824/3824 [==============================] - 1s 349us/step - loss: 229.9393 - val_loss: 1211.8464\n",
      "Epoch 708/2000\n",
      "3824/3824 [==============================] - 1s 344us/step - loss: 366.3587 - val_loss: 1184.8328\n",
      "Epoch 709/2000\n",
      "3824/3824 [==============================] - 1s 355us/step - loss: 336.0340 - val_loss: 1304.2363\n",
      "Epoch 710/2000\n",
      "3824/3824 [==============================] - 1s 352us/step - loss: 416.4719 - val_loss: 1313.6165\n",
      "Epoch 711/2000\n",
      "3824/3824 [==============================] - 1s 343us/step - loss: 367.0520 - val_loss: 1239.3891\n",
      "Epoch 712/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 352.5053 - val_loss: 1186.1473\n",
      "Epoch 713/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 287.3229 - val_loss: 1187.3949\n",
      "Epoch 714/2000\n",
      "3824/3824 [==============================] - 1s 340us/step - loss: 255.6750 - val_loss: 1159.9980\n",
      "Epoch 715/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 230.2814 - val_loss: 1140.0114\n",
      "Epoch 716/2000\n",
      "3824/3824 [==============================] - 1s 330us/step - loss: 221.5603 - val_loss: 1101.9633\n",
      "Epoch 717/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 216.3301 - val_loss: 1068.2377\n",
      "Epoch 718/2000\n",
      "3824/3824 [==============================] - 1s 338us/step - loss: 209.9040 - val_loss: 1051.5814\n",
      "Epoch 719/2000\n",
      "3824/3824 [==============================] - 1s 353us/step - loss: 209.5387 - val_loss: 1058.0042\n",
      "Epoch 720/2000\n",
      "3824/3824 [==============================] - 1s 351us/step - loss: 224.9065 - val_loss: 1040.6131\n",
      "Epoch 721/2000\n",
      "3824/3824 [==============================] - 1s 352us/step - loss: 204.8359 - val_loss: 1047.0598\n",
      "Epoch 722/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824/3824 [==============================] - 1s 343us/step - loss: 201.6818 - val_loss: 1039.2592\n",
      "Epoch 723/2000\n",
      "3824/3824 [==============================] - 1s 341us/step - loss: 210.2595 - val_loss: 1043.6193\n",
      "Epoch 724/2000\n",
      "3824/3824 [==============================] - 1s 325us/step - loss: 210.8069 - val_loss: 1034.7388\n",
      "Epoch 725/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 205.3605 - val_loss: 1042.7867\n",
      "Epoch 726/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 200.2902 - val_loss: 1052.0155\n",
      "Epoch 727/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 209.1259 - val_loss: 1055.2119\n",
      "Epoch 728/2000\n",
      "3824/3824 [==============================] - 1s 328us/step - loss: 207.2134 - val_loss: 1055.2558\n",
      "Epoch 729/2000\n",
      "3824/3824 [==============================] - 1s 325us/step - loss: 201.2875 - val_loss: 1050.8096\n",
      "Epoch 730/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 206.6514 - val_loss: 1036.6375\n",
      "Epoch 731/2000\n",
      "3824/3824 [==============================] - 1s 340us/step - loss: 213.8535 - val_loss: 1040.5849\n",
      "Epoch 732/2000\n",
      "3824/3824 [==============================] - 1s 347us/step - loss: 218.2610 - val_loss: 1046.7606\n",
      "Epoch 733/2000\n",
      "3824/3824 [==============================] - 1s 347us/step - loss: 205.9621 - val_loss: 1034.6864\n",
      "Epoch 734/2000\n",
      "3824/3824 [==============================] - 1s 340us/step - loss: 232.1771 - val_loss: 1062.7265\n",
      "Epoch 735/2000\n",
      "3824/3824 [==============================] - 1s 341us/step - loss: 224.1444 - val_loss: 1068.0272\n",
      "Epoch 736/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 236.1896 - val_loss: 1091.7737\n",
      "Epoch 737/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 339.5690 - val_loss: 1604.0093\n",
      "Epoch 738/2000\n",
      "3824/3824 [==============================] - 1s 325us/step - loss: 345.9354 - val_loss: 1391.2425\n",
      "Epoch 739/2000\n",
      "3824/3824 [==============================] - 1s 328us/step - loss: 263.1314 - val_loss: 1247.7855\n",
      "Epoch 740/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 231.8584 - val_loss: 1174.7239\n",
      "Epoch 741/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 221.0536 - val_loss: 1136.1414\n",
      "Epoch 742/2000\n",
      "3824/3824 [==============================] - 1s 330us/step - loss: 215.3778 - val_loss: 1121.3087\n",
      "Epoch 743/2000\n",
      "3824/3824 [==============================] - 1s 347us/step - loss: 221.8980 - val_loss: 1112.7011\n",
      "Epoch 744/2000\n",
      "3824/3824 [==============================] - 1s 340us/step - loss: 216.8714 - val_loss: 1103.8552\n",
      "Epoch 745/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 210.1385 - val_loss: 1107.4154\n",
      "Epoch 746/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 207.7308 - val_loss: 1102.2941\n",
      "Epoch 747/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 220.1382 - val_loss: 1105.1011\n",
      "Epoch 748/2000\n",
      "3824/3824 [==============================] - 1s 338us/step - loss: 221.1847 - val_loss: 1110.5806\n",
      "Epoch 749/2000\n",
      "3824/3824 [==============================] - 1s 336us/step - loss: 213.7354 - val_loss: 1105.8796\n",
      "Epoch 750/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 203.1487 - val_loss: 1106.4787\n",
      "Epoch 751/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 205.9775 - val_loss: 1104.0054\n",
      "Epoch 752/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 201.5999 - val_loss: 1106.1892\n",
      "Epoch 753/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 209.7100 - val_loss: 1102.0898\n",
      "Epoch 754/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 215.5595 - val_loss: 1100.0619\n",
      "Epoch 755/2000\n",
      "3824/3824 [==============================] - 1s 328us/step - loss: 217.9242 - val_loss: 1091.7641\n",
      "Epoch 756/2000\n",
      "3824/3824 [==============================] - 1s 344us/step - loss: 209.0656 - val_loss: 1105.9224\n",
      "Epoch 757/2000\n",
      "3824/3824 [==============================] - 1s 350us/step - loss: 227.8784 - val_loss: 1101.5947\n",
      "Epoch 758/2000\n",
      "3824/3824 [==============================] - 1s 340us/step - loss: 233.3796 - val_loss: 1122.4320\n",
      "Epoch 759/2000\n",
      "3824/3824 [==============================] - 1s 346us/step - loss: 249.9420 - val_loss: 1121.0499\n",
      "Epoch 760/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 246.8560 - val_loss: 1120.3687\n",
      "Epoch 761/2000\n",
      "3824/3824 [==============================] - 1s 330us/step - loss: 274.3498 - val_loss: 1136.0336\n",
      "Epoch 762/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 258.5642 - val_loss: 1125.8243\n",
      "Epoch 763/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 255.8140 - val_loss: 1098.2559\n",
      "Epoch 764/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 224.9723 - val_loss: 1107.0492\n",
      "Epoch 765/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 205.8636 - val_loss: 1092.0029\n",
      "Epoch 766/2000\n",
      "3824/3824 [==============================] - 1s 325us/step - loss: 212.1082 - val_loss: 1097.5225\n",
      "Epoch 767/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 211.6210 - val_loss: 1120.6112\n",
      "Epoch 768/2000\n",
      "3824/3824 [==============================] - 1s 343us/step - loss: 220.2099 - val_loss: 1101.8742\n",
      "Epoch 769/2000\n",
      "3824/3824 [==============================] - 1s 343us/step - loss: 216.0743 - val_loss: 1086.1377\n",
      "Epoch 770/2000\n",
      "3824/3824 [==============================] - 1s 345us/step - loss: 214.2309 - val_loss: 1097.1382\n",
      "Epoch 771/2000\n",
      "3824/3824 [==============================] - 1s 348us/step - loss: 230.4641 - val_loss: 1060.7860\n",
      "Epoch 772/2000\n",
      "3824/3824 [==============================] - 1s 343us/step - loss: 220.7894 - val_loss: 1062.2145\n",
      "Epoch 773/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 208.8311 - val_loss: 1064.2903\n",
      "Epoch 774/2000\n",
      "3824/3824 [==============================] - 1s 330us/step - loss: 240.0199 - val_loss: 1066.8015\n",
      "Epoch 775/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 272.3517 - val_loss: 1072.8041\n",
      "Epoch 776/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 241.3026 - val_loss: 1070.4696\n",
      "Epoch 777/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 230.5126 - val_loss: 1068.0757\n",
      "Epoch 778/2000\n",
      "3824/3824 [==============================] - 1s 391us/step - loss: 223.5418 - val_loss: 1079.5635\n",
      "Epoch 779/2000\n",
      "3824/3824 [==============================] - 1s 345us/step - loss: 204.5215 - val_loss: 1047.2256\n",
      "Epoch 780/2000\n",
      "3824/3824 [==============================] - 1s 370us/step - loss: 205.5903 - val_loss: 1048.3743\n",
      "Epoch 781/2000\n",
      "3824/3824 [==============================] - 1s 367us/step - loss: 209.1809 - val_loss: 1060.5848\n",
      "Epoch 782/2000\n",
      "3824/3824 [==============================] - 1s 375us/step - loss: 214.0963 - val_loss: 1050.1366\n",
      "Epoch 783/2000\n",
      "3824/3824 [==============================] - 1s 348us/step - loss: 210.0071 - val_loss: 1041.6373\n",
      "Epoch 784/2000\n",
      "3824/3824 [==============================] - 1s 348us/step - loss: 207.3613 - val_loss: 1043.8190\n",
      "Epoch 785/2000\n",
      "3824/3824 [==============================] - 1s 349us/step - loss: 196.1785 - val_loss: 1043.4794\n",
      "Epoch 786/2000\n",
      "3824/3824 [==============================] - 1s 317us/step - loss: 195.7329 - val_loss: 1055.4037\n",
      "Epoch 787/2000\n",
      "3824/3824 [==============================] - 1s 326us/step - loss: 206.2773 - val_loss: 1050.5901\n",
      "Epoch 788/2000\n",
      "3824/3824 [==============================] - 1s 347us/step - loss: 219.7391 - val_loss: 1058.8109\n",
      "Epoch 789/2000\n",
      "3824/3824 [==============================] - 1s 313us/step - loss: 222.8955 - val_loss: 1044.0005\n",
      "Epoch 790/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 215.5808 - val_loss: 1038.5132\n",
      "Epoch 791/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 203.3029 - val_loss: 1048.7825\n",
      "Epoch 792/2000\n",
      "3824/3824 [==============================] - 1s 317us/step - loss: 198.6432 - val_loss: 1049.3441\n",
      "Epoch 793/2000\n",
      "3824/3824 [==============================] - 1s 361us/step - loss: 205.4423 - val_loss: 1046.6449\n",
      "Epoch 794/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824/3824 [==============================] - 1s 341us/step - loss: 216.3988 - val_loss: 1043.2332\n",
      "Epoch 795/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 202.6533 - val_loss: 1080.5808\n",
      "Epoch 796/2000\n",
      "3824/3824 [==============================] - 1s 350us/step - loss: 223.2715 - val_loss: 1050.5652\n",
      "Epoch 797/2000\n",
      "3824/3824 [==============================] - 1s 351us/step - loss: 217.2443 - val_loss: 1095.3407\n",
      "Epoch 798/2000\n",
      "3824/3824 [==============================] - 1s 321us/step - loss: 298.9605 - val_loss: 1064.3481\n",
      "Epoch 799/2000\n",
      "3824/3824 [==============================] - 1s 321us/step - loss: 265.5699 - val_loss: 1109.6077\n",
      "Epoch 800/2000\n",
      "3824/3824 [==============================] - 1s 308us/step - loss: 310.2153 - val_loss: 1061.7016\n",
      "Epoch 801/2000\n",
      "3824/3824 [==============================] - 1s 316us/step - loss: 282.5458 - val_loss: 1040.2831\n",
      "Epoch 802/2000\n",
      "3824/3824 [==============================] - 1s 312us/step - loss: 242.0963 - val_loss: 1065.6987\n",
      "Epoch 803/2000\n",
      "3824/3824 [==============================] - 1s 308us/step - loss: 230.7018 - val_loss: 1057.1103\n",
      "Epoch 804/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 213.9362 - val_loss: 1070.4837\n",
      "Epoch 805/2000\n",
      "3824/3824 [==============================] - 1s 326us/step - loss: 230.8847 - val_loss: 1032.6797\n",
      "Epoch 806/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 204.4749 - val_loss: 1022.5860\n",
      "Epoch 807/2000\n",
      "3824/3824 [==============================] - 1s 330us/step - loss: 200.0043 - val_loss: 1020.1396\n",
      "Epoch 808/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 195.9214 - val_loss: 1018.5488\n",
      "Epoch 809/2000\n",
      "3824/3824 [==============================] - 1s 328us/step - loss: 191.3954 - val_loss: 1021.9654\n",
      "Epoch 810/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 190.0517 - val_loss: 1032.6161\n",
      "Epoch 811/2000\n",
      "3824/3824 [==============================] - 1s 315us/step - loss: 201.1788 - val_loss: 1017.5108\n",
      "Epoch 812/2000\n",
      "3824/3824 [==============================] - 1s 320us/step - loss: 199.7995 - val_loss: 1018.6851\n",
      "Epoch 813/2000\n",
      "3824/3824 [==============================] - 1s 314us/step - loss: 190.2980 - val_loss: 1020.8285\n",
      "Epoch 814/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 203.3556 - val_loss: 1016.7540\n",
      "Epoch 815/2000\n",
      "3824/3824 [==============================] - 1s 318us/step - loss: 192.7801 - val_loss: 1020.9514\n",
      "Epoch 816/2000\n",
      "3824/3824 [==============================] - 1s 312us/step - loss: 187.3879 - val_loss: 1018.2542\n",
      "Epoch 817/2000\n",
      "3824/3824 [==============================] - 1s 319us/step - loss: 197.2492 - val_loss: 1028.1387\n",
      "Epoch 818/2000\n",
      "3824/3824 [==============================] - 1s 312us/step - loss: 205.4647 - val_loss: 1012.7473\n",
      "Epoch 819/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 196.6587 - val_loss: 1027.9995\n",
      "Epoch 820/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 198.6068 - val_loss: 1024.8735\n",
      "Epoch 821/2000\n",
      "3824/3824 [==============================] - 1s 362us/step - loss: 202.7029 - val_loss: 1020.0425\n",
      "Epoch 822/2000\n",
      "3824/3824 [==============================] - 1s 340us/step - loss: 248.5705 - val_loss: 1038.5626\n",
      "Epoch 823/2000\n",
      "3824/3824 [==============================] - 1s 336us/step - loss: 219.5585 - val_loss: 1039.5866\n",
      "Epoch 824/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 212.7692 - val_loss: 1021.6196\n",
      "Epoch 825/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 215.4154 - val_loss: 1039.8967\n",
      "Epoch 826/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 202.5658 - val_loss: 1020.6656\n",
      "Epoch 827/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 199.2361 - val_loss: 1029.1660\n",
      "Epoch 828/2000\n",
      "3824/3824 [==============================] - 1s 338us/step - loss: 191.5043 - val_loss: 1029.0179\n",
      "Epoch 829/2000\n",
      "3824/3824 [==============================] - 1s 328us/step - loss: 193.2193 - val_loss: 1016.0216\n",
      "Epoch 830/2000\n",
      "3824/3824 [==============================] - 1s 330us/step - loss: 198.7403 - val_loss: 1026.0556\n",
      "Epoch 831/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 197.7195 - val_loss: 1013.9484\n",
      "Epoch 832/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 194.9588 - val_loss: 1008.9409\n",
      "Epoch 833/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 186.9364 - val_loss: 1023.5659\n",
      "Epoch 834/2000\n",
      "3824/3824 [==============================] - 1s 330us/step - loss: 196.8466 - val_loss: 1030.5547\n",
      "Epoch 835/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 193.4944 - val_loss: 1008.9236\n",
      "Epoch 836/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 194.8192 - val_loss: 1021.3837\n",
      "Epoch 837/2000\n",
      "3824/3824 [==============================] - 1s 318us/step - loss: 196.9326 - val_loss: 1012.4246\n",
      "Epoch 838/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 192.8198 - val_loss: 1013.7716\n",
      "Epoch 839/2000\n",
      "3824/3824 [==============================] - 1s 330us/step - loss: 191.2942 - val_loss: 1020.4703\n",
      "Epoch 840/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 197.4253 - val_loss: 1015.4469\n",
      "Epoch 841/2000\n",
      "3824/3824 [==============================] - 1s 321us/step - loss: 232.8480 - val_loss: 1058.6217\n",
      "Epoch 842/2000\n",
      "3824/3824 [==============================] - 1s 325us/step - loss: 253.8616 - val_loss: 1220.1609\n",
      "Epoch 843/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 448.0268 - val_loss: 1090.9072\n",
      "Epoch 844/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 299.2509 - val_loss: 1088.8956\n",
      "Epoch 845/2000\n",
      "3824/3824 [==============================] - 1s 358us/step - loss: 310.0372 - val_loss: 1049.0192\n",
      "Epoch 846/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 254.7499 - val_loss: 1105.6593\n",
      "Epoch 847/2000\n",
      "3824/3824 [==============================] - 1s 338us/step - loss: 221.2131 - val_loss: 1072.0435\n",
      "Epoch 848/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 226.2502 - val_loss: 1070.6536\n",
      "Epoch 849/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 209.5450 - val_loss: 1057.6696\n",
      "Epoch 850/2000\n",
      "3824/3824 [==============================] - 1s 318us/step - loss: 225.5419 - val_loss: 1048.7316\n",
      "Epoch 851/2000\n",
      "3824/3824 [==============================] - 1s 326us/step - loss: 221.4884 - val_loss: 1025.4185\n",
      "Epoch 852/2000\n",
      "3824/3824 [==============================] - 1s 325us/step - loss: 202.1836 - val_loss: 1034.5422\n",
      "Epoch 853/2000\n",
      "3824/3824 [==============================] - 1s 317us/step - loss: 204.9906 - val_loss: 1041.0490\n",
      "Epoch 854/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 204.0829 - val_loss: 1027.3149\n",
      "Epoch 855/2000\n",
      "3824/3824 [==============================] - 1s 319us/step - loss: 200.5087 - val_loss: 1033.6352\n",
      "Epoch 856/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 213.1333 - val_loss: 1008.4297\n",
      "Epoch 857/2000\n",
      "3824/3824 [==============================] - 1s 340us/step - loss: 196.1688 - val_loss: 1021.9142\n",
      "Epoch 858/2000\n",
      "3824/3824 [==============================] - 1s 336us/step - loss: 198.6306 - val_loss: 1007.3466\n",
      "Epoch 859/2000\n",
      "3824/3824 [==============================] - 1s 336us/step - loss: 195.3591 - val_loss: 1014.4313\n",
      "Epoch 860/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 200.3020 - val_loss: 1006.3340\n",
      "Epoch 861/2000\n",
      "3824/3824 [==============================] - 1s 343us/step - loss: 193.5116 - val_loss: 1008.4849\n",
      "Epoch 862/2000\n",
      "3824/3824 [==============================] - 1s 341us/step - loss: 199.0941 - val_loss: 1014.8698\n",
      "Epoch 863/2000\n",
      "3824/3824 [==============================] - 1s 363us/step - loss: 199.8998 - val_loss: 1017.5403\n",
      "Epoch 864/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 199.7095 - val_loss: 1002.4886\n",
      "Epoch 865/2000\n",
      "3824/3824 [==============================] - 1s 340us/step - loss: 208.6998 - val_loss: 1014.7958\n",
      "Epoch 866/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824/3824 [==============================] - 1s 327us/step - loss: 192.1301 - val_loss: 997.9784\n",
      "Epoch 867/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 195.2765 - val_loss: 1021.5629\n",
      "Epoch 868/2000\n",
      "3824/3824 [==============================] - 1s 325us/step - loss: 189.1043 - val_loss: 1000.0436\n",
      "Epoch 869/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 195.8366 - val_loss: 1027.5513\n",
      "Epoch 870/2000\n",
      "3824/3824 [==============================] - 1s 346us/step - loss: 199.9248 - val_loss: 1025.3041\n",
      "Epoch 871/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 196.5647 - val_loss: 1004.4461\n",
      "Epoch 872/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 198.3282 - val_loss: 999.4564\n",
      "Epoch 873/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 219.6907 - val_loss: 1028.5001\n",
      "Epoch 874/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 248.0275 - val_loss: 1109.7538\n",
      "Epoch 875/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 297.8742 - val_loss: 1057.6046\n",
      "Epoch 876/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 257.0768 - val_loss: 1044.2451\n",
      "Epoch 877/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 269.2609 - val_loss: 1068.0424\n",
      "Epoch 878/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 229.6696 - val_loss: 1061.9717\n",
      "Epoch 879/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 249.8524 - val_loss: 1107.0526\n",
      "Epoch 880/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 249.7831 - val_loss: 1099.4037\n",
      "Epoch 881/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 277.5677 - val_loss: 1110.1921\n",
      "Epoch 882/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 305.8167 - val_loss: 1060.2718\n",
      "Epoch 883/2000\n",
      "3824/3824 [==============================] - 1s 354us/step - loss: 231.0705 - val_loss: 1070.0858\n",
      "Epoch 884/2000\n",
      "3824/3824 [==============================] - 1s 338us/step - loss: 203.2714 - val_loss: 1050.2167\n",
      "Epoch 885/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 209.9047 - val_loss: 1062.3069\n",
      "Epoch 886/2000\n",
      "3824/3824 [==============================] - 1s 350us/step - loss: 213.2934 - val_loss: 1053.3736\n",
      "Epoch 887/2000\n",
      "3824/3824 [==============================] - 1s 376us/step - loss: 206.2496 - val_loss: 1065.4579\n",
      "Epoch 888/2000\n",
      "3824/3824 [==============================] - 1s 348us/step - loss: 203.9001 - val_loss: 1044.4041\n",
      "Epoch 889/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 204.5363 - val_loss: 1052.0650\n",
      "Epoch 890/2000\n",
      "3824/3824 [==============================] - 1s 381us/step - loss: 205.5016 - val_loss: 1035.0712\n",
      "Epoch 891/2000\n",
      "3824/3824 [==============================] - 2s 403us/step - loss: 196.8497 - val_loss: 1040.7994\n",
      "Epoch 892/2000\n",
      "3824/3824 [==============================] - 1s 369us/step - loss: 195.9394 - val_loss: 1032.7720\n",
      "Epoch 893/2000\n",
      "3824/3824 [==============================] - 2s 403us/step - loss: 189.1543 - val_loss: 1031.3663\n",
      "Epoch 894/2000\n",
      "3824/3824 [==============================] - 2s 489us/step - loss: 197.5749 - val_loss: 1020.0204\n",
      "Epoch 895/2000\n",
      "3824/3824 [==============================] - 2s 444us/step - loss: 203.3963 - val_loss: 1032.4347\n",
      "Epoch 896/2000\n",
      "3824/3824 [==============================] - 2s 396us/step - loss: 196.9874 - val_loss: 1024.8461\n",
      "Epoch 897/2000\n",
      "3824/3824 [==============================] - 1s 389us/step - loss: 198.1339 - val_loss: 1014.0768\n",
      "Epoch 898/2000\n",
      "3824/3824 [==============================] - 1s 390us/step - loss: 193.3706 - val_loss: 1005.4032\n",
      "Epoch 899/2000\n",
      "3824/3824 [==============================] - 1s 340us/step - loss: 205.6679 - val_loss: 1055.8517\n",
      "Epoch 900/2000\n",
      "3824/3824 [==============================] - 1s 346us/step - loss: 228.3892 - val_loss: 1020.8722\n",
      "Epoch 901/2000\n",
      "3824/3824 [==============================] - 1s 326us/step - loss: 202.0199 - val_loss: 1028.7656\n",
      "Epoch 902/2000\n",
      "3824/3824 [==============================] - 1s 328us/step - loss: 206.2727 - val_loss: 1008.9926\n",
      "Epoch 903/2000\n",
      "3824/3824 [==============================] - 1s 273us/step - loss: 201.6115 - val_loss: 996.0659\n",
      "Epoch 904/2000\n",
      "3824/3824 [==============================] - 1s 305us/step - loss: 191.4639 - val_loss: 1005.0231\n",
      "Epoch 905/2000\n",
      "3824/3824 [==============================] - 1s 320us/step - loss: 208.6849 - val_loss: 999.6310\n",
      "Epoch 906/2000\n",
      "3824/3824 [==============================] - 1s 282us/step - loss: 203.3933 - val_loss: 1010.3223\n",
      "Epoch 907/2000\n",
      "3824/3824 [==============================] - 1s 293us/step - loss: 201.0641 - val_loss: 984.3680\n",
      "Epoch 908/2000\n",
      "3824/3824 [==============================] - 1s 279us/step - loss: 188.5895 - val_loss: 967.5263\n",
      "Epoch 909/2000\n",
      "3824/3824 [==============================] - 1s 272us/step - loss: 190.6504 - val_loss: 992.1490\n",
      "Epoch 910/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 199.5413 - val_loss: 1018.6554\n",
      "Epoch 911/2000\n",
      "3824/3824 [==============================] - 1s 264us/step - loss: 196.7273 - val_loss: 1038.8849\n",
      "Epoch 912/2000\n",
      "3824/3824 [==============================] - 1s 277us/step - loss: 220.2508 - val_loss: 1204.4377\n",
      "Epoch 913/2000\n",
      "3824/3824 [==============================] - 1s 257us/step - loss: 219.6149 - val_loss: 1056.8896\n",
      "Epoch 914/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 242.4012 - val_loss: 1051.9325\n",
      "Epoch 915/2000\n",
      "3824/3824 [==============================] - 1s 288us/step - loss: 190.9272 - val_loss: 1018.8831\n",
      "Epoch 916/2000\n",
      "3824/3824 [==============================] - 1s 255us/step - loss: 190.6279 - val_loss: 1051.9227\n",
      "Epoch 917/2000\n",
      "3824/3824 [==============================] - 1s 257us/step - loss: 196.9763 - val_loss: 1032.5074\n",
      "Epoch 918/2000\n",
      "3824/3824 [==============================] - 1s 266us/step - loss: 203.9656 - val_loss: 1013.5084\n",
      "Epoch 919/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 201.8457 - val_loss: 1018.4715\n",
      "Epoch 920/2000\n",
      "3824/3824 [==============================] - 1s 278us/step - loss: 210.8014 - val_loss: 1007.0187\n",
      "Epoch 921/2000\n",
      "3824/3824 [==============================] - 1s 282us/step - loss: 212.6985 - val_loss: 1019.9608\n",
      "Epoch 922/2000\n",
      "3824/3824 [==============================] - 1s 286us/step - loss: 221.5873 - val_loss: 993.9417\n",
      "Epoch 923/2000\n",
      "3824/3824 [==============================] - 1s 278us/step - loss: 207.2864 - val_loss: 1016.2096\n",
      "Epoch 924/2000\n",
      "3824/3824 [==============================] - 1s 277us/step - loss: 201.8642 - val_loss: 993.7146\n",
      "Epoch 925/2000\n",
      "3824/3824 [==============================] - 1s 267us/step - loss: 191.7327 - val_loss: 999.1799\n",
      "Epoch 926/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 187.2576 - val_loss: 1005.6729\n",
      "Epoch 927/2000\n",
      "3824/3824 [==============================] - 1s 261us/step - loss: 189.1102 - val_loss: 1011.7838\n",
      "Epoch 928/2000\n",
      "3824/3824 [==============================] - 1s 258us/step - loss: 191.1460 - val_loss: 1010.7634\n",
      "Epoch 929/2000\n",
      "3824/3824 [==============================] - 1s 263us/step - loss: 185.2149 - val_loss: 1013.9421\n",
      "Epoch 930/2000\n",
      "3824/3824 [==============================] - 1s 259us/step - loss: 201.8467 - val_loss: 1054.4283\n",
      "Epoch 931/2000\n",
      "3824/3824 [==============================] - 1s 260us/step - loss: 277.7462 - val_loss: 1039.7713\n",
      "Epoch 932/2000\n",
      "3824/3824 [==============================] - 1s 263us/step - loss: 268.4152 - val_loss: 1345.0462\n",
      "Epoch 933/2000\n",
      "3824/3824 [==============================] - 1s 259us/step - loss: 392.3668 - val_loss: 1411.4141\n",
      "Epoch 934/2000\n",
      "3824/3824 [==============================] - 1s 258us/step - loss: 319.6838 - val_loss: 1282.9968\n",
      "Epoch 935/2000\n",
      "3824/3824 [==============================] - 1s 278us/step - loss: 368.0451 - val_loss: 1152.2206\n",
      "Epoch 936/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 259.9983 - val_loss: 1116.4284\n",
      "Epoch 937/2000\n",
      "3824/3824 [==============================] - 1s 288us/step - loss: 294.6768 - val_loss: 1081.0101\n",
      "Epoch 938/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824/3824 [==============================] - 1s 281us/step - loss: 223.6489 - val_loss: 1065.5045\n",
      "Epoch 939/2000\n",
      "3824/3824 [==============================] - 1s 282us/step - loss: 267.7802 - val_loss: 1112.9269\n",
      "Epoch 940/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 353.3476 - val_loss: 1188.0599\n",
      "Epoch 941/2000\n",
      "3824/3824 [==============================] - 1s 258us/step - loss: 355.2101 - val_loss: 1076.5891\n",
      "Epoch 942/2000\n",
      "3824/3824 [==============================] - 1s 260us/step - loss: 225.4817 - val_loss: 1086.6147\n",
      "Epoch 943/2000\n",
      "3824/3824 [==============================] - 1s 259us/step - loss: 237.0500 - val_loss: 1062.8601\n",
      "Epoch 944/2000\n",
      "3824/3824 [==============================] - 1s 267us/step - loss: 274.5614 - val_loss: 1105.0701\n",
      "Epoch 945/2000\n",
      "3824/3824 [==============================] - 1s 268us/step - loss: 250.4273 - val_loss: 1041.6276\n",
      "Epoch 946/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 216.9596 - val_loss: 1036.7603\n",
      "Epoch 947/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 214.5718 - val_loss: 997.2482\n",
      "Epoch 948/2000\n",
      "3824/3824 [==============================] - 1s 272us/step - loss: 200.2119 - val_loss: 995.4031\n",
      "Epoch 949/2000\n",
      "3824/3824 [==============================] - 1s 258us/step - loss: 192.2246 - val_loss: 992.2786\n",
      "Epoch 950/2000\n",
      "3824/3824 [==============================] - 1s 260us/step - loss: 198.0361 - val_loss: 962.7542\n",
      "Epoch 951/2000\n",
      "3824/3824 [==============================] - 1s 274us/step - loss: 192.1285 - val_loss: 989.1788\n",
      "Epoch 952/2000\n",
      "3824/3824 [==============================] - 1s 285us/step - loss: 192.2004 - val_loss: 989.2715\n",
      "Epoch 953/2000\n",
      "3824/3824 [==============================] - 1s 278us/step - loss: 196.4411 - val_loss: 1020.0653\n",
      "Epoch 954/2000\n",
      "3824/3824 [==============================] - 1s 288us/step - loss: 218.2527 - val_loss: 986.6474\n",
      "Epoch 955/2000\n",
      "3824/3824 [==============================] - 1s 282us/step - loss: 199.8398 - val_loss: 998.1443\n",
      "Epoch 956/2000\n",
      "3824/3824 [==============================] - 1s 265us/step - loss: 199.0786 - val_loss: 1111.5996\n",
      "Epoch 957/2000\n",
      "3824/3824 [==============================] - 1s 261us/step - loss: 204.9909 - val_loss: 1053.2656\n",
      "Epoch 958/2000\n",
      "3824/3824 [==============================] - 1s 263us/step - loss: 194.9334 - val_loss: 1051.7705\n",
      "Epoch 959/2000\n",
      "3824/3824 [==============================] - 1s 257us/step - loss: 187.3165 - val_loss: 1050.7036\n",
      "Epoch 960/2000\n",
      "3824/3824 [==============================] - 1s 259us/step - loss: 191.3965 - val_loss: 1033.7288\n",
      "Epoch 961/2000\n",
      "3824/3824 [==============================] - 1s 264us/step - loss: 195.8776 - val_loss: 1030.9823\n",
      "Epoch 962/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 211.4839 - val_loss: 1042.9239\n",
      "Epoch 963/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 262.5401 - val_loss: 1025.7685\n",
      "Epoch 964/2000\n",
      "3824/3824 [==============================] - 1s 266us/step - loss: 251.3891 - val_loss: 997.6367\n",
      "Epoch 965/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 240.0552 - val_loss: 1018.9224\n",
      "Epoch 966/2000\n",
      "3824/3824 [==============================] - 1s 275us/step - loss: 213.3704 - val_loss: 988.6707\n",
      "Epoch 967/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 196.6657 - val_loss: 981.2296\n",
      "Epoch 968/2000\n",
      "3824/3824 [==============================] - 1s 282us/step - loss: 192.1576 - val_loss: 974.9229\n",
      "Epoch 969/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 197.9693 - val_loss: 967.7045\n",
      "Epoch 970/2000\n",
      "3824/3824 [==============================] - 1s 352us/step - loss: 211.3972 - val_loss: 1000.5697\n",
      "Epoch 971/2000\n",
      "3824/3824 [==============================] - 2s 396us/step - loss: 191.4028 - val_loss: 989.2188\n",
      "Epoch 972/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 199.2937 - val_loss: 985.8975\n",
      "Epoch 973/2000\n",
      "3824/3824 [==============================] - 1s 341us/step - loss: 193.3350 - val_loss: 991.0891\n",
      "Epoch 974/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 190.1901 - val_loss: 996.9055\n",
      "Epoch 975/2000\n",
      "3824/3824 [==============================] - 1s 267us/step - loss: 192.1457 - val_loss: 982.1125\n",
      "Epoch 976/2000\n",
      "3824/3824 [==============================] - 1s 259us/step - loss: 194.2875 - val_loss: 999.9339\n",
      "Epoch 977/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 201.9506 - val_loss: 988.9877\n",
      "Epoch 978/2000\n",
      "3824/3824 [==============================] - 1s 261us/step - loss: 217.1959 - val_loss: 988.9987\n",
      "Epoch 979/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 200.6832 - val_loss: 971.3162\n",
      "Epoch 980/2000\n",
      "3824/3824 [==============================] - 1s 269us/step - loss: 190.9931 - val_loss: 998.3767\n",
      "Epoch 981/2000\n",
      "3824/3824 [==============================] - 1s 282us/step - loss: 214.7436 - val_loss: 971.0686\n",
      "Epoch 982/2000\n",
      "3824/3824 [==============================] - 1s 290us/step - loss: 216.2834 - val_loss: 1021.4097\n",
      "Epoch 983/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 214.4105 - val_loss: 982.0734\n",
      "Epoch 984/2000\n",
      "3824/3824 [==============================] - 1s 286us/step - loss: 208.0479 - val_loss: 995.9227\n",
      "Epoch 985/2000\n",
      "3824/3824 [==============================] - 1s 266us/step - loss: 199.5750 - val_loss: 964.0639\n",
      "Epoch 986/2000\n",
      "3824/3824 [==============================] - 1s 257us/step - loss: 203.1258 - val_loss: 1051.4031\n",
      "Epoch 987/2000\n",
      "3824/3824 [==============================] - 1s 258us/step - loss: 260.4667 - val_loss: 1240.1504\n",
      "Epoch 988/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 431.2504 - val_loss: 1048.7665\n",
      "Epoch 989/2000\n",
      "3824/3824 [==============================] - 1s 266us/step - loss: 281.7679 - val_loss: 1008.4654\n",
      "Epoch 990/2000\n",
      "3824/3824 [==============================] - 1s 265us/step - loss: 217.0055 - val_loss: 1065.3889\n",
      "Epoch 991/2000\n",
      "3824/3824 [==============================] - 1s 270us/step - loss: 200.0671 - val_loss: 1019.7553\n",
      "Epoch 992/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 208.8186 - val_loss: 990.9206\n",
      "Epoch 993/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 220.7252 - val_loss: 1033.3962\n",
      "Epoch 994/2000\n",
      "3824/3824 [==============================] - 1s 257us/step - loss: 271.1254 - val_loss: 1111.4756\n",
      "Epoch 995/2000\n",
      "3824/3824 [==============================] - 1s 266us/step - loss: 213.9022 - val_loss: 1097.0761\n",
      "Epoch 996/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 210.3781 - val_loss: 1031.0480\n",
      "Epoch 997/2000\n",
      "3824/3824 [==============================] - 1s 289us/step - loss: 201.2031 - val_loss: 1006.8758\n",
      "Epoch 998/2000\n",
      "3824/3824 [==============================] - 1s 282us/step - loss: 196.1486 - val_loss: 1058.9821\n",
      "Epoch 999/2000\n",
      "3824/3824 [==============================] - 1s 274us/step - loss: 195.5410 - val_loss: 988.9353\n",
      "Epoch 1000/2000\n",
      "3824/3824 [==============================] - 1s 266us/step - loss: 196.1774 - val_loss: 1008.0120\n",
      "Epoch 1001/2000\n",
      "3824/3824 [==============================] - 1s 263us/step - loss: 200.2957 - val_loss: 1024.3531\n",
      "Epoch 1002/2000\n",
      "3824/3824 [==============================] - 1s 256us/step - loss: 193.5439 - val_loss: 984.8134\n",
      "Epoch 1003/2000\n",
      "3824/3824 [==============================] - 1s 270us/step - loss: 198.9581 - val_loss: 979.3114\n",
      "Epoch 1004/2000\n",
      "3824/3824 [==============================] - 1s 258us/step - loss: 189.8114 - val_loss: 1009.1309\n",
      "Epoch 1005/2000\n",
      "3824/3824 [==============================] - 1s 258us/step - loss: 211.8943 - val_loss: 1013.6545\n",
      "Epoch 1006/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 246.5632 - val_loss: 1051.4179\n",
      "Epoch 1007/2000\n",
      "3824/3824 [==============================] - 1s 266us/step - loss: 293.2857 - val_loss: 1032.1446\n",
      "Epoch 1008/2000\n",
      "3824/3824 [==============================] - 1s 258us/step - loss: 203.5847 - val_loss: 1030.5565\n",
      "Epoch 1009/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 199.8911 - val_loss: 1000.1809\n",
      "Epoch 1010/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 216.0211 - val_loss: 1017.6018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1011/2000\n",
      "3824/3824 [==============================] - 1s 282us/step - loss: 196.1804 - val_loss: 999.6564\n",
      "Epoch 1012/2000\n",
      "3824/3824 [==============================] - 1s 286us/step - loss: 194.6089 - val_loss: 1013.1516\n",
      "Epoch 1013/2000\n",
      "3824/3824 [==============================] - 1s 272us/step - loss: 195.7814 - val_loss: 1003.2713\n",
      "Epoch 1014/2000\n",
      "3824/3824 [==============================] - 1s 281us/step - loss: 194.2051 - val_loss: 1006.8489\n",
      "Epoch 1015/2000\n",
      "3824/3824 [==============================] - 1s 283us/step - loss: 188.0439 - val_loss: 1010.4592\n",
      "Epoch 1016/2000\n",
      "3824/3824 [==============================] - 1s 264us/step - loss: 187.4635 - val_loss: 979.3049\n",
      "Epoch 1017/2000\n",
      "3824/3824 [==============================] - 1s 260us/step - loss: 193.2094 - val_loss: 997.7741\n",
      "Epoch 1018/2000\n",
      "3824/3824 [==============================] - 1s 263us/step - loss: 183.1517 - val_loss: 985.4949\n",
      "Epoch 1019/2000\n",
      "3824/3824 [==============================] - 1s 266us/step - loss: 183.5743 - val_loss: 993.4187\n",
      "Epoch 1020/2000\n",
      "3824/3824 [==============================] - 1s 263us/step - loss: 188.9642 - val_loss: 999.2068\n",
      "Epoch 1021/2000\n",
      "3824/3824 [==============================] - 1s 265us/step - loss: 188.2195 - val_loss: 997.2584\n",
      "Epoch 1022/2000\n",
      "3824/3824 [==============================] - 1s 266us/step - loss: 195.6998 - val_loss: 1006.7666\n",
      "Epoch 1023/2000\n",
      "3824/3824 [==============================] - 1s 258us/step - loss: 186.2529 - val_loss: 1007.6598\n",
      "Epoch 1024/2000\n",
      "3824/3824 [==============================] - 1s 261us/step - loss: 195.0530 - val_loss: 1013.5352\n",
      "Epoch 1025/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 189.7957 - val_loss: 999.4067\n",
      "Epoch 1026/2000\n",
      "3824/3824 [==============================] - 1s 259us/step - loss: 194.5139 - val_loss: 989.1589\n",
      "Epoch 1027/2000\n",
      "3824/3824 [==============================] - 1s 277us/step - loss: 178.5266 - val_loss: 985.1651\n",
      "Epoch 1028/2000\n",
      "3824/3824 [==============================] - 1s 283us/step - loss: 180.4868 - val_loss: 983.8773\n",
      "Epoch 1029/2000\n",
      "3824/3824 [==============================] - 1s 279us/step - loss: 181.8630 - val_loss: 992.0568\n",
      "Epoch 1030/2000\n",
      "3824/3824 [==============================] - 1s 289us/step - loss: 186.9946 - val_loss: 984.5022\n",
      "Epoch 1031/2000\n",
      "3824/3824 [==============================] - 1s 270us/step - loss: 178.8118 - val_loss: 984.4422\n",
      "Epoch 1032/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 184.8778 - val_loss: 997.7686\n",
      "Epoch 1033/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 184.3164 - val_loss: 977.3520\n",
      "Epoch 1034/2000\n",
      "3824/3824 [==============================] - 1s 258us/step - loss: 180.4197 - val_loss: 960.4504\n",
      "Epoch 1035/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 188.6129 - val_loss: 950.0622\n",
      "Epoch 1036/2000\n",
      "3824/3824 [==============================] - 1s 258us/step - loss: 180.0103 - val_loss: 941.5069\n",
      "Epoch 1037/2000\n",
      "3824/3824 [==============================] - 1s 263us/step - loss: 179.7855 - val_loss: 957.9135\n",
      "Epoch 1038/2000\n",
      "3824/3824 [==============================] - 1s 265us/step - loss: 177.5473 - val_loss: 952.5006\n",
      "Epoch 1039/2000\n",
      "3824/3824 [==============================] - 1s 263us/step - loss: 182.9989 - val_loss: 963.6848\n",
      "Epoch 1040/2000\n",
      "3824/3824 [==============================] - 1s 272us/step - loss: 179.1566 - val_loss: 971.0593\n",
      "Epoch 1041/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 177.5331 - val_loss: 952.0118\n",
      "Epoch 1042/2000\n",
      "3824/3824 [==============================] - 1s 272us/step - loss: 181.5279 - val_loss: 969.7244\n",
      "Epoch 1043/2000\n",
      "3824/3824 [==============================] - 1s 292us/step - loss: 190.0803 - val_loss: 939.1129\n",
      "Epoch 1044/2000\n",
      "3824/3824 [==============================] - 1s 274us/step - loss: 189.2872 - val_loss: 952.4621\n",
      "Epoch 1045/2000\n",
      "3824/3824 [==============================] - 1s 282us/step - loss: 200.8772 - val_loss: 947.5632\n",
      "Epoch 1046/2000\n",
      "3824/3824 [==============================] - 1s 277us/step - loss: 211.3988 - val_loss: 1007.6368\n",
      "Epoch 1047/2000\n",
      "3824/3824 [==============================] - 1s 263us/step - loss: 250.5916 - val_loss: 1036.1109\n",
      "Epoch 1048/2000\n",
      "3824/3824 [==============================] - 1s 270us/step - loss: 280.2540 - val_loss: 1009.0656\n",
      "Epoch 1049/2000\n",
      "3824/3824 [==============================] - 1s 257us/step - loss: 224.9046 - val_loss: 972.8771\n",
      "Epoch 1050/2000\n",
      "3824/3824 [==============================] - 1s 267us/step - loss: 185.8059 - val_loss: 973.5052\n",
      "Epoch 1051/2000\n",
      "3824/3824 [==============================] - 1s 265us/step - loss: 188.8499 - val_loss: 963.5184\n",
      "Epoch 1052/2000\n",
      "3824/3824 [==============================] - 1s 260us/step - loss: 181.5305 - val_loss: 1001.5421\n",
      "Epoch 1053/2000\n",
      "3824/3824 [==============================] - 1s 260us/step - loss: 216.9626 - val_loss: 970.3696\n",
      "Epoch 1054/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 223.0073 - val_loss: 982.3734\n",
      "Epoch 1055/2000\n",
      "3824/3824 [==============================] - 1s 258us/step - loss: 196.6035 - val_loss: 971.1609\n",
      "Epoch 1056/2000\n",
      "3824/3824 [==============================] - 1s 257us/step - loss: 190.6654 - val_loss: 1006.6388\n",
      "Epoch 1057/2000\n",
      "3824/3824 [==============================] - 1s 258us/step - loss: 203.7490 - val_loss: 976.1266\n",
      "Epoch 1058/2000\n",
      "3824/3824 [==============================] - 1s 274us/step - loss: 193.2137 - val_loss: 980.4864\n",
      "Epoch 1059/2000\n",
      "3824/3824 [==============================] - 1s 282us/step - loss: 187.2992 - val_loss: 991.3180\n",
      "Epoch 1060/2000\n",
      "3824/3824 [==============================] - 1s 292us/step - loss: 186.9972 - val_loss: 990.3718\n",
      "Epoch 1061/2000\n",
      "3824/3824 [==============================] - 1s 281us/step - loss: 182.7510 - val_loss: 992.3842\n",
      "Epoch 1062/2000\n",
      "3824/3824 [==============================] - 1s 286us/step - loss: 185.5124 - val_loss: 990.8750\n",
      "Epoch 1063/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 179.8344 - val_loss: 963.3311\n",
      "Epoch 1064/2000\n",
      "3824/3824 [==============================] - 1s 275us/step - loss: 178.1002 - val_loss: 961.6082\n",
      "Epoch 1065/2000\n",
      "3824/3824 [==============================] - 1s 269us/step - loss: 175.6372 - val_loss: 986.5329\n",
      "Epoch 1066/2000\n",
      "3824/3824 [==============================] - 1s 274us/step - loss: 198.5542 - val_loss: 970.5877\n",
      "Epoch 1067/2000\n",
      "3824/3824 [==============================] - 1s 271us/step - loss: 197.2393 - val_loss: 1018.2146\n",
      "Epoch 1068/2000\n",
      "3824/3824 [==============================] - 1s 274us/step - loss: 204.9732 - val_loss: 1014.2850\n",
      "Epoch 1069/2000\n",
      "3824/3824 [==============================] - 1s 270us/step - loss: 199.0260 - val_loss: 1048.7927\n",
      "Epoch 1070/2000\n",
      "3824/3824 [==============================] - 1s 275us/step - loss: 231.1325 - val_loss: 964.6531\n",
      "Epoch 1071/2000\n",
      "3824/3824 [==============================] - 1s 278us/step - loss: 206.2551 - val_loss: 1004.1543\n",
      "Epoch 1072/2000\n",
      "3824/3824 [==============================] - 1s 274us/step - loss: 195.6672 - val_loss: 976.1111\n",
      "Epoch 1073/2000\n",
      "3824/3824 [==============================] - 1s 278us/step - loss: 192.6082 - val_loss: 997.7960\n",
      "Epoch 1074/2000\n",
      "3824/3824 [==============================] - 1s 287us/step - loss: 197.1925 - val_loss: 964.7142\n",
      "Epoch 1075/2000\n",
      "3824/3824 [==============================] - 1s 290us/step - loss: 190.4882 - val_loss: 1008.1019\n",
      "Epoch 1076/2000\n",
      "3824/3824 [==============================] - 1s 290us/step - loss: 181.1168 - val_loss: 971.5155\n",
      "Epoch 1077/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 181.2585 - val_loss: 971.2527\n",
      "Epoch 1078/2000\n",
      "3824/3824 [==============================] - 1s 270us/step - loss: 177.1806 - val_loss: 967.6719\n",
      "Epoch 1079/2000\n",
      "3824/3824 [==============================] - 1s 278us/step - loss: 177.9889 - val_loss: 970.8799\n",
      "Epoch 1080/2000\n",
      "3824/3824 [==============================] - 1s 282us/step - loss: 172.6331 - val_loss: 956.5445\n",
      "Epoch 1081/2000\n",
      "3824/3824 [==============================] - 1s 274us/step - loss: 177.8463 - val_loss: 968.5546\n",
      "Epoch 1082/2000\n",
      "3824/3824 [==============================] - 1s 270us/step - loss: 169.7580 - val_loss: 961.6986\n",
      "Epoch 1083/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824/3824 [==============================] - 1s 269us/step - loss: 183.8347 - val_loss: 966.2307\n",
      "Epoch 1084/2000\n",
      "3824/3824 [==============================] - 1s 271us/step - loss: 184.1014 - val_loss: 973.4203\n",
      "Epoch 1085/2000\n",
      "3824/3824 [==============================] - 1s 256us/step - loss: 178.2320 - val_loss: 938.3380\n",
      "Epoch 1086/2000\n",
      "3824/3824 [==============================] - 1s 258us/step - loss: 174.0420 - val_loss: 945.5846\n",
      "Epoch 1087/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 170.6505 - val_loss: 955.1838\n",
      "Epoch 1088/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 178.0628 - val_loss: 954.9057\n",
      "Epoch 1089/2000\n",
      "3824/3824 [==============================] - 1s 286us/step - loss: 172.9695 - val_loss: 951.9162\n",
      "Epoch 1090/2000\n",
      "3824/3824 [==============================] - 1s 285us/step - loss: 182.4080 - val_loss: 954.6620\n",
      "Epoch 1091/2000\n",
      "3824/3824 [==============================] - 1s 282us/step - loss: 176.4851 - val_loss: 954.3529\n",
      "Epoch 1092/2000\n",
      "3824/3824 [==============================] - 1s 281us/step - loss: 166.1266 - val_loss: 948.6831\n",
      "Epoch 1093/2000\n",
      "3824/3824 [==============================] - 1s 270us/step - loss: 179.3822 - val_loss: 947.0100\n",
      "Epoch 1094/2000\n",
      "3824/3824 [==============================] - 1s 257us/step - loss: 180.3793 - val_loss: 946.0844\n",
      "Epoch 1095/2000\n",
      "3824/3824 [==============================] - 1s 264us/step - loss: 174.1409 - val_loss: 939.7006\n",
      "Epoch 1096/2000\n",
      "3824/3824 [==============================] - 1s 256us/step - loss: 177.0796 - val_loss: 947.2575\n",
      "Epoch 1097/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 179.3010 - val_loss: 957.1249\n",
      "Epoch 1098/2000\n",
      "3824/3824 [==============================] - 1s 265us/step - loss: 184.9215 - val_loss: 940.9328\n",
      "Epoch 1099/2000\n",
      "3824/3824 [==============================] - 1s 258us/step - loss: 181.0811 - val_loss: 945.3625\n",
      "Epoch 1100/2000\n",
      "3824/3824 [==============================] - 1s 266us/step - loss: 176.9480 - val_loss: 946.9343\n",
      "Epoch 1101/2000\n",
      "3824/3824 [==============================] - 1s 260us/step - loss: 171.8353 - val_loss: 977.6790\n",
      "Epoch 1102/2000\n",
      "3824/3824 [==============================] - 1s 264us/step - loss: 180.5969 - val_loss: 989.5841\n",
      "Epoch 1103/2000\n",
      "3824/3824 [==============================] - 1s 257us/step - loss: 176.1790 - val_loss: 952.7368\n",
      "Epoch 1104/2000\n",
      "3824/3824 [==============================] - 1s 279us/step - loss: 174.8670 - val_loss: 934.2019\n",
      "Epoch 1105/2000\n",
      "3824/3824 [==============================] - 1s 281us/step - loss: 171.8972 - val_loss: 938.5086\n",
      "Epoch 1106/2000\n",
      "3824/3824 [==============================] - 1s 278us/step - loss: 176.3845 - val_loss: 978.4010\n",
      "Epoch 1107/2000\n",
      "3824/3824 [==============================] - 1s 291us/step - loss: 188.2379 - val_loss: 953.3369\n",
      "Epoch 1108/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 208.5191 - val_loss: 942.0272\n",
      "Epoch 1109/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 182.3081 - val_loss: 971.0803\n",
      "Epoch 1110/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 217.0946 - val_loss: 1204.6734\n",
      "Epoch 1111/2000\n",
      "3824/3824 [==============================] - 1s 265us/step - loss: 411.1564 - val_loss: 1037.0188\n",
      "Epoch 1112/2000\n",
      "3824/3824 [==============================] - 1s 273us/step - loss: 333.9850 - val_loss: 1024.2523\n",
      "Epoch 1113/2000\n",
      "3824/3824 [==============================] - 1s 271us/step - loss: 269.9478 - val_loss: 1043.4074\n",
      "Epoch 1114/2000\n",
      "3824/3824 [==============================] - 1s 265us/step - loss: 238.6447 - val_loss: 962.0709\n",
      "Epoch 1115/2000\n",
      "3824/3824 [==============================] - 1s 263us/step - loss: 234.8962 - val_loss: 971.5384\n",
      "Epoch 1116/2000\n",
      "3824/3824 [==============================] - 1s 263us/step - loss: 186.7726 - val_loss: 953.8823\n",
      "Epoch 1117/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 193.1936 - val_loss: 987.2474\n",
      "Epoch 1118/2000\n",
      "3824/3824 [==============================] - 1s 271us/step - loss: 201.2432 - val_loss: 947.3444\n",
      "Epoch 1119/2000\n",
      "3824/3824 [==============================] - 1s 270us/step - loss: 189.3016 - val_loss: 942.6723\n",
      "Epoch 1120/2000\n",
      "3824/3824 [==============================] - 1s 283us/step - loss: 189.0851 - val_loss: 931.7790\n",
      "Epoch 1121/2000\n",
      "3824/3824 [==============================] - 1s 289us/step - loss: 186.8478 - val_loss: 949.4239\n",
      "Epoch 1122/2000\n",
      "3824/3824 [==============================] - 1s 283us/step - loss: 176.7336 - val_loss: 930.5395\n",
      "Epoch 1123/2000\n",
      "3824/3824 [==============================] - 1s 283us/step - loss: 181.4097 - val_loss: 925.1964\n",
      "Epoch 1124/2000\n",
      "3824/3824 [==============================] - 1s 268us/step - loss: 172.0837 - val_loss: 923.1768\n",
      "Epoch 1125/2000\n",
      "3824/3824 [==============================] - 1s 256us/step - loss: 170.7739 - val_loss: 926.4188\n",
      "Epoch 1126/2000\n",
      "3824/3824 [==============================] - 1s 263us/step - loss: 180.4241 - val_loss: 933.6103\n",
      "Epoch 1127/2000\n",
      "3824/3824 [==============================] - 1s 270us/step - loss: 181.7231 - val_loss: 962.2539\n",
      "Epoch 1128/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 200.3980 - val_loss: 972.9080\n",
      "Epoch 1129/2000\n",
      "3824/3824 [==============================] - 1s 265us/step - loss: 182.7325 - val_loss: 978.0801\n",
      "Epoch 1130/2000\n",
      "3824/3824 [==============================] - 1s 260us/step - loss: 168.6042 - val_loss: 984.8043\n",
      "Epoch 1131/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 173.4689 - val_loss: 960.1892\n",
      "Epoch 1132/2000\n",
      "3824/3824 [==============================] - 1s 252us/step - loss: 199.6587 - val_loss: 997.9821\n",
      "Epoch 1133/2000\n",
      "3824/3824 [==============================] - 1s 267us/step - loss: 171.0336 - val_loss: 986.7257\n",
      "Epoch 1134/2000\n",
      "3824/3824 [==============================] - 1s 255us/step - loss: 173.3862 - val_loss: 988.6662\n",
      "Epoch 1135/2000\n",
      "3824/3824 [==============================] - 1s 278us/step - loss: 172.7530 - val_loss: 986.3515\n",
      "Epoch 1136/2000\n",
      "3824/3824 [==============================] - 1s 302us/step - loss: 169.1433 - val_loss: 998.4301\n",
      "Epoch 1137/2000\n",
      "3824/3824 [==============================] - 1s 299us/step - loss: 169.2788 - val_loss: 994.8243\n",
      "Epoch 1138/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 174.0238 - val_loss: 991.1113\n",
      "Epoch 1139/2000\n",
      "3824/3824 [==============================] - 1s 307us/step - loss: 185.6211 - val_loss: 1031.2676\n",
      "Epoch 1140/2000\n",
      "3824/3824 [==============================] - 1s 274us/step - loss: 268.6665 - val_loss: 1255.3583\n",
      "Epoch 1141/2000\n",
      "3824/3824 [==============================] - 1s 260us/step - loss: 354.0142 - val_loss: 1076.2951\n",
      "Epoch 1142/2000\n",
      "3824/3824 [==============================] - 1s 264us/step - loss: 239.5897 - val_loss: 983.8099\n",
      "Epoch 1143/2000\n",
      "3824/3824 [==============================] - 1s 260us/step - loss: 220.3766 - val_loss: 1037.7187\n",
      "Epoch 1144/2000\n",
      "3824/3824 [==============================] - 1s 263us/step - loss: 216.2392 - val_loss: 981.5075\n",
      "Epoch 1145/2000\n",
      "3824/3824 [==============================] - 1s 258us/step - loss: 186.6625 - val_loss: 982.7104\n",
      "Epoch 1146/2000\n",
      "3824/3824 [==============================] - 1s 268us/step - loss: 191.3468 - val_loss: 971.2096\n",
      "Epoch 1147/2000\n",
      "3824/3824 [==============================] - 1s 284us/step - loss: 180.1341 - val_loss: 969.1411\n",
      "Epoch 1148/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 177.1074 - val_loss: 979.4306\n",
      "Epoch 1149/2000\n",
      "3824/3824 [==============================] - 1s 257us/step - loss: 199.1153 - val_loss: 1042.7379\n",
      "Epoch 1150/2000\n",
      "3824/3824 [==============================] - 1s 266us/step - loss: 189.7465 - val_loss: 982.0655\n",
      "Epoch 1151/2000\n",
      "3824/3824 [==============================] - 1s 292us/step - loss: 186.2371 - val_loss: 965.3879\n",
      "Epoch 1152/2000\n",
      "3824/3824 [==============================] - 1s 277us/step - loss: 177.8338 - val_loss: 965.6313\n",
      "Epoch 1153/2000\n",
      "3824/3824 [==============================] - 1s 278us/step - loss: 180.0317 - val_loss: 982.9719\n",
      "Epoch 1154/2000\n",
      "3824/3824 [==============================] - 1s 282us/step - loss: 174.4625 - val_loss: 970.8836\n",
      "Epoch 1155/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824/3824 [==============================] - 1s 258us/step - loss: 177.7015 - val_loss: 986.7390\n",
      "Epoch 1156/2000\n",
      "3824/3824 [==============================] - 1s 270us/step - loss: 173.3914 - val_loss: 998.0633\n",
      "Epoch 1157/2000\n",
      "3824/3824 [==============================] - 1s 262us/step - loss: 172.3069 - val_loss: 998.1123\n",
      "Epoch 1158/2000\n",
      "3824/3824 [==============================] - 1s 258us/step - loss: 170.1330 - val_loss: 998.2786\n",
      "Epoch 1159/2000\n",
      "3824/3824 [==============================] - 1s 266us/step - loss: 171.9976 - val_loss: 1006.6606\n",
      "Epoch 1160/2000\n",
      "3824/3824 [==============================] - 1s 258us/step - loss: 169.8132 - val_loss: 1000.7824\n",
      "Epoch 1161/2000\n",
      "3824/3824 [==============================] - 1s 288us/step - loss: 172.5472 - val_loss: 995.4586\n",
      "Epoch 1162/2000\n",
      "3824/3824 [==============================] - 1s 328us/step - loss: 173.4688 - val_loss: 977.8797\n",
      "Epoch 1163/2000\n",
      "3824/3824 [==============================] - 1s 379us/step - loss: 182.8095 - val_loss: 991.4090\n",
      "Epoch 1164/2000\n",
      "3824/3824 [==============================] - 1s 382us/step - loss: 175.6186 - val_loss: 978.7669\n",
      "Epoch 1165/2000\n",
      "3824/3824 [==============================] - 1s 367us/step - loss: 165.2283 - val_loss: 982.1667\n",
      "Epoch 1166/2000\n",
      "3824/3824 [==============================] - 1s 345us/step - loss: 178.3274 - val_loss: 987.9487\n",
      "Epoch 1167/2000\n",
      "3824/3824 [==============================] - 1s 328us/step - loss: 175.5551 - val_loss: 969.6083\n",
      "Epoch 1168/2000\n",
      "3824/3824 [==============================] - 1s 319us/step - loss: 187.6276 - val_loss: 1002.0450\n",
      "Epoch 1169/2000\n",
      "3824/3824 [==============================] - 1s 321us/step - loss: 199.0931 - val_loss: 1006.9843\n",
      "Epoch 1170/2000\n",
      "3824/3824 [==============================] - 1s 302us/step - loss: 240.0302 - val_loss: 1021.8871\n",
      "Epoch 1171/2000\n",
      "3824/3824 [==============================] - 1s 304us/step - loss: 233.4098 - val_loss: 994.8066\n",
      "Epoch 1172/2000\n",
      "3824/3824 [==============================] - 1s 298us/step - loss: 178.2994 - val_loss: 1007.4457\n",
      "Epoch 1173/2000\n",
      "3824/3824 [==============================] - 1s 312us/step - loss: 181.6757 - val_loss: 985.1202\n",
      "Epoch 1174/2000\n",
      "3824/3824 [==============================] - ETA: 0s - loss: 175.955 - 1s 309us/step - loss: 170.4345 - val_loss: 973.2141\n",
      "Epoch 1175/2000\n",
      "3824/3824 [==============================] - 1s 307us/step - loss: 180.8149 - val_loss: 998.8072\n",
      "Epoch 1176/2000\n",
      "3824/3824 [==============================] - 1s 310us/step - loss: 176.1182 - val_loss: 989.1997\n",
      "Epoch 1177/2000\n",
      "3824/3824 [==============================] - 1s 305us/step - loss: 169.9629 - val_loss: 989.9663\n",
      "Epoch 1178/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 170.8198 - val_loss: 962.9833\n",
      "Epoch 1179/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 167.5506 - val_loss: 959.6014\n",
      "Epoch 1180/2000\n",
      "3824/3824 [==============================] - 1s 330us/step - loss: 176.7331 - val_loss: 985.3440\n",
      "Epoch 1181/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 171.3270 - val_loss: 978.2030\n",
      "Epoch 1182/2000\n",
      "3824/3824 [==============================] - 1s 325us/step - loss: 178.4470 - val_loss: 964.0432\n",
      "Epoch 1183/2000\n",
      "3824/3824 [==============================] - 1s 309us/step - loss: 172.2860 - val_loss: 991.4533\n",
      "Epoch 1184/2000\n",
      "3824/3824 [==============================] - 1s 308us/step - loss: 171.4955 - val_loss: 991.0749\n",
      "Epoch 1185/2000\n",
      "3824/3824 [==============================] - 1s 310us/step - loss: 167.8162 - val_loss: 987.5097\n",
      "Epoch 1186/2000\n",
      "3824/3824 [==============================] - 1s 312us/step - loss: 167.4110 - val_loss: 998.2156\n",
      "Epoch 1187/2000\n",
      "3824/3824 [==============================] - 1s 307us/step - loss: 177.5374 - val_loss: 972.2487\n",
      "Epoch 1188/2000\n",
      "3824/3824 [==============================] - 1s 310us/step - loss: 170.9944 - val_loss: 976.4436\n",
      "Epoch 1189/2000\n",
      "3824/3824 [==============================] - 1s 307us/step - loss: 178.8552 - val_loss: 983.4622\n",
      "Epoch 1190/2000\n",
      "3824/3824 [==============================] - 1s 320us/step - loss: 176.9034 - val_loss: 1000.6133\n",
      "Epoch 1191/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 191.3686 - val_loss: 997.2354\n",
      "Epoch 1192/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 218.9929 - val_loss: 1058.7899\n",
      "Epoch 1193/2000\n",
      "3824/3824 [==============================] - 1s 326us/step - loss: 267.5749 - val_loss: 1057.0889\n",
      "Epoch 1194/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 221.1499 - val_loss: 1008.9552\n",
      "Epoch 1195/2000\n",
      "3824/3824 [==============================] - 1s 328us/step - loss: 186.0928 - val_loss: 1035.3810\n",
      "Epoch 1196/2000\n",
      "3824/3824 [==============================] - 1s 311us/step - loss: 197.2470 - val_loss: 992.2669\n",
      "Epoch 1197/2000\n",
      "3824/3824 [==============================] - 1s 309us/step - loss: 204.5500 - val_loss: 987.3520\n",
      "Epoch 1198/2000\n",
      "3824/3824 [==============================] - 1s 317us/step - loss: 173.3888 - val_loss: 994.0096\n",
      "Epoch 1199/2000\n",
      "3824/3824 [==============================] - 1s 312us/step - loss: 179.6168 - val_loss: 969.9738\n",
      "Epoch 1200/2000\n",
      "3824/3824 [==============================] - 1s 311us/step - loss: 181.7723 - val_loss: 978.6178\n",
      "Epoch 1201/2000\n",
      "3824/3824 [==============================] - 1s 307us/step - loss: 175.2482 - val_loss: 971.8404\n",
      "Epoch 1202/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 180.6206 - val_loss: 987.4611\n",
      "Epoch 1203/2000\n",
      "3824/3824 [==============================] - 1s 313us/step - loss: 174.5768 - val_loss: 977.1202\n",
      "Epoch 1204/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 170.1540 - val_loss: 960.8335\n",
      "Epoch 1205/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 174.8922 - val_loss: 985.6268\n",
      "Epoch 1206/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 166.1563 - val_loss: 976.4950\n",
      "Epoch 1207/2000\n",
      "3824/3824 [==============================] - 1s 330us/step - loss: 163.6126 - val_loss: 996.0898\n",
      "Epoch 1208/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 173.2342 - val_loss: 993.2455\n",
      "Epoch 1209/2000\n",
      "3824/3824 [==============================] - 1s 307us/step - loss: 181.9755 - val_loss: 959.3766\n",
      "Epoch 1210/2000\n",
      "3824/3824 [==============================] - 1s 312us/step - loss: 170.1796 - val_loss: 955.9749\n",
      "Epoch 1211/2000\n",
      "3824/3824 [==============================] - 1s 314us/step - loss: 172.6638 - val_loss: 946.5153\n",
      "Epoch 1212/2000\n",
      "3824/3824 [==============================] - 1s 305us/step - loss: 165.6138 - val_loss: 946.8201\n",
      "Epoch 1213/2000\n",
      "3824/3824 [==============================] - 1s 312us/step - loss: 165.2447 - val_loss: 951.5182\n",
      "Epoch 1214/2000\n",
      "3824/3824 [==============================] - 1s 317us/step - loss: 165.6931 - val_loss: 945.6466\n",
      "Epoch 1215/2000\n",
      "3824/3824 [==============================] - 1s 319us/step - loss: 165.1106 - val_loss: 951.5452\n",
      "Epoch 1216/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 167.8964 - val_loss: 976.0872\n",
      "Epoch 1217/2000\n",
      "3824/3824 [==============================] - 1s 336us/step - loss: 169.6237 - val_loss: 959.0698\n",
      "Epoch 1218/2000\n",
      "3824/3824 [==============================] - 1s 347us/step - loss: 170.7838 - val_loss: 999.5254\n",
      "Epoch 1219/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 173.3788 - val_loss: 995.0251\n",
      "Epoch 1220/2000\n",
      "3824/3824 [==============================] - 1s 330us/step - loss: 174.3363 - val_loss: 990.7999\n",
      "Epoch 1221/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 176.1434 - val_loss: 975.9721\n",
      "Epoch 1222/2000\n",
      "3824/3824 [==============================] - 1s 318us/step - loss: 222.3638 - val_loss: 1003.7190\n",
      "Epoch 1223/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 208.4179 - val_loss: 959.0812\n",
      "Epoch 1224/2000\n",
      "3824/3824 [==============================] - 1s 313us/step - loss: 213.6043 - val_loss: 1062.4531\n",
      "Epoch 1225/2000\n",
      "3824/3824 [==============================] - 1s 314us/step - loss: 304.3928 - val_loss: 980.5012\n",
      "Epoch 1226/2000\n",
      "3824/3824 [==============================] - 1s 311us/step - loss: 236.0406 - val_loss: 976.5190\n",
      "Epoch 1227/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824/3824 [==============================] - 1s 306us/step - loss: 185.2819 - val_loss: 980.4993\n",
      "Epoch 1228/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 169.5397 - val_loss: 981.1854\n",
      "Epoch 1229/2000\n",
      "3824/3824 [==============================] - 1s 296us/step - loss: 180.3573 - val_loss: 983.6258\n",
      "Epoch 1230/2000\n",
      "3824/3824 [==============================] - 1s 315us/step - loss: 165.2895 - val_loss: 979.4782\n",
      "Epoch 1231/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 169.2252 - val_loss: 987.3117\n",
      "Epoch 1232/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 174.5658 - val_loss: 940.4216\n",
      "Epoch 1233/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 189.2346 - val_loss: 1014.0097\n",
      "Epoch 1234/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 213.9318 - val_loss: 1054.9832\n",
      "Epoch 1235/2000\n",
      "3824/3824 [==============================] - 1s 299us/step - loss: 208.8737 - val_loss: 1022.6421\n",
      "Epoch 1236/2000\n",
      "3824/3824 [==============================] - 1s 309us/step - loss: 177.1134 - val_loss: 974.8148\n",
      "Epoch 1237/2000\n",
      "3824/3824 [==============================] - 1s 298us/step - loss: 178.1021 - val_loss: 992.6542\n",
      "Epoch 1238/2000\n",
      "3824/3824 [==============================] - 1s 317us/step - loss: 171.4520 - val_loss: 982.3978\n",
      "Epoch 1239/2000\n",
      "3824/3824 [==============================] - 1s 316us/step - loss: 187.0054 - val_loss: 990.3723\n",
      "Epoch 1240/2000\n",
      "3824/3824 [==============================] - 1s 306us/step - loss: 165.9643 - val_loss: 994.7466\n",
      "Epoch 1241/2000\n",
      "3824/3824 [==============================] - 1s 304us/step - loss: 163.0452 - val_loss: 955.6828\n",
      "Epoch 1242/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 166.7193 - val_loss: 991.3690\n",
      "Epoch 1243/2000\n",
      "3824/3824 [==============================] - 1s 313us/step - loss: 174.2463 - val_loss: 975.5035\n",
      "Epoch 1244/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 169.9390 - val_loss: 972.3238\n",
      "Epoch 1245/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 163.2131 - val_loss: 951.6889\n",
      "Epoch 1246/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 165.6944 - val_loss: 970.9459\n",
      "Epoch 1247/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 171.1638 - val_loss: 957.3115\n",
      "Epoch 1248/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 164.8919 - val_loss: 944.3621\n",
      "Epoch 1249/2000\n",
      "3824/3824 [==============================] - 1s 311us/step - loss: 162.0914 - val_loss: 959.1218\n",
      "Epoch 1250/2000\n",
      "3824/3824 [==============================] - 1s 313us/step - loss: 177.4331 - val_loss: 988.7122\n",
      "Epoch 1251/2000\n",
      "3824/3824 [==============================] - 1s 320us/step - loss: 170.3957 - val_loss: 984.0062\n",
      "Epoch 1252/2000\n",
      "3824/3824 [==============================] - 1s 319us/step - loss: 165.4677 - val_loss: 987.8358\n",
      "Epoch 1253/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 177.0203 - val_loss: 963.1725\n",
      "Epoch 1254/2000\n",
      "3824/3824 [==============================] - 1s 316us/step - loss: 179.5728 - val_loss: 984.9142\n",
      "Epoch 1255/2000\n",
      "3824/3824 [==============================] - 1s 308us/step - loss: 202.3170 - val_loss: 988.8849\n",
      "Epoch 1256/2000\n",
      "3824/3824 [==============================] - 1s 317us/step - loss: 199.0570 - val_loss: 995.1245\n",
      "Epoch 1257/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 203.1715 - val_loss: 978.1577\n",
      "Epoch 1258/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 181.7392 - val_loss: 1028.6163\n",
      "Epoch 1259/2000\n",
      "3824/3824 [==============================] - 1s 319us/step - loss: 183.1807 - val_loss: 968.5129\n",
      "Epoch 1260/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 191.3568 - val_loss: 1017.6458\n",
      "Epoch 1261/2000\n",
      "3824/3824 [==============================] - 1s 320us/step - loss: 210.1763 - val_loss: 1063.0866\n",
      "Epoch 1262/2000\n",
      "3824/3824 [==============================] - 1s 304us/step - loss: 295.6292 - val_loss: 1026.6789\n",
      "Epoch 1263/2000\n",
      "3824/3824 [==============================] - 1s 302us/step - loss: 265.8988 - val_loss: 995.1432\n",
      "Epoch 1264/2000\n",
      "3824/3824 [==============================] - 1s 315us/step - loss: 255.2329 - val_loss: 1045.9272\n",
      "Epoch 1265/2000\n",
      "3824/3824 [==============================] - 1s 298us/step - loss: 302.4115 - val_loss: 1113.9753\n",
      "Epoch 1266/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 360.3147 - val_loss: 1061.4412\n",
      "Epoch 1267/2000\n",
      "3824/3824 [==============================] - 1s 301us/step - loss: 270.2818 - val_loss: 948.6689\n",
      "Epoch 1268/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 186.8925 - val_loss: 965.9628\n",
      "Epoch 1269/2000\n",
      "3824/3824 [==============================] - 1s 305us/step - loss: 165.8378 - val_loss: 950.6421\n",
      "Epoch 1270/2000\n",
      "3824/3824 [==============================] - 1s 330us/step - loss: 183.3215 - val_loss: 969.4216\n",
      "Epoch 1271/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 172.6098 - val_loss: 968.9642\n",
      "Epoch 1272/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 178.0520 - val_loss: 965.0063\n",
      "Epoch 1273/2000\n",
      "3824/3824 [==============================] - 1s 336us/step - loss: 226.2518 - val_loss: 1022.3350\n",
      "Epoch 1274/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 332.8266 - val_loss: 996.8788\n",
      "Epoch 1275/2000\n",
      "3824/3824 [==============================] - 1s 311us/step - loss: 230.6197 - val_loss: 997.5660\n",
      "Epoch 1276/2000\n",
      "3824/3824 [==============================] - 1s 302us/step - loss: 201.3520 - val_loss: 994.7791\n",
      "Epoch 1277/2000\n",
      "3824/3824 [==============================] - 1s 307us/step - loss: 174.3408 - val_loss: 994.6902\n",
      "Epoch 1278/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 180.4193 - val_loss: 986.6069\n",
      "Epoch 1279/2000\n",
      "3824/3824 [==============================] - 1s 307us/step - loss: 169.8128 - val_loss: 995.7193\n",
      "Epoch 1280/2000\n",
      "3824/3824 [==============================] - 1s 314us/step - loss: 180.6660 - val_loss: 1002.6406\n",
      "Epoch 1281/2000\n",
      "3824/3824 [==============================] - 1s 308us/step - loss: 176.0791 - val_loss: 993.2443\n",
      "Epoch 1282/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 173.1202 - val_loss: 987.6178\n",
      "Epoch 1283/2000\n",
      "3824/3824 [==============================] - 1s 325us/step - loss: 174.6476 - val_loss: 998.7916\n",
      "Epoch 1284/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 164.3426 - val_loss: 992.0487\n",
      "Epoch 1285/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 165.0080 - val_loss: 988.9839\n",
      "Epoch 1286/2000\n",
      "3824/3824 [==============================] - 1s 330us/step - loss: 174.7712 - val_loss: 991.0139\n",
      "Epoch 1287/2000\n",
      "3824/3824 [==============================] - 1s 321us/step - loss: 160.6270 - val_loss: 991.6427\n",
      "Epoch 1288/2000\n",
      "3824/3824 [==============================] - 1s 305us/step - loss: 157.9504 - val_loss: 983.4795\n",
      "Epoch 1289/2000\n",
      "3824/3824 [==============================] - 1s 316us/step - loss: 162.5795 - val_loss: 979.1780\n",
      "Epoch 1290/2000\n",
      "3824/3824 [==============================] - 1s 302us/step - loss: 165.5385 - val_loss: 983.7622\n",
      "Epoch 1291/2000\n",
      "3824/3824 [==============================] - 1s 312us/step - loss: 171.3059 - val_loss: 987.5221\n",
      "Epoch 1292/2000\n",
      "3824/3824 [==============================] - 1s 307us/step - loss: 166.1994 - val_loss: 988.7347\n",
      "Epoch 1293/2000\n",
      "3824/3824 [==============================] - 1s 316us/step - loss: 167.3848 - val_loss: 984.0177\n",
      "Epoch 1294/2000\n",
      "3824/3824 [==============================] - 1s 316us/step - loss: 172.5782 - val_loss: 967.2301\n",
      "Epoch 1295/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 171.4727 - val_loss: 954.0496\n",
      "Epoch 1296/2000\n",
      "3824/3824 [==============================] - 1s 314us/step - loss: 159.2287 - val_loss: 966.1047\n",
      "Epoch 1297/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 162.6253 - val_loss: 961.0418\n",
      "Epoch 1298/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 162.5253 - val_loss: 948.3006\n",
      "Epoch 1299/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824/3824 [==============================] - 1s 320us/step - loss: 165.8418 - val_loss: 948.7837\n",
      "Epoch 1300/2000\n",
      "3824/3824 [==============================] - 1s 316us/step - loss: 164.3760 - val_loss: 952.5766\n",
      "Epoch 1301/2000\n",
      "3824/3824 [==============================] - 1s 292us/step - loss: 168.8662 - val_loss: 962.0771\n",
      "Epoch 1302/2000\n",
      "3824/3824 [==============================] - 1s 291us/step - loss: 170.0271 - val_loss: 986.2981\n",
      "Epoch 1303/2000\n",
      "3824/3824 [==============================] - 1s 291us/step - loss: 171.2575 - val_loss: 986.2582\n",
      "Epoch 1304/2000\n",
      "3824/3824 [==============================] - 1s 292us/step - loss: 167.7216 - val_loss: 987.3622\n",
      "Epoch 1305/2000\n",
      "3824/3824 [==============================] - 1s 286us/step - loss: 159.0775 - val_loss: 966.1502\n",
      "Epoch 1306/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 161.6737 - val_loss: 961.9245\n",
      "Epoch 1307/2000\n",
      "3824/3824 [==============================] - 1s 291us/step - loss: 166.4140 - val_loss: 958.0910\n",
      "Epoch 1308/2000\n",
      "3824/3824 [==============================] - 1s 292us/step - loss: 171.8747 - val_loss: 955.5316\n",
      "Epoch 1309/2000\n",
      "3824/3824 [==============================] - 1s 288us/step - loss: 166.9391 - val_loss: 954.7202\n",
      "Epoch 1310/2000\n",
      "3824/3824 [==============================] - 1s 306us/step - loss: 166.6255 - val_loss: 948.5874\n",
      "Epoch 1311/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 164.4604 - val_loss: 972.7576\n",
      "Epoch 1312/2000\n",
      "3824/3824 [==============================] - 1s 316us/step - loss: 159.6870 - val_loss: 957.6500\n",
      "Epoch 1313/2000\n",
      "3824/3824 [==============================] - 1s 311us/step - loss: 165.0828 - val_loss: 970.3857\n",
      "Epoch 1314/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 164.1473 - val_loss: 969.8762\n",
      "Epoch 1315/2000\n",
      "3824/3824 [==============================] - 1s 299us/step - loss: 165.4625 - val_loss: 974.6693\n",
      "Epoch 1316/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 161.4858 - val_loss: 963.7941\n",
      "Epoch 1317/2000\n",
      "3824/3824 [==============================] - 1s 307us/step - loss: 161.0140 - val_loss: 950.5416\n",
      "Epoch 1318/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 162.1646 - val_loss: 980.1602\n",
      "Epoch 1319/2000\n",
      "3824/3824 [==============================] - 1s 287us/step - loss: 160.2937 - val_loss: 944.2051\n",
      "Epoch 1320/2000\n",
      "3824/3824 [==============================] - 1s 291us/step - loss: 172.6277 - val_loss: 989.1757\n",
      "Epoch 1321/2000\n",
      "3824/3824 [==============================] - 1s 293us/step - loss: 162.9328 - val_loss: 989.2370\n",
      "Epoch 1322/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 154.8154 - val_loss: 989.5701\n",
      "Epoch 1323/2000\n",
      "3824/3824 [==============================] - 1s 299us/step - loss: 167.7234 - val_loss: 984.6362\n",
      "Epoch 1324/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 159.2408 - val_loss: 970.7964\n",
      "Epoch 1325/2000\n",
      "3824/3824 [==============================] - 1s 319us/step - loss: 161.4059 - val_loss: 947.4393\n",
      "Epoch 1326/2000\n",
      "3824/3824 [==============================] - 1s 325us/step - loss: 171.3995 - val_loss: 980.2039\n",
      "Epoch 1327/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 175.0840 - val_loss: 1066.4883\n",
      "Epoch 1328/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 267.9683 - val_loss: 952.2354\n",
      "Epoch 1329/2000\n",
      "3824/3824 [==============================] - 1s 297us/step - loss: 194.0117 - val_loss: 968.3639\n",
      "Epoch 1330/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 225.8145 - val_loss: 1082.1105\n",
      "Epoch 1331/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 370.2246 - val_loss: 1007.8622\n",
      "Epoch 1332/2000\n",
      "3824/3824 [==============================] - 1s 301us/step - loss: 281.3293 - val_loss: 1007.4254\n",
      "Epoch 1333/2000\n",
      "3824/3824 [==============================] - 1s 290us/step - loss: 212.5686 - val_loss: 982.4859\n",
      "Epoch 1334/2000\n",
      "3824/3824 [==============================] - 1s 296us/step - loss: 174.5192 - val_loss: 968.0788\n",
      "Epoch 1335/2000\n",
      "3824/3824 [==============================] - 1s 301us/step - loss: 172.4531 - val_loss: 965.5064\n",
      "Epoch 1336/2000\n",
      "3824/3824 [==============================] - 1s 292us/step - loss: 183.3077 - val_loss: 992.6563\n",
      "Epoch 1337/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 188.0468 - val_loss: 974.0011\n",
      "Epoch 1338/2000\n",
      "3824/3824 [==============================] - 1s 318us/step - loss: 197.4947 - val_loss: 996.7725\n",
      "Epoch 1339/2000\n",
      "3824/3824 [==============================] - 1s 316us/step - loss: 201.3637 - val_loss: 950.3925\n",
      "Epoch 1340/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 184.7784 - val_loss: 993.1765\n",
      "Epoch 1341/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 172.1963 - val_loss: 984.1912\n",
      "Epoch 1342/2000\n",
      "3824/3824 [==============================] - 1s 314us/step - loss: 177.2353 - val_loss: 965.2996\n",
      "Epoch 1343/2000\n",
      "3824/3824 [==============================] - 1s 298us/step - loss: 162.0974 - val_loss: 964.9964\n",
      "Epoch 1344/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 169.7340 - val_loss: 958.2046\n",
      "Epoch 1345/2000\n",
      "3824/3824 [==============================] - 1s 302us/step - loss: 158.2051 - val_loss: 952.7539\n",
      "Epoch 1346/2000\n",
      "3824/3824 [==============================] - 1s 291us/step - loss: 166.2132 - val_loss: 967.3047\n",
      "Epoch 1347/2000\n",
      "3824/3824 [==============================] - 1s 347us/step - loss: 163.5969 - val_loss: 972.8666\n",
      "Epoch 1348/2000\n",
      "3824/3824 [==============================] - 1s 314us/step - loss: 177.8099 - val_loss: 962.0836\n",
      "Epoch 1349/2000\n",
      "3824/3824 [==============================] - 1s 311us/step - loss: 170.5474 - val_loss: 971.0229\n",
      "Epoch 1350/2000\n",
      "3824/3824 [==============================] - 1s 296us/step - loss: 163.2919 - val_loss: 946.1622\n",
      "Epoch 1351/2000\n",
      "3824/3824 [==============================] - 1s 321us/step - loss: 161.8022 - val_loss: 953.9428\n",
      "Epoch 1352/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 167.5735 - val_loss: 954.4506\n",
      "Epoch 1353/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 175.9896 - val_loss: 948.4956\n",
      "Epoch 1354/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 164.2281 - val_loss: 937.7287\n",
      "Epoch 1355/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 159.0923 - val_loss: 950.1588\n",
      "Epoch 1356/2000\n",
      "3824/3824 [==============================] - 1s 312us/step - loss: 163.0619 - val_loss: 945.8871\n",
      "Epoch 1357/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 169.2658 - val_loss: 971.3704\n",
      "Epoch 1358/2000\n",
      "3824/3824 [==============================] - 1s 298us/step - loss: 180.0514 - val_loss: 943.2605\n",
      "Epoch 1359/2000\n",
      "3824/3824 [==============================] - 1s 297us/step - loss: 166.3767 - val_loss: 966.2108\n",
      "Epoch 1360/2000\n",
      "3824/3824 [==============================] - 1s 302us/step - loss: 156.5347 - val_loss: 952.7398\n",
      "Epoch 1361/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 155.2566 - val_loss: 947.9865\n",
      "Epoch 1362/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 152.6645 - val_loss: 950.3398\n",
      "Epoch 1363/2000\n",
      "3824/3824 [==============================] - 1s 293us/step - loss: 159.8435 - val_loss: 952.3216\n",
      "Epoch 1364/2000\n",
      "3824/3824 [==============================] - 1s 305us/step - loss: 156.1192 - val_loss: 943.0330\n",
      "Epoch 1365/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 164.5103 - val_loss: 957.2379\n",
      "Epoch 1366/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 164.8631 - val_loss: 962.5273\n",
      "Epoch 1367/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 160.6252 - val_loss: 951.9720\n",
      "Epoch 1368/2000\n",
      "3824/3824 [==============================] - 1s 318us/step - loss: 162.0886 - val_loss: 948.2622\n",
      "Epoch 1369/2000\n",
      "3824/3824 [==============================] - 1s 311us/step - loss: 168.4534 - val_loss: 969.7872\n",
      "Epoch 1370/2000\n",
      "3824/3824 [==============================] - 1s 298us/step - loss: 176.3799 - val_loss: 966.3370\n",
      "Epoch 1371/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824/3824 [==============================] - 1s 292us/step - loss: 161.2206 - val_loss: 1036.2198\n",
      "Epoch 1372/2000\n",
      "3824/3824 [==============================] - 1s 297us/step - loss: 164.7653 - val_loss: 982.8923\n",
      "Epoch 1373/2000\n",
      "3824/3824 [==============================] - 1s 296us/step - loss: 167.3002 - val_loss: 983.0894\n",
      "Epoch 1374/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 164.0501 - val_loss: 994.3605\n",
      "Epoch 1375/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 162.4205 - val_loss: 980.3257\n",
      "Epoch 1376/2000\n",
      "3824/3824 [==============================] - 1s 298us/step - loss: 167.7689 - val_loss: 982.9841\n",
      "Epoch 1377/2000\n",
      "3824/3824 [==============================] - 1s 291us/step - loss: 158.8567 - val_loss: 979.7070\n",
      "Epoch 1378/2000\n",
      "3824/3824 [==============================] - 1s 304us/step - loss: 154.2805 - val_loss: 978.6349\n",
      "Epoch 1379/2000\n",
      "3824/3824 [==============================] - 1s 321us/step - loss: 166.7107 - val_loss: 964.5851\n",
      "Epoch 1380/2000\n",
      "3824/3824 [==============================] - 1s 313us/step - loss: 160.5978 - val_loss: 974.9986\n",
      "Epoch 1381/2000\n",
      "3824/3824 [==============================] - 1s 317us/step - loss: 159.0312 - val_loss: 949.2156\n",
      "Epoch 1382/2000\n",
      "3824/3824 [==============================] - 1s 318us/step - loss: 159.4474 - val_loss: 979.6974\n",
      "Epoch 1383/2000\n",
      "3824/3824 [==============================] - 1s 312us/step - loss: 159.4432 - val_loss: 979.4145\n",
      "Epoch 1384/2000\n",
      "3824/3824 [==============================] - 1s 311us/step - loss: 160.8846 - val_loss: 986.7561\n",
      "Epoch 1385/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 162.3076 - val_loss: 959.7515\n",
      "Epoch 1386/2000\n",
      "3824/3824 [==============================] - 1s 293us/step - loss: 161.0452 - val_loss: 999.9123\n",
      "Epoch 1387/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 159.2981 - val_loss: 958.7489\n",
      "Epoch 1388/2000\n",
      "3824/3824 [==============================] - 1s 298us/step - loss: 206.2851 - val_loss: 981.8726\n",
      "Epoch 1389/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 216.4412 - val_loss: 953.8431\n",
      "Epoch 1390/2000\n",
      "3824/3824 [==============================] - 1s 293us/step - loss: 186.4002 - val_loss: 991.6209\n",
      "Epoch 1391/2000\n",
      "3824/3824 [==============================] - 1s 293us/step - loss: 200.6003 - val_loss: 972.6147\n",
      "Epoch 1392/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 220.3958 - val_loss: 994.6544\n",
      "Epoch 1393/2000\n",
      "3824/3824 [==============================] - 1s 320us/step - loss: 194.9350 - val_loss: 963.0580\n",
      "Epoch 1394/2000\n",
      "3824/3824 [==============================] - 1s 319us/step - loss: 199.4872 - val_loss: 1010.1831\n",
      "Epoch 1395/2000\n",
      "3824/3824 [==============================] - 1s 311us/step - loss: 194.2910 - val_loss: 1041.6042\n",
      "Epoch 1396/2000\n",
      "3824/3824 [==============================] - 1s 317us/step - loss: 287.3439 - val_loss: 1064.4575\n",
      "Epoch 1397/2000\n",
      "3824/3824 [==============================] - 1s 296us/step - loss: 329.1938 - val_loss: 1007.0985\n",
      "Epoch 1398/2000\n",
      "3824/3824 [==============================] - 1s 297us/step - loss: 223.3153 - val_loss: 1035.9582\n",
      "Epoch 1399/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 246.7890 - val_loss: 976.2409\n",
      "Epoch 1400/2000\n",
      "3824/3824 [==============================] - 1s 296us/step - loss: 189.0014 - val_loss: 992.0719\n",
      "Epoch 1401/2000\n",
      "3824/3824 [==============================] - 1s 296us/step - loss: 182.6780 - val_loss: 1017.3245\n",
      "Epoch 1402/2000\n",
      "3824/3824 [==============================] - 1s 302us/step - loss: 201.8093 - val_loss: 983.6421\n",
      "Epoch 1403/2000\n",
      "3824/3824 [==============================] - 1s 293us/step - loss: 163.4574 - val_loss: 950.0104\n",
      "Epoch 1404/2000\n",
      "3824/3824 [==============================] - 1s 304us/step - loss: 186.0844 - val_loss: 955.5745\n",
      "Epoch 1405/2000\n",
      "3824/3824 [==============================] - 1s 298us/step - loss: 154.5944 - val_loss: 960.3955\n",
      "Epoch 1406/2000\n",
      "3824/3824 [==============================] - 1s 319us/step - loss: 156.8318 - val_loss: 960.9471\n",
      "Epoch 1407/2000\n",
      "3824/3824 [==============================] - 1s 319us/step - loss: 160.4677 - val_loss: 957.1560\n",
      "Epoch 1408/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 163.1400 - val_loss: 952.2276\n",
      "Epoch 1409/2000\n",
      "3824/3824 [==============================] - 1s 325us/step - loss: 165.7487 - val_loss: 922.6203\n",
      "Epoch 1410/2000\n",
      "3824/3824 [==============================] - 1s 309us/step - loss: 162.1375 - val_loss: 926.3607\n",
      "Epoch 1411/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 157.5654 - val_loss: 924.4331\n",
      "Epoch 1412/2000\n",
      "3824/3824 [==============================] - 1s 292us/step - loss: 164.0270 - val_loss: 920.9996\n",
      "Epoch 1413/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 168.2184 - val_loss: 917.5997\n",
      "Epoch 1414/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 165.2705 - val_loss: 918.1274\n",
      "Epoch 1415/2000\n",
      "3824/3824 [==============================] - 1s 293us/step - loss: 168.5089 - val_loss: 926.6046\n",
      "Epoch 1416/2000\n",
      "3824/3824 [==============================] - 1s 296us/step - loss: 160.1568 - val_loss: 934.5117\n",
      "Epoch 1417/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 156.2636 - val_loss: 925.3807\n",
      "Epoch 1418/2000\n",
      "3824/3824 [==============================] - 1s 306us/step - loss: 153.7440 - val_loss: 919.8979\n",
      "Epoch 1419/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 150.9844 - val_loss: 921.4385\n",
      "Epoch 1420/2000\n",
      "3824/3824 [==============================] - 1s 326us/step - loss: 165.1638 - val_loss: 919.1952\n",
      "Epoch 1421/2000\n",
      "3824/3824 [==============================] - 1s 319us/step - loss: 160.3877 - val_loss: 915.3071\n",
      "Epoch 1422/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 157.5621 - val_loss: 941.5022\n",
      "Epoch 1423/2000\n",
      "3824/3824 [==============================] - 1s 318us/step - loss: 175.7275 - val_loss: 1056.9258\n",
      "Epoch 1424/2000\n",
      "3824/3824 [==============================] - 1s 301us/step - loss: 188.6090 - val_loss: 1006.1155\n",
      "Epoch 1425/2000\n",
      "3824/3824 [==============================] - 1s 304us/step - loss: 189.2947 - val_loss: 990.0939\n",
      "Epoch 1426/2000\n",
      "3824/3824 [==============================] - 1s 306us/step - loss: 166.8839 - val_loss: 1008.6199\n",
      "Epoch 1427/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 166.6138 - val_loss: 1015.0061\n",
      "Epoch 1428/2000\n",
      "3824/3824 [==============================] - 1s 299us/step - loss: 162.2035 - val_loss: 993.4925\n",
      "Epoch 1429/2000\n",
      "3824/3824 [==============================] - 1s 301us/step - loss: 159.8777 - val_loss: 990.7427\n",
      "Epoch 1430/2000\n",
      "3824/3824 [==============================] - 1s 302us/step - loss: 151.0984 - val_loss: 954.3676\n",
      "Epoch 1431/2000\n",
      "3824/3824 [==============================] - 1s 298us/step - loss: 158.6508 - val_loss: 961.3606\n",
      "Epoch 1432/2000\n",
      "3824/3824 [==============================] - 1s 292us/step - loss: 153.6764 - val_loss: 965.4952\n",
      "Epoch 1433/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 158.4092 - val_loss: 964.2657\n",
      "Epoch 1434/2000\n",
      "3824/3824 [==============================] - 1s 325us/step - loss: 160.0030 - val_loss: 967.3713\n",
      "Epoch 1435/2000\n",
      "3824/3824 [==============================] - 1s 328us/step - loss: 160.1595 - val_loss: 970.5902\n",
      "Epoch 1436/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 160.1642 - val_loss: 989.0710\n",
      "Epoch 1437/2000\n",
      "3824/3824 [==============================] - 1s 321us/step - loss: 149.3700 - val_loss: 980.0868\n",
      "Epoch 1438/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 152.6505 - val_loss: 984.6384\n",
      "Epoch 1439/2000\n",
      "3824/3824 [==============================] - 1s 297us/step - loss: 161.3108 - val_loss: 988.4042\n",
      "Epoch 1440/2000\n",
      "3824/3824 [==============================] - 1s 318us/step - loss: 175.2932 - val_loss: 941.4258\n",
      "Epoch 1441/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 181.9760 - val_loss: 995.5860\n",
      "Epoch 1442/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 176.1746 - val_loss: 974.4770\n",
      "Epoch 1443/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824/3824 [==============================] - 1s 331us/step - loss: 180.8682 - val_loss: 1025.3759\n",
      "Epoch 1444/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 199.1231 - val_loss: 1002.2753\n",
      "Epoch 1445/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 266.4378 - val_loss: 970.7158\n",
      "Epoch 1446/2000\n",
      "3824/3824 [==============================] - 1s 351us/step - loss: 235.3273 - val_loss: 977.0889\n",
      "Epoch 1447/2000\n",
      "3824/3824 [==============================] - 1s 341us/step - loss: 223.4400 - val_loss: 1010.6830\n",
      "Epoch 1448/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 241.7180 - val_loss: 969.5253\n",
      "Epoch 1449/2000\n",
      "3824/3824 [==============================] - 1s 349us/step - loss: 174.9397 - val_loss: 973.7504\n",
      "Epoch 1450/2000\n",
      "3824/3824 [==============================] - 1s 360us/step - loss: 167.2602 - val_loss: 984.2652\n",
      "Epoch 1451/2000\n",
      "3824/3824 [==============================] - 1s 357us/step - loss: 158.3954 - val_loss: 979.0212\n",
      "Epoch 1452/2000\n",
      "3824/3824 [==============================] - 1s 330us/step - loss: 167.7392 - val_loss: 978.4667\n",
      "Epoch 1453/2000\n",
      "3824/3824 [==============================] - 1s 330us/step - loss: 160.4902 - val_loss: 976.8828\n",
      "Epoch 1454/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 163.3902 - val_loss: 973.6057\n",
      "Epoch 1455/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 159.0900 - val_loss: 969.4121\n",
      "Epoch 1456/2000\n",
      "3824/3824 [==============================] - 1s 321us/step - loss: 161.5880 - val_loss: 950.3181\n",
      "Epoch 1457/2000\n",
      "3824/3824 [==============================] - 1s 338us/step - loss: 167.0412 - val_loss: 960.6680\n",
      "Epoch 1458/2000\n",
      "3824/3824 [==============================] - 1s 342us/step - loss: 164.4578 - val_loss: 975.6216\n",
      "Epoch 1459/2000\n",
      "3824/3824 [==============================] - 1s 341us/step - loss: 169.8616 - val_loss: 957.2140\n",
      "Epoch 1460/2000\n",
      "3824/3824 [==============================] - 1s 312us/step - loss: 166.4143 - val_loss: 981.1662\n",
      "Epoch 1461/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 165.8410 - val_loss: 974.1423\n",
      "Epoch 1462/2000\n",
      "3824/3824 [==============================] - 1s 317us/step - loss: 165.8604 - val_loss: 980.1079\n",
      "Epoch 1463/2000\n",
      "3824/3824 [==============================] - 1s 298us/step - loss: 159.0674 - val_loss: 977.9845\n",
      "Epoch 1464/2000\n",
      "3824/3824 [==============================] - 1s 297us/step - loss: 154.9580 - val_loss: 973.4675\n",
      "Epoch 1465/2000\n",
      "3824/3824 [==============================] - 1s 296us/step - loss: 153.9894 - val_loss: 965.9191\n",
      "Epoch 1466/2000\n",
      "3824/3824 [==============================] - 1s 298us/step - loss: 156.9648 - val_loss: 977.7218\n",
      "Epoch 1467/2000\n",
      "3824/3824 [==============================] - 1s 301us/step - loss: 157.5056 - val_loss: 974.7265\n",
      "Epoch 1468/2000\n",
      "3824/3824 [==============================] - 1s 290us/step - loss: 164.6524 - val_loss: 994.1338\n",
      "Epoch 1469/2000\n",
      "3824/3824 [==============================] - 1s 289us/step - loss: 160.4991 - val_loss: 996.9050\n",
      "Epoch 1470/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 162.9303 - val_loss: 993.4539\n",
      "Epoch 1471/2000\n",
      "3824/3824 [==============================] - 1s 302us/step - loss: 163.0771 - val_loss: 980.7696\n",
      "Epoch 1472/2000\n",
      "3824/3824 [==============================] - 1s 321us/step - loss: 169.9849 - val_loss: 962.3718\n",
      "Epoch 1473/2000\n",
      "3824/3824 [==============================] - 1s 319us/step - loss: 156.7241 - val_loss: 967.5733\n",
      "Epoch 1474/2000\n",
      "3824/3824 [==============================] - 1s 326us/step - loss: 155.7142 - val_loss: 958.9167\n",
      "Epoch 1475/2000\n",
      "3824/3824 [==============================] - 1s 313us/step - loss: 161.3434 - val_loss: 967.3972\n",
      "Epoch 1476/2000\n",
      "3824/3824 [==============================] - 1s 317us/step - loss: 158.2093 - val_loss: 972.5797\n",
      "Epoch 1477/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 156.6892 - val_loss: 980.8658\n",
      "Epoch 1478/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 151.3837 - val_loss: 969.7650\n",
      "Epoch 1479/2000\n",
      "3824/3824 [==============================] - 1s 299us/step - loss: 156.0443 - val_loss: 971.4182\n",
      "Epoch 1480/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 152.1300 - val_loss: 976.2291\n",
      "Epoch 1481/2000\n",
      "3824/3824 [==============================] - 1s 296us/step - loss: 148.2849 - val_loss: 977.2669\n",
      "Epoch 1482/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 149.5783 - val_loss: 977.7631\n",
      "Epoch 1483/2000\n",
      "3824/3824 [==============================] - 1s 299us/step - loss: 158.1233 - val_loss: 975.0603\n",
      "Epoch 1484/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 155.9464 - val_loss: 976.0492\n",
      "Epoch 1485/2000\n",
      "3824/3824 [==============================] - 1s 317us/step - loss: 154.5527 - val_loss: 983.7627\n",
      "Epoch 1486/2000\n",
      "3824/3824 [==============================] - 1s 318us/step - loss: 157.3566 - val_loss: 977.7365\n",
      "Epoch 1487/2000\n",
      "3824/3824 [==============================] - 1s 318us/step - loss: 156.9059 - val_loss: 980.7255\n",
      "Epoch 1488/2000\n",
      "3824/3824 [==============================] - 1s 361us/step - loss: 180.4430 - val_loss: 951.7519\n",
      "Epoch 1489/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 180.1445 - val_loss: 1005.7911\n",
      "Epoch 1490/2000\n",
      "3824/3824 [==============================] - 1s 302us/step - loss: 189.3962 - val_loss: 1057.4104\n",
      "Epoch 1491/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 222.6637 - val_loss: 1125.4321\n",
      "Epoch 1492/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 402.3593 - val_loss: 1107.2011\n",
      "Epoch 1493/2000\n",
      "3824/3824 [==============================] - 1s 308us/step - loss: 269.5652 - val_loss: 1078.9048\n",
      "Epoch 1494/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 193.9709 - val_loss: 1026.1851\n",
      "Epoch 1495/2000\n",
      "3824/3824 [==============================] - 1s 302us/step - loss: 175.4378 - val_loss: 1018.2269\n",
      "Epoch 1496/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 162.4431 - val_loss: 994.8159\n",
      "Epoch 1497/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 155.2105 - val_loss: 990.2946\n",
      "Epoch 1498/2000\n",
      "3824/3824 [==============================] - 1s 299us/step - loss: 160.6675 - val_loss: 988.0713\n",
      "Epoch 1499/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 157.7870 - val_loss: 975.6761\n",
      "Epoch 1500/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 148.2145 - val_loss: 963.8991\n",
      "Epoch 1501/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 157.4973 - val_loss: 960.7650\n",
      "Epoch 1502/2000\n",
      "3824/3824 [==============================] - 1s 325us/step - loss: 145.5749 - val_loss: 951.5235\n",
      "Epoch 1503/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 145.5996 - val_loss: 942.3237\n",
      "Epoch 1504/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 147.5846 - val_loss: 937.9079\n",
      "Epoch 1505/2000\n",
      "3824/3824 [==============================] - 1s 297us/step - loss: 150.6951 - val_loss: 931.9303\n",
      "Epoch 1506/2000\n",
      "3824/3824 [==============================] - 1s 298us/step - loss: 152.6667 - val_loss: 954.1961\n",
      "Epoch 1507/2000\n",
      "3824/3824 [==============================] - 1s 302us/step - loss: 153.3322 - val_loss: 967.8894\n",
      "Epoch 1508/2000\n",
      "3824/3824 [==============================] - 1s 305us/step - loss: 155.9544 - val_loss: 978.8847\n",
      "Epoch 1509/2000\n",
      "3824/3824 [==============================] - 1s 347us/step - loss: 152.0068 - val_loss: 970.4590\n",
      "Epoch 1510/2000\n",
      "3824/3824 [==============================] - 2s 414us/step - loss: 150.7495 - val_loss: 934.3964\n",
      "Epoch 1511/2000\n",
      "3824/3824 [==============================] - 1s 362us/step - loss: 153.9512 - val_loss: 959.6040\n",
      "Epoch 1512/2000\n",
      "3824/3824 [==============================] - 1s 361us/step - loss: 150.8579 - val_loss: 956.4600\n",
      "Epoch 1513/2000\n",
      "3824/3824 [==============================] - 2s 393us/step - loss: 145.4060 - val_loss: 925.7996\n",
      "Epoch 1514/2000\n",
      "3824/3824 [==============================] - 1s 367us/step - loss: 162.9346 - val_loss: 936.9013\n",
      "Epoch 1515/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824/3824 [==============================] - 1s 347us/step - loss: 160.0419 - val_loss: 948.4611\n",
      "Epoch 1516/2000\n",
      "3824/3824 [==============================] - 1s 362us/step - loss: 159.9705 - val_loss: 943.6902\n",
      "Epoch 1517/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 157.8818 - val_loss: 972.8734\n",
      "Epoch 1518/2000\n",
      "3824/3824 [==============================] - 1s 298us/step - loss: 151.5020 - val_loss: 975.2121\n",
      "Epoch 1519/2000\n",
      "3824/3824 [==============================] - 1s 308us/step - loss: 147.0114 - val_loss: 980.8588\n",
      "Epoch 1520/2000\n",
      "3824/3824 [==============================] - 1s 307us/step - loss: 149.5092 - val_loss: 940.1477\n",
      "Epoch 1521/2000\n",
      "3824/3824 [==============================] - 1s 305us/step - loss: 159.9797 - val_loss: 968.5334\n",
      "Epoch 1522/2000\n",
      "3824/3824 [==============================] - 1s 298us/step - loss: 155.5185 - val_loss: 976.4406\n",
      "Epoch 1523/2000\n",
      "3824/3824 [==============================] - 1s 301us/step - loss: 162.5713 - val_loss: 983.1794\n",
      "Epoch 1524/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 153.1124 - val_loss: 972.7207\n",
      "Epoch 1525/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 146.4844 - val_loss: 977.7245\n",
      "Epoch 1526/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 153.7705 - val_loss: 975.0984\n",
      "Epoch 1527/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 148.8981 - val_loss: 1019.2800\n",
      "Epoch 1528/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 167.0103 - val_loss: 1006.8163\n",
      "Epoch 1529/2000\n",
      "3824/3824 [==============================] - 1s 310us/step - loss: 156.4988 - val_loss: 1006.9085\n",
      "Epoch 1530/2000\n",
      "3824/3824 [==============================] - 1s 313us/step - loss: 151.5093 - val_loss: 1014.4650\n",
      "Epoch 1531/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 157.0063 - val_loss: 1024.4329\n",
      "Epoch 1532/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 157.2570 - val_loss: 1007.5344\n",
      "Epoch 1533/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 157.3321 - val_loss: 1011.6152\n",
      "Epoch 1534/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 148.1629 - val_loss: 1006.1948\n",
      "Epoch 1535/2000\n",
      "3824/3824 [==============================] - 1s 320us/step - loss: 156.0712 - val_loss: 982.8193\n",
      "Epoch 1536/2000\n",
      "3824/3824 [==============================] - 1s 326us/step - loss: 153.8555 - val_loss: 977.8563\n",
      "Epoch 1537/2000\n",
      "3824/3824 [==============================] - 1s 340us/step - loss: 147.6562 - val_loss: 983.6415\n",
      "Epoch 1538/2000\n",
      "3824/3824 [==============================] - 1s 344us/step - loss: 149.4837 - val_loss: 983.4728\n",
      "Epoch 1539/2000\n",
      "3824/3824 [==============================] - 1s 345us/step - loss: 143.4698 - val_loss: 979.6571\n",
      "Epoch 1540/2000\n",
      "3824/3824 [==============================] - 1s 349us/step - loss: 160.6117 - val_loss: 978.0233\n",
      "Epoch 1541/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 160.0487 - val_loss: 943.4095\n",
      "Epoch 1542/2000\n",
      "3824/3824 [==============================] - 1s 321us/step - loss: 160.0499 - val_loss: 944.2976\n",
      "Epoch 1543/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 163.3469 - val_loss: 982.6296\n",
      "Epoch 1544/2000\n",
      "3824/3824 [==============================] - 1s 304us/step - loss: 208.3466 - val_loss: 1000.7858\n",
      "Epoch 1545/2000\n",
      "3824/3824 [==============================] - 1s 301us/step - loss: 168.6934 - val_loss: 1002.2827\n",
      "Epoch 1546/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 169.9839 - val_loss: 1063.6384\n",
      "Epoch 1547/2000\n",
      "3824/3824 [==============================] - 1s 307us/step - loss: 207.6168 - val_loss: 996.3472\n",
      "Epoch 1548/2000\n",
      "3824/3824 [==============================] - 1s 306us/step - loss: 182.6613 - val_loss: 966.8670\n",
      "Epoch 1549/2000\n",
      "3824/3824 [==============================] - 1s 310us/step - loss: 166.4948 - val_loss: 963.7302\n",
      "Epoch 1550/2000\n",
      "3824/3824 [==============================] - 1s 326us/step - loss: 157.7636 - val_loss: 963.4603\n",
      "Epoch 1551/2000\n",
      "3824/3824 [==============================] - 1s 336us/step - loss: 151.4952 - val_loss: 979.6400\n",
      "Epoch 1552/2000\n",
      "3824/3824 [==============================] - 1s 326us/step - loss: 160.7779 - val_loss: 936.9479\n",
      "Epoch 1553/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 156.3846 - val_loss: 965.2939\n",
      "Epoch 1554/2000\n",
      "3824/3824 [==============================] - 1s 328us/step - loss: 170.1657 - val_loss: 957.3685\n",
      "Epoch 1555/2000\n",
      "3824/3824 [==============================] - 1s 314us/step - loss: 185.6192 - val_loss: 992.8619\n",
      "Epoch 1556/2000\n",
      "3824/3824 [==============================] - 1s 313us/step - loss: 162.3828 - val_loss: 977.2563\n",
      "Epoch 1557/2000\n",
      "3824/3824 [==============================] - 1s 304us/step - loss: 167.9376 - val_loss: 1001.3695\n",
      "Epoch 1558/2000\n",
      "3824/3824 [==============================] - 1s 313us/step - loss: 175.5784 - val_loss: 947.4795\n",
      "Epoch 1559/2000\n",
      "3824/3824 [==============================] - 1s 308us/step - loss: 167.2196 - val_loss: 965.7208\n",
      "Epoch 1560/2000\n",
      "3824/3824 [==============================] - 1s 316us/step - loss: 156.3610 - val_loss: 950.4823\n",
      "Epoch 1561/2000\n",
      "3824/3824 [==============================] - 1s 307us/step - loss: 150.7485 - val_loss: 955.9033\n",
      "Epoch 1562/2000\n",
      "3824/3824 [==============================] - 1s 306us/step - loss: 160.7758 - val_loss: 946.3457\n",
      "Epoch 1563/2000\n",
      "3824/3824 [==============================] - 1s 328us/step - loss: 158.5638 - val_loss: 940.7130\n",
      "Epoch 1564/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 154.0420 - val_loss: 956.6106\n",
      "Epoch 1565/2000\n",
      "3824/3824 [==============================] - 1s 341us/step - loss: 147.9545 - val_loss: 946.0831\n",
      "Epoch 1566/2000\n",
      "3824/3824 [==============================] - 1s 336us/step - loss: 156.1462 - val_loss: 950.9539\n",
      "Epoch 1567/2000\n",
      "3824/3824 [==============================] - 1s 336us/step - loss: 150.7228 - val_loss: 950.1807\n",
      "Epoch 1568/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 149.8363 - val_loss: 951.5541\n",
      "Epoch 1569/2000\n",
      "3824/3824 [==============================] - 1s 313us/step - loss: 144.1952 - val_loss: 922.3584\n",
      "Epoch 1570/2000\n",
      "3824/3824 [==============================] - 1s 318us/step - loss: 153.5898 - val_loss: 922.5942\n",
      "Epoch 1571/2000\n",
      "3824/3824 [==============================] - 1s 306us/step - loss: 154.1513 - val_loss: 920.6101\n",
      "Epoch 1572/2000\n",
      "3824/3824 [==============================] - 1s 310us/step - loss: 147.7924 - val_loss: 934.5564\n",
      "Epoch 1573/2000\n",
      "3824/3824 [==============================] - 1s 312us/step - loss: 145.4169 - val_loss: 919.0536\n",
      "Epoch 1574/2000\n",
      "3824/3824 [==============================] - 1s 314us/step - loss: 163.0970 - val_loss: 913.9383\n",
      "Epoch 1575/2000\n",
      "3824/3824 [==============================] - 1s 315us/step - loss: 166.7636 - val_loss: 914.5298\n",
      "Epoch 1576/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 152.9936 - val_loss: 908.4241\n",
      "Epoch 1577/2000\n",
      "3824/3824 [==============================] - 1s 338us/step - loss: 163.6250 - val_loss: 968.7669\n",
      "Epoch 1578/2000\n",
      "3824/3824 [==============================] - 1s 328us/step - loss: 202.1515 - val_loss: 928.5247\n",
      "Epoch 1579/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 169.6483 - val_loss: 906.6478\n",
      "Epoch 1580/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 166.7362 - val_loss: 929.5464\n",
      "Epoch 1581/2000\n",
      "3824/3824 [==============================] - 1s 317us/step - loss: 171.3646 - val_loss: 1038.7931\n",
      "Epoch 1582/2000\n",
      "3824/3824 [==============================] - 1s 308us/step - loss: 241.8698 - val_loss: 987.7331\n",
      "Epoch 1583/2000\n",
      "3824/3824 [==============================] - 1s 311us/step - loss: 294.9872 - val_loss: 1008.4872\n",
      "Epoch 1584/2000\n",
      "3824/3824 [==============================] - 1s 311us/step - loss: 265.0231 - val_loss: 967.1188\n",
      "Epoch 1585/2000\n",
      "3824/3824 [==============================] - 1s 312us/step - loss: 170.3206 - val_loss: 946.7192\n",
      "Epoch 1586/2000\n",
      "3824/3824 [==============================] - 1s 313us/step - loss: 152.6411 - val_loss: 953.9884\n",
      "Epoch 1587/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824/3824 [==============================] - 1s 293us/step - loss: 150.9534 - val_loss: 917.1175\n",
      "Epoch 1588/2000\n",
      "3824/3824 [==============================] - 1s 296us/step - loss: 154.6297 - val_loss: 954.8970\n",
      "Epoch 1589/2000\n",
      "3824/3824 [==============================] - 1s 317us/step - loss: 157.1571 - val_loss: 954.8224\n",
      "Epoch 1590/2000\n",
      "3824/3824 [==============================] - 1s 318us/step - loss: 164.5202 - val_loss: 965.2993\n",
      "Epoch 1591/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 152.5899 - val_loss: 924.5406\n",
      "Epoch 1592/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 148.3293 - val_loss: 931.2362\n",
      "Epoch 1593/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 155.8388 - val_loss: 962.8239\n",
      "Epoch 1594/2000\n",
      "3824/3824 [==============================] - 1s 317us/step - loss: 153.2401 - val_loss: 960.5648\n",
      "Epoch 1595/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 149.3296 - val_loss: 952.4858\n",
      "Epoch 1596/2000\n",
      "3824/3824 [==============================] - 1s 319us/step - loss: 148.6703 - val_loss: 962.9772\n",
      "Epoch 1597/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 152.4694 - val_loss: 957.5129\n",
      "Epoch 1598/2000\n",
      "3824/3824 [==============================] - 1s 310us/step - loss: 146.1820 - val_loss: 998.9519\n",
      "Epoch 1599/2000\n",
      "3824/3824 [==============================] - 1s 305us/step - loss: 150.8485 - val_loss: 971.1399\n",
      "Epoch 1600/2000\n",
      "3824/3824 [==============================] - 1s 304us/step - loss: 154.4541 - val_loss: 972.2784\n",
      "Epoch 1601/2000\n",
      "3824/3824 [==============================] - 1s 302us/step - loss: 160.0877 - val_loss: 976.3713\n",
      "Epoch 1602/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 146.5424 - val_loss: 970.0513\n",
      "Epoch 1603/2000\n",
      "3824/3824 [==============================] - 1s 328us/step - loss: 153.0849 - val_loss: 971.8547\n",
      "Epoch 1604/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 143.8661 - val_loss: 998.2862\n",
      "Epoch 1605/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 148.0360 - val_loss: 961.3339\n",
      "Epoch 1606/2000\n",
      "3824/3824 [==============================] - 1s 317us/step - loss: 155.2609 - val_loss: 990.8686\n",
      "Epoch 1607/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 160.5714 - val_loss: 949.6425\n",
      "Epoch 1608/2000\n",
      "3824/3824 [==============================] - 1s 316us/step - loss: 150.9472 - val_loss: 975.9156\n",
      "Epoch 1609/2000\n",
      "3824/3824 [==============================] - 1s 306us/step - loss: 153.8792 - val_loss: 980.9985\n",
      "Epoch 1610/2000\n",
      "3824/3824 [==============================] - 1s 299us/step - loss: 150.8054 - val_loss: 981.8188\n",
      "Epoch 1611/2000\n",
      "3824/3824 [==============================] - 1s 312us/step - loss: 147.4016 - val_loss: 980.2161\n",
      "Epoch 1612/2000\n",
      "3824/3824 [==============================] - 1s 297us/step - loss: 147.9123 - val_loss: 988.6013\n",
      "Epoch 1613/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 150.6134 - val_loss: 987.9383\n",
      "Epoch 1614/2000\n",
      "3824/3824 [==============================] - 1s 304us/step - loss: 150.2881 - val_loss: 983.5232\n",
      "Epoch 1615/2000\n",
      "3824/3824 [==============================] - 1s 304us/step - loss: 148.0238 - val_loss: 988.1017\n",
      "Epoch 1616/2000\n",
      "3824/3824 [==============================] - 1s 338us/step - loss: 162.9300 - val_loss: 962.7057\n",
      "Epoch 1617/2000\n",
      "3824/3824 [==============================] - 1s 339us/step - loss: 150.1978 - val_loss: 980.9879\n",
      "Epoch 1618/2000\n",
      "3824/3824 [==============================] - 1s 325us/step - loss: 147.5213 - val_loss: 979.8493\n",
      "Epoch 1619/2000\n",
      "3824/3824 [==============================] - 1s 328us/step - loss: 145.0545 - val_loss: 962.7979\n",
      "Epoch 1620/2000\n",
      "3824/3824 [==============================] - 1s 326us/step - loss: 146.1810 - val_loss: 953.5470\n",
      "Epoch 1621/2000\n",
      "3824/3824 [==============================] - 1s 313us/step - loss: 150.8244 - val_loss: 962.2775\n",
      "Epoch 1622/2000\n",
      "3824/3824 [==============================] - 1s 310us/step - loss: 165.7165 - val_loss: 954.5733\n",
      "Epoch 1623/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 154.6738 - val_loss: 1007.3323\n",
      "Epoch 1624/2000\n",
      "3824/3824 [==============================] - 1s 305us/step - loss: 182.3325 - val_loss: 1033.5064\n",
      "Epoch 1625/2000\n",
      "3824/3824 [==============================] - 1s 310us/step - loss: 258.1720 - val_loss: 948.9714\n",
      "Epoch 1626/2000\n",
      "3824/3824 [==============================] - 1s 305us/step - loss: 194.9553 - val_loss: 941.4527\n",
      "Epoch 1627/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 180.5189 - val_loss: 1003.4911\n",
      "Epoch 1628/2000\n",
      "3824/3824 [==============================] - 1s 304us/step - loss: 219.0360 - val_loss: 967.7475\n",
      "Epoch 1629/2000\n",
      "3824/3824 [==============================] - 1s 314us/step - loss: 189.1648 - val_loss: 991.7773\n",
      "Epoch 1630/2000\n",
      "3824/3824 [==============================] - 1s 325us/step - loss: 157.6171 - val_loss: 973.5898\n",
      "Epoch 1631/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 145.9551 - val_loss: 954.9416\n",
      "Epoch 1632/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 167.5569 - val_loss: 1003.7844\n",
      "Epoch 1633/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 167.0346 - val_loss: 968.2970\n",
      "Epoch 1634/2000\n",
      "3824/3824 [==============================] - 1s 318us/step - loss: 164.8644 - val_loss: 981.7143\n",
      "Epoch 1635/2000\n",
      "3824/3824 [==============================] - 1s 304us/step - loss: 156.9567 - val_loss: 980.4906\n",
      "Epoch 1636/2000\n",
      "3824/3824 [==============================] - 1s 306us/step - loss: 147.3788 - val_loss: 978.5202\n",
      "Epoch 1637/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 143.0375 - val_loss: 988.6490\n",
      "Epoch 1638/2000\n",
      "3824/3824 [==============================] - 1s 319us/step - loss: 145.7192 - val_loss: 994.2674\n",
      "Epoch 1639/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 151.7144 - val_loss: 984.9270\n",
      "Epoch 1640/2000\n",
      "3824/3824 [==============================] - 1s 308us/step - loss: 143.3698 - val_loss: 982.2837\n",
      "Epoch 1641/2000\n",
      "3824/3824 [==============================] - 1s 304us/step - loss: 159.9670 - val_loss: 997.8483\n",
      "Epoch 1642/2000\n",
      "3824/3824 [==============================] - 1s 316us/step - loss: 158.5592 - val_loss: 977.5326\n",
      "Epoch 1643/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 153.0309 - val_loss: 992.1202\n",
      "Epoch 1644/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 148.4264 - val_loss: 986.2017\n",
      "Epoch 1645/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 144.8725 - val_loss: 978.8204\n",
      "Epoch 1646/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 147.9915 - val_loss: 996.5385\n",
      "Epoch 1647/2000\n",
      "3824/3824 [==============================] - 1s 325us/step - loss: 150.6049 - val_loss: 945.7845\n",
      "Epoch 1648/2000\n",
      "3824/3824 [==============================] - 1s 312us/step - loss: 167.5831 - val_loss: 1007.3784\n",
      "Epoch 1649/2000\n",
      "3824/3824 [==============================] - 1s 311us/step - loss: 176.8250 - val_loss: 980.0994\n",
      "Epoch 1650/2000\n",
      "3824/3824 [==============================] - 1s 306us/step - loss: 180.0809 - val_loss: 936.9921\n",
      "Epoch 1651/2000\n",
      "3824/3824 [==============================] - 1s 301us/step - loss: 148.5514 - val_loss: 952.1539\n",
      "Epoch 1652/2000\n",
      "3824/3824 [==============================] - 1s 310us/step - loss: 160.6127 - val_loss: 959.7168\n",
      "Epoch 1653/2000\n",
      "3824/3824 [==============================] - 1s 301us/step - loss: 148.2449 - val_loss: 961.2974\n",
      "Epoch 1654/2000\n",
      "3824/3824 [==============================] - 1s 302us/step - loss: 153.8938 - val_loss: 983.5062\n",
      "Epoch 1655/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 144.6646 - val_loss: 986.2826\n",
      "Epoch 1656/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 147.3443 - val_loss: 974.1583\n",
      "Epoch 1657/2000\n",
      "3824/3824 [==============================] - 1s 335us/step - loss: 164.0223 - val_loss: 983.5407\n",
      "Epoch 1658/2000\n",
      "3824/3824 [==============================] - 1s 336us/step - loss: 150.5151 - val_loss: 980.5998\n",
      "Epoch 1659/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824/3824 [==============================] - 1s 317us/step - loss: 155.7439 - val_loss: 958.0559\n",
      "Epoch 1660/2000\n",
      "3824/3824 [==============================] - 1s 325us/step - loss: 150.5495 - val_loss: 935.7119\n",
      "Epoch 1661/2000\n",
      "3824/3824 [==============================] - 1s 288us/step - loss: 146.7862 - val_loss: 983.0559\n",
      "Epoch 1662/2000\n",
      "3824/3824 [==============================] - 1s 293us/step - loss: 150.3102 - val_loss: 983.9810\n",
      "Epoch 1663/2000\n",
      "3824/3824 [==============================] - 1s 289us/step - loss: 155.7734 - val_loss: 1007.2298\n",
      "Epoch 1664/2000\n",
      "3824/3824 [==============================] - 1s 293us/step - loss: 182.7736 - val_loss: 960.7209\n",
      "Epoch 1665/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 189.2549 - val_loss: 974.0070\n",
      "Epoch 1666/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 170.4788 - val_loss: 940.4630\n",
      "Epoch 1667/2000\n",
      "3824/3824 [==============================] - 1s 305us/step - loss: 182.7319 - val_loss: 987.6813\n",
      "Epoch 1668/2000\n",
      "3824/3824 [==============================] - 1s 296us/step - loss: 196.5831 - val_loss: 992.4306\n",
      "Epoch 1669/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 173.1283 - val_loss: 981.1701\n",
      "Epoch 1670/2000\n",
      "3824/3824 [==============================] - 1s 315us/step - loss: 172.6377 - val_loss: 982.9590\n",
      "Epoch 1671/2000\n",
      "3824/3824 [==============================] - 1s 318us/step - loss: 181.4933 - val_loss: 1122.4011\n",
      "Epoch 1672/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 351.8106 - val_loss: 1044.6095\n",
      "Epoch 1673/2000\n",
      "3824/3824 [==============================] - 1s 319us/step - loss: 278.9363 - val_loss: 988.3956\n",
      "Epoch 1674/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 250.8155 - val_loss: 1018.1722\n",
      "Epoch 1675/2000\n",
      "3824/3824 [==============================] - 1s 308us/step - loss: 233.9617 - val_loss: 1049.5063\n",
      "Epoch 1676/2000\n",
      "3824/3824 [==============================] - 1s 301us/step - loss: 193.0537 - val_loss: 1048.1196\n",
      "Epoch 1677/2000\n",
      "3824/3824 [==============================] - 1s 299us/step - loss: 161.7799 - val_loss: 1022.4037\n",
      "Epoch 1678/2000\n",
      "3824/3824 [==============================] - 1s 299us/step - loss: 165.0437 - val_loss: 988.3171\n",
      "Epoch 1679/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 159.4964 - val_loss: 988.0213\n",
      "Epoch 1680/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 155.7736 - val_loss: 973.3532\n",
      "Epoch 1681/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 148.2823 - val_loss: 982.7597\n",
      "Epoch 1682/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 149.7898 - val_loss: 969.8420\n",
      "Epoch 1683/2000\n",
      "3824/3824 [==============================] - 1s 309us/step - loss: 149.6570 - val_loss: 986.4838\n",
      "Epoch 1684/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 164.1415 - val_loss: 976.2029\n",
      "Epoch 1685/2000\n",
      "3824/3824 [==============================] - 1s 320us/step - loss: 153.6315 - val_loss: 946.3296\n",
      "Epoch 1686/2000\n",
      "3824/3824 [==============================] - 1s 317us/step - loss: 150.3092 - val_loss: 937.6069\n",
      "Epoch 1687/2000\n",
      "3824/3824 [==============================] - 1s 314us/step - loss: 155.8431 - val_loss: 967.2646\n",
      "Epoch 1688/2000\n",
      "3824/3824 [==============================] - 1s 305us/step - loss: 152.8144 - val_loss: 986.2978\n",
      "Epoch 1689/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 150.3729 - val_loss: 987.9528\n",
      "Epoch 1690/2000\n",
      "3824/3824 [==============================] - 1s 292us/step - loss: 144.5812 - val_loss: 981.1015\n",
      "Epoch 1691/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 150.9815 - val_loss: 973.3246\n",
      "Epoch 1692/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 160.2493 - val_loss: 984.6252\n",
      "Epoch 1693/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 155.2564 - val_loss: 966.5017\n",
      "Epoch 1694/2000\n",
      "3824/3824 [==============================] - 1s 291us/step - loss: 150.9529 - val_loss: 968.9534\n",
      "Epoch 1695/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 153.8425 - val_loss: 966.2411\n",
      "Epoch 1696/2000\n",
      "3824/3824 [==============================] - 1s 288us/step - loss: 138.8560 - val_loss: 939.6611\n",
      "Epoch 1697/2000\n",
      "3824/3824 [==============================] - 1s 310us/step - loss: 145.2086 - val_loss: 936.8359\n",
      "Epoch 1698/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 140.7597 - val_loss: 969.6635\n",
      "Epoch 1699/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 142.6454 - val_loss: 979.7766\n",
      "Epoch 1700/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 152.9340 - val_loss: 976.1669\n",
      "Epoch 1701/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 147.1799 - val_loss: 967.1276\n",
      "Epoch 1702/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 140.6680 - val_loss: 955.9504\n",
      "Epoch 1703/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 145.7938 - val_loss: 943.8840\n",
      "Epoch 1704/2000\n",
      "3824/3824 [==============================] - 1s 296us/step - loss: 157.1521 - val_loss: 986.1353\n",
      "Epoch 1705/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 158.2656 - val_loss: 961.5897\n",
      "Epoch 1706/2000\n",
      "3824/3824 [==============================] - 1s 291us/step - loss: 146.8228 - val_loss: 963.0761\n",
      "Epoch 1707/2000\n",
      "3824/3824 [==============================] - 1s 299us/step - loss: 136.3995 - val_loss: 960.8806\n",
      "Epoch 1708/2000\n",
      "3824/3824 [==============================] - 1s 296us/step - loss: 144.9671 - val_loss: 949.9779\n",
      "Epoch 1709/2000\n",
      "3824/3824 [==============================] - 1s 304us/step - loss: 140.9377 - val_loss: 963.3724\n",
      "Epoch 1710/2000\n",
      "3824/3824 [==============================] - 1s 304us/step - loss: 144.4030 - val_loss: 955.6132\n",
      "Epoch 1711/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 142.1677 - val_loss: 974.9302\n",
      "Epoch 1712/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 146.0118 - val_loss: 965.4357\n",
      "Epoch 1713/2000\n",
      "3824/3824 [==============================] - 1s 321us/step - loss: 149.0768 - val_loss: 973.4541\n",
      "Epoch 1714/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 146.4097 - val_loss: 970.6993\n",
      "Epoch 1715/2000\n",
      "3824/3824 [==============================] - 1s 318us/step - loss: 143.4859 - val_loss: 993.7015\n",
      "Epoch 1716/2000\n",
      "3824/3824 [==============================] - 1s 297us/step - loss: 150.1250 - val_loss: 982.1276\n",
      "Epoch 1717/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 147.6438 - val_loss: 981.7284\n",
      "Epoch 1718/2000\n",
      "3824/3824 [==============================] - 1s 314us/step - loss: 157.6593 - val_loss: 987.9778\n",
      "Epoch 1719/2000\n",
      "3824/3824 [==============================] - 1s 301us/step - loss: 148.2235 - val_loss: 983.5297\n",
      "Epoch 1720/2000\n",
      "3824/3824 [==============================] - 1s 301us/step - loss: 149.6602 - val_loss: 977.6059\n",
      "Epoch 1721/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 142.3746 - val_loss: 964.9721\n",
      "Epoch 1722/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 153.4861 - val_loss: 979.6097\n",
      "Epoch 1723/2000\n",
      "3824/3824 [==============================] - 1s 307us/step - loss: 146.3727 - val_loss: 978.5845\n",
      "Epoch 1724/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 146.5213 - val_loss: 970.6064\n",
      "Epoch 1725/2000\n",
      "3824/3824 [==============================] - 1s 326us/step - loss: 146.7063 - val_loss: 932.7250\n",
      "Epoch 1726/2000\n",
      "3824/3824 [==============================] - 1s 325us/step - loss: 150.6385 - val_loss: 932.5159\n",
      "Epoch 1727/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 150.0618 - val_loss: 941.5236\n",
      "Epoch 1728/2000\n",
      "3824/3824 [==============================] - 1s 344us/step - loss: 138.6878 - val_loss: 977.0627\n",
      "Epoch 1729/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 146.5587 - val_loss: 965.5451\n",
      "Epoch 1730/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 140.4505 - val_loss: 950.4466\n",
      "Epoch 1731/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824/3824 [==============================] - 1s 295us/step - loss: 149.5147 - val_loss: 972.2309\n",
      "Epoch 1732/2000\n",
      "3824/3824 [==============================] - 1s 298us/step - loss: 154.2244 - val_loss: 966.5559\n",
      "Epoch 1733/2000\n",
      "3824/3824 [==============================] - 1s 288us/step - loss: 157.7386 - val_loss: 988.1483\n",
      "Epoch 1734/2000\n",
      "3824/3824 [==============================] - 1s 293us/step - loss: 148.3887 - val_loss: 957.5714\n",
      "Epoch 1735/2000\n",
      "3824/3824 [==============================] - 1s 301us/step - loss: 149.6419 - val_loss: 970.1690\n",
      "Epoch 1736/2000\n",
      "3824/3824 [==============================] - 1s 292us/step - loss: 152.1702 - val_loss: 941.5263\n",
      "Epoch 1737/2000\n",
      "3824/3824 [==============================] - 1s 304us/step - loss: 149.3040 - val_loss: 974.7633\n",
      "Epoch 1738/2000\n",
      "3824/3824 [==============================] - 1s 317us/step - loss: 149.4429 - val_loss: 970.2422\n",
      "Epoch 1739/2000\n",
      "3824/3824 [==============================] - 1s 316us/step - loss: 149.6065 - val_loss: 973.9463\n",
      "Epoch 1740/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 144.8432 - val_loss: 984.9213\n",
      "Epoch 1741/2000\n",
      "3824/3824 [==============================] - 1s 316us/step - loss: 150.3326 - val_loss: 974.6509\n",
      "Epoch 1742/2000\n",
      "3824/3824 [==============================] - 1s 313us/step - loss: 142.3190 - val_loss: 964.9792\n",
      "Epoch 1743/2000\n",
      "3824/3824 [==============================] - 1s 290us/step - loss: 143.6554 - val_loss: 975.4805\n",
      "Epoch 1744/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 153.8951 - val_loss: 937.8746\n",
      "Epoch 1745/2000\n",
      "3824/3824 [==============================] - 1s 292us/step - loss: 176.2108 - val_loss: 967.8014\n",
      "Epoch 1746/2000\n",
      "3824/3824 [==============================] - 1s 298us/step - loss: 156.6377 - val_loss: 1002.4880\n",
      "Epoch 1747/2000\n",
      "3824/3824 [==============================] - 1s 297us/step - loss: 175.2405 - val_loss: 954.9343\n",
      "Epoch 1748/2000\n",
      "3824/3824 [==============================] - 1s 292us/step - loss: 204.8955 - val_loss: 985.0269\n",
      "Epoch 1749/2000\n",
      "3824/3824 [==============================] - 1s 297us/step - loss: 181.1751 - val_loss: 993.5821\n",
      "Epoch 1750/2000\n",
      "3824/3824 [==============================] - 1s 293us/step - loss: 183.9422 - val_loss: 998.6491\n",
      "Epoch 1751/2000\n",
      "3824/3824 [==============================] - 1s 311us/step - loss: 157.2664 - val_loss: 972.7934\n",
      "Epoch 1752/2000\n",
      "3824/3824 [==============================] - 1s 340us/step - loss: 157.2241 - val_loss: 1020.3707\n",
      "Epoch 1753/2000\n",
      "3824/3824 [==============================] - 1s 315us/step - loss: 178.5285 - val_loss: 1053.7186\n",
      "Epoch 1754/2000\n",
      "3824/3824 [==============================] - 1s 317us/step - loss: 209.4458 - val_loss: 1041.8366\n",
      "Epoch 1755/2000\n",
      "3824/3824 [==============================] - 1s 318us/step - loss: 216.4889 - val_loss: 1033.0218\n",
      "Epoch 1756/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 205.5470 - val_loss: 1044.1899\n",
      "Epoch 1757/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 169.1529 - val_loss: 1026.6245\n",
      "Epoch 1758/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 153.9948 - val_loss: 1015.6138\n",
      "Epoch 1759/2000\n",
      "3824/3824 [==============================] - 1s 292us/step - loss: 156.1849 - val_loss: 1013.7715\n",
      "Epoch 1760/2000\n",
      "3824/3824 [==============================] - 1s 298us/step - loss: 146.3232 - val_loss: 1018.3896\n",
      "Epoch 1761/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 147.8795 - val_loss: 1006.3653\n",
      "Epoch 1762/2000\n",
      "3824/3824 [==============================] - 1s 298us/step - loss: 153.1895 - val_loss: 982.8081\n",
      "Epoch 1763/2000\n",
      "3824/3824 [==============================] - 1s 297us/step - loss: 166.1872 - val_loss: 974.6014\n",
      "Epoch 1764/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 154.1105 - val_loss: 974.6879\n",
      "Epoch 1765/2000\n",
      "3824/3824 [==============================] - 1s 316us/step - loss: 158.7232 - val_loss: 953.4735\n",
      "Epoch 1766/2000\n",
      "3824/3824 [==============================] - 1s 330us/step - loss: 160.3277 - val_loss: 970.8069\n",
      "Epoch 1767/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 184.2658 - val_loss: 1010.4366\n",
      "Epoch 1768/2000\n",
      "3824/3824 [==============================] - 1s 332us/step - loss: 165.5791 - val_loss: 1005.2199\n",
      "Epoch 1769/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 171.6160 - val_loss: 969.9810\n",
      "Epoch 1770/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 151.4561 - val_loss: 960.7075\n",
      "Epoch 1771/2000\n",
      "3824/3824 [==============================] - 1s 299us/step - loss: 163.2497 - val_loss: 954.8543\n",
      "Epoch 1772/2000\n",
      "3824/3824 [==============================] - 1s 305us/step - loss: 176.1537 - val_loss: 992.7921\n",
      "Epoch 1773/2000\n",
      "3824/3824 [==============================] - 1s 296us/step - loss: 180.4436 - val_loss: 987.3458\n",
      "Epoch 1774/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 211.8487 - val_loss: 987.4127\n",
      "Epoch 1775/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 260.3802 - val_loss: 1033.8899\n",
      "Epoch 1776/2000\n",
      "3824/3824 [==============================] - 1s 304us/step - loss: 301.3154 - val_loss: 1010.0586\n",
      "Epoch 1777/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 202.1677 - val_loss: 948.4321\n",
      "Epoch 1778/2000\n",
      "3824/3824 [==============================] - 1s 312us/step - loss: 173.5510 - val_loss: 992.3853\n",
      "Epoch 1779/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 172.3902 - val_loss: 984.7954\n",
      "Epoch 1780/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 155.2366 - val_loss: 983.2357\n",
      "Epoch 1781/2000\n",
      "3824/3824 [==============================] - 1s 325us/step - loss: 154.5204 - val_loss: 978.4631\n",
      "Epoch 1782/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 147.4402 - val_loss: 980.7550\n",
      "Epoch 1783/2000\n",
      "3824/3824 [==============================] - 1s 312us/step - loss: 148.6737 - val_loss: 973.1447\n",
      "Epoch 1784/2000\n",
      "3824/3824 [==============================] - 1s 312us/step - loss: 151.3178 - val_loss: 974.5830\n",
      "Epoch 1785/2000\n",
      "3824/3824 [==============================] - 1s 306us/step - loss: 148.5100 - val_loss: 973.1247\n",
      "Epoch 1786/2000\n",
      "3824/3824 [==============================] - 1s 309us/step - loss: 166.1225 - val_loss: 966.0951\n",
      "Epoch 1787/2000\n",
      "3824/3824 [==============================] - 1s 298us/step - loss: 147.0408 - val_loss: 959.3374\n",
      "Epoch 1788/2000\n",
      "3824/3824 [==============================] - 1s 299us/step - loss: 156.8450 - val_loss: 980.9162\n",
      "Epoch 1789/2000\n",
      "3824/3824 [==============================] - 1s 298us/step - loss: 160.9863 - val_loss: 975.1649\n",
      "Epoch 1790/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 141.1781 - val_loss: 974.0946\n",
      "Epoch 1791/2000\n",
      "3824/3824 [==============================] - 1s 306us/step - loss: 147.3438 - val_loss: 986.9839\n",
      "Epoch 1792/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 162.4419 - val_loss: 955.2363\n",
      "Epoch 1793/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 168.5751 - val_loss: 946.8902\n",
      "Epoch 1794/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 187.9015 - val_loss: 984.5524\n",
      "Epoch 1795/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 232.9906 - val_loss: 982.6027\n",
      "Epoch 1796/2000\n",
      "3824/3824 [==============================] - 1s 320us/step - loss: 178.7591 - val_loss: 994.3472\n",
      "Epoch 1797/2000\n",
      "3824/3824 [==============================] - 1s 304us/step - loss: 173.8912 - val_loss: 977.9834\n",
      "Epoch 1798/2000\n",
      "3824/3824 [==============================] - 1s 302us/step - loss: 164.0169 - val_loss: 980.0729\n",
      "Epoch 1799/2000\n",
      "3824/3824 [==============================] - 1s 301us/step - loss: 160.4468 - val_loss: 990.9114\n",
      "Epoch 1800/2000\n",
      "3824/3824 [==============================] - 1s 307us/step - loss: 169.3419 - val_loss: 979.2301\n",
      "Epoch 1801/2000\n",
      "3824/3824 [==============================] - 1s 306us/step - loss: 155.8653 - val_loss: 987.6207\n",
      "Epoch 1802/2000\n",
      "3824/3824 [==============================] - 1s 301us/step - loss: 156.1185 - val_loss: 985.0596\n",
      "Epoch 1803/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824/3824 [==============================] - 1s 297us/step - loss: 161.4252 - val_loss: 989.7031\n",
      "Epoch 1804/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 155.7404 - val_loss: 995.7288\n",
      "Epoch 1805/2000\n",
      "3824/3824 [==============================] - 1s 304us/step - loss: 199.2232 - val_loss: 975.5023\n",
      "Epoch 1806/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 152.6380 - val_loss: 970.5973\n",
      "Epoch 1807/2000\n",
      "3824/3824 [==============================] - 1s 317us/step - loss: 145.1612 - val_loss: 984.4829\n",
      "Epoch 1808/2000\n",
      "3824/3824 [==============================] - 1s 319us/step - loss: 151.2102 - val_loss: 984.3509\n",
      "Epoch 1809/2000\n",
      "3824/3824 [==============================] - 1s 325us/step - loss: 155.4033 - val_loss: 982.5165\n",
      "Epoch 1810/2000\n",
      "3824/3824 [==============================] - 1s 299us/step - loss: 144.4483 - val_loss: 973.7414\n",
      "Epoch 1811/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 135.0006 - val_loss: 956.9323\n",
      "Epoch 1812/2000\n",
      "3824/3824 [==============================] - 1s 305us/step - loss: 147.2732 - val_loss: 958.9197\n",
      "Epoch 1813/2000\n",
      "3824/3824 [==============================] - 1s 296us/step - loss: 149.0734 - val_loss: 965.7131\n",
      "Epoch 1814/2000\n",
      "3824/3824 [==============================] - 1s 291us/step - loss: 145.5775 - val_loss: 956.3749\n",
      "Epoch 1815/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 154.9999 - val_loss: 954.3576\n",
      "Epoch 1816/2000\n",
      "3824/3824 [==============================] - 1s 292us/step - loss: 147.3710 - val_loss: 964.9103\n",
      "Epoch 1817/2000\n",
      "3824/3824 [==============================] - 1s 304us/step - loss: 143.8083 - val_loss: 961.1093\n",
      "Epoch 1818/2000\n",
      "3824/3824 [==============================] - 1s 305us/step - loss: 151.8653 - val_loss: 974.7033\n",
      "Epoch 1819/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 141.6469 - val_loss: 979.5913\n",
      "Epoch 1820/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 142.5105 - val_loss: 977.1193\n",
      "Epoch 1821/2000\n",
      "3824/3824 [==============================] - 1s 330us/step - loss: 149.4943 - val_loss: 963.9726\n",
      "Epoch 1822/2000\n",
      "3824/3824 [==============================] - 1s 325us/step - loss: 148.7040 - val_loss: 962.7844\n",
      "Epoch 1823/2000\n",
      "3824/3824 [==============================] - 1s 319us/step - loss: 136.4178 - val_loss: 970.6446\n",
      "Epoch 1824/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 144.7766 - val_loss: 959.5022\n",
      "Epoch 1825/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 145.3670 - val_loss: 938.2717\n",
      "Epoch 1826/2000\n",
      "3824/3824 [==============================] - 1s 302us/step - loss: 145.8060 - val_loss: 949.1610\n",
      "Epoch 1827/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 139.4118 - val_loss: 953.1385\n",
      "Epoch 1828/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 143.5196 - val_loss: 971.6200\n",
      "Epoch 1829/2000\n",
      "3824/3824 [==============================] - 1s 306us/step - loss: 145.4771 - val_loss: 961.6274\n",
      "Epoch 1830/2000\n",
      "3824/3824 [==============================] - 1s 299us/step - loss: 141.3659 - val_loss: 974.6577\n",
      "Epoch 1831/2000\n",
      "3824/3824 [==============================] - 1s 296us/step - loss: 151.2953 - val_loss: 931.3064\n",
      "Epoch 1832/2000\n",
      "3824/3824 [==============================] - 1s 309us/step - loss: 182.4123 - val_loss: 984.5326\n",
      "Epoch 1833/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 172.0206 - val_loss: 986.8748\n",
      "Epoch 1834/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 181.4731 - val_loss: 1005.5525\n",
      "Epoch 1835/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 166.5771 - val_loss: 973.7257\n",
      "Epoch 1836/2000\n",
      "3824/3824 [==============================] - 1s 321us/step - loss: 198.7127 - val_loss: 978.6210\n",
      "Epoch 1837/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 161.9771 - val_loss: 954.9049\n",
      "Epoch 1838/2000\n",
      "3824/3824 [==============================] - 1s 304us/step - loss: 153.0618 - val_loss: 961.7028\n",
      "Epoch 1839/2000\n",
      "3824/3824 [==============================] - 1s 291us/step - loss: 153.1187 - val_loss: 932.8381\n",
      "Epoch 1840/2000\n",
      "3824/3824 [==============================] - 1s 296us/step - loss: 148.7154 - val_loss: 914.1624\n",
      "Epoch 1841/2000\n",
      "3824/3824 [==============================] - 1s 291us/step - loss: 145.8059 - val_loss: 945.2279\n",
      "Epoch 1842/2000\n",
      "3824/3824 [==============================] - 1s 305us/step - loss: 149.2325 - val_loss: 910.2899\n",
      "Epoch 1843/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 162.4808 - val_loss: 925.7397\n",
      "Epoch 1844/2000\n",
      "3824/3824 [==============================] - 1s 296us/step - loss: 149.5911 - val_loss: 929.3484\n",
      "Epoch 1845/2000\n",
      "3824/3824 [==============================] - 1s 296us/step - loss: 153.0219 - val_loss: 947.3071\n",
      "Epoch 1846/2000\n",
      "3824/3824 [==============================] - 1s 314us/step - loss: 143.4135 - val_loss: 952.7056\n",
      "Epoch 1847/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 150.2149 - val_loss: 983.1381\n",
      "Epoch 1848/2000\n",
      "3824/3824 [==============================] - 1s 321us/step - loss: 143.7092 - val_loss: 980.5813\n",
      "Epoch 1849/2000\n",
      "3824/3824 [==============================] - 1s 358us/step - loss: 140.6537 - val_loss: 977.4510\n",
      "Epoch 1850/2000\n",
      "3824/3824 [==============================] - 1s 333us/step - loss: 146.8343 - val_loss: 959.7814\n",
      "Epoch 1851/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 152.3608 - val_loss: 970.3182\n",
      "Epoch 1852/2000\n",
      "3824/3824 [==============================] - 1s 305us/step - loss: 143.3327 - val_loss: 956.3989\n",
      "Epoch 1853/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 144.5535 - val_loss: 964.0820\n",
      "Epoch 1854/2000\n",
      "3824/3824 [==============================] - 1s 310us/step - loss: 148.8231 - val_loss: 947.8617\n",
      "Epoch 1855/2000\n",
      "3824/3824 [==============================] - 1s 301us/step - loss: 145.5905 - val_loss: 978.0474\n",
      "Epoch 1856/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 144.8523 - val_loss: 975.1507\n",
      "Epoch 1857/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 144.1672 - val_loss: 986.3931\n",
      "Epoch 1858/2000\n",
      "3824/3824 [==============================] - 1s 301us/step - loss: 149.2268 - val_loss: 980.4364\n",
      "Epoch 1859/2000\n",
      "3824/3824 [==============================] - 1s 312us/step - loss: 154.0738 - val_loss: 949.8003\n",
      "Epoch 1860/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 150.0284 - val_loss: 936.0102\n",
      "Epoch 1861/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 144.3060 - val_loss: 978.2220\n",
      "Epoch 1862/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 137.9294 - val_loss: 978.0727\n",
      "Epoch 1863/2000\n",
      "3824/3824 [==============================] - 1s 330us/step - loss: 140.4098 - val_loss: 988.6528\n",
      "Epoch 1864/2000\n",
      "3824/3824 [==============================] - 1s 313us/step - loss: 140.0727 - val_loss: 973.0781\n",
      "Epoch 1865/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 141.1923 - val_loss: 973.2908\n",
      "Epoch 1866/2000\n",
      "3824/3824 [==============================] - 1s 299us/step - loss: 141.7118 - val_loss: 972.8077\n",
      "Epoch 1867/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 140.9395 - val_loss: 972.0814\n",
      "Epoch 1868/2000\n",
      "3824/3824 [==============================] - 1s 306us/step - loss: 141.4140 - val_loss: 969.5948\n",
      "Epoch 1869/2000\n",
      "3824/3824 [==============================] - 1s 316us/step - loss: 144.8788 - val_loss: 965.4374\n",
      "Epoch 1870/2000\n",
      "3824/3824 [==============================] - 1s 310us/step - loss: 146.6086 - val_loss: 972.8495\n",
      "Epoch 1871/2000\n",
      "3824/3824 [==============================] - 1s 306us/step - loss: 144.8526 - val_loss: 976.0251\n",
      "Epoch 1872/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 141.3676 - val_loss: 979.7649\n",
      "Epoch 1873/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 140.2778 - val_loss: 972.0721\n",
      "Epoch 1874/2000\n",
      "3824/3824 [==============================] - 1s 320us/step - loss: 143.5329 - val_loss: 971.7404\n",
      "Epoch 1875/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824/3824 [==============================] - 1s 327us/step - loss: 138.7795 - val_loss: 970.2148\n",
      "Epoch 1876/2000\n",
      "3824/3824 [==============================] - 1s 326us/step - loss: 138.2360 - val_loss: 963.8394\n",
      "Epoch 1877/2000\n",
      "3824/3824 [==============================] - 1s 317us/step - loss: 141.2970 - val_loss: 976.9040\n",
      "Epoch 1878/2000\n",
      "3824/3824 [==============================] - 1s 285us/step - loss: 162.2510 - val_loss: 998.8200\n",
      "Epoch 1879/2000\n",
      "3824/3824 [==============================] - 1s 289us/step - loss: 175.1762 - val_loss: 1003.7854\n",
      "Epoch 1880/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 160.6615 - val_loss: 964.9503\n",
      "Epoch 1881/2000\n",
      "3824/3824 [==============================] - 1s 289us/step - loss: 166.8655 - val_loss: 980.4324\n",
      "Epoch 1882/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 192.1419 - val_loss: 1113.7295\n",
      "Epoch 1883/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 270.3217 - val_loss: 1098.6439\n",
      "Epoch 1884/2000\n",
      "3824/3824 [==============================] - 1s 288us/step - loss: 350.2091 - val_loss: 1048.1642\n",
      "Epoch 1885/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 299.7758 - val_loss: 1017.2264\n",
      "Epoch 1886/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 161.7207 - val_loss: 979.2646\n",
      "Epoch 1887/2000\n",
      "3824/3824 [==============================] - 1s 317us/step - loss: 152.6118 - val_loss: 952.3226\n",
      "Epoch 1888/2000\n",
      "3824/3824 [==============================] - 1s 313us/step - loss: 150.3174 - val_loss: 927.7249\n",
      "Epoch 1889/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 137.7069 - val_loss: 930.5196\n",
      "Epoch 1890/2000\n",
      "3824/3824 [==============================] - 1s 317us/step - loss: 146.3586 - val_loss: 971.9359\n",
      "Epoch 1891/2000\n",
      "3824/3824 [==============================] - 1s 319us/step - loss: 135.7150 - val_loss: 969.4148\n",
      "Epoch 1892/2000\n",
      "3824/3824 [==============================] - 1s 296us/step - loss: 140.8262 - val_loss: 968.1475\n",
      "Epoch 1893/2000\n",
      "3824/3824 [==============================] - 1s 291us/step - loss: 148.6149 - val_loss: 1002.6784\n",
      "Epoch 1894/2000\n",
      "3824/3824 [==============================] - 1s 292us/step - loss: 152.4605 - val_loss: 1053.5494\n",
      "Epoch 1895/2000\n",
      "3824/3824 [==============================] - 1s 290us/step - loss: 143.7279 - val_loss: 941.5761\n",
      "Epoch 1896/2000\n",
      "3824/3824 [==============================] - 1s 300us/step - loss: 146.8391 - val_loss: 934.5227\n",
      "Epoch 1897/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 145.2823 - val_loss: 949.5577\n",
      "Epoch 1898/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 149.1196 - val_loss: 963.3159\n",
      "Epoch 1899/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 147.3372 - val_loss: 956.8094\n",
      "Epoch 1900/2000\n",
      "3824/3824 [==============================] - 1s 305us/step - loss: 138.6814 - val_loss: 983.8530\n",
      "Epoch 1901/2000\n",
      "3824/3824 [==============================] - 1s 319us/step - loss: 163.5740 - val_loss: 971.6423\n",
      "Epoch 1902/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 157.0635 - val_loss: 961.6646\n",
      "Epoch 1903/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 155.4439 - val_loss: 942.3526\n",
      "Epoch 1904/2000\n",
      "3824/3824 [==============================] - 1s 314us/step - loss: 156.5847 - val_loss: 971.1439\n",
      "Epoch 1905/2000\n",
      "3824/3824 [==============================] - 1s 309us/step - loss: 152.1194 - val_loss: 944.7452\n",
      "Epoch 1906/2000\n",
      "3824/3824 [==============================] - 1s 307us/step - loss: 152.2863 - val_loss: 973.9865\n",
      "Epoch 1907/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 144.4995 - val_loss: 984.8319\n",
      "Epoch 1908/2000\n",
      "3824/3824 [==============================] - 1s 291us/step - loss: 150.7495 - val_loss: 953.3606\n",
      "Epoch 1909/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 138.3539 - val_loss: 988.8238\n",
      "Epoch 1910/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 146.0070 - val_loss: 992.9032\n",
      "Epoch 1911/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 149.9484 - val_loss: 973.2153\n",
      "Epoch 1912/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 147.8088 - val_loss: 1000.6827\n",
      "Epoch 1913/2000\n",
      "3824/3824 [==============================] - 1s 298us/step - loss: 151.0221 - val_loss: 1005.4735\n",
      "Epoch 1914/2000\n",
      "3824/3824 [==============================] - 1s 318us/step - loss: 144.8656 - val_loss: 991.5648\n",
      "Epoch 1915/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 137.2844 - val_loss: 965.7806\n",
      "Epoch 1916/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 150.2084 - val_loss: 940.7372\n",
      "Epoch 1917/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 161.2488 - val_loss: 952.5767\n",
      "Epoch 1918/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 140.0565 - val_loss: 975.4173\n",
      "Epoch 1919/2000\n",
      "3824/3824 [==============================] - 1s 297us/step - loss: 146.7362 - val_loss: 966.2513\n",
      "Epoch 1920/2000\n",
      "3824/3824 [==============================] - 1s 316us/step - loss: 140.9959 - val_loss: 964.5713\n",
      "Epoch 1921/2000\n",
      "3824/3824 [==============================] - 1s 301us/step - loss: 153.3838 - val_loss: 969.4061\n",
      "Epoch 1922/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 138.0001 - val_loss: 962.9315\n",
      "Epoch 1923/2000\n",
      "3824/3824 [==============================] - 1s 309us/step - loss: 134.6467 - val_loss: 952.1603\n",
      "Epoch 1924/2000\n",
      "3824/3824 [==============================] - 1s 298us/step - loss: 136.4227 - val_loss: 968.0604\n",
      "Epoch 1925/2000\n",
      "3824/3824 [==============================] - 1s 296us/step - loss: 145.2892 - val_loss: 944.0765\n",
      "Epoch 1926/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 137.6630 - val_loss: 972.2059\n",
      "Epoch 1927/2000\n",
      "3824/3824 [==============================] - 1s 314us/step - loss: 135.3174 - val_loss: 962.2821\n",
      "Epoch 1928/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 142.1558 - val_loss: 925.5356\n",
      "Epoch 1929/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 138.5576 - val_loss: 971.7902\n",
      "Epoch 1930/2000\n",
      "3824/3824 [==============================] - 1s 322us/step - loss: 147.4680 - val_loss: 926.5972\n",
      "Epoch 1931/2000\n",
      "3824/3824 [==============================] - 1s 326us/step - loss: 142.9288 - val_loss: 964.9033\n",
      "Epoch 1932/2000\n",
      "3824/3824 [==============================] - 1s 321us/step - loss: 140.8727 - val_loss: 973.7815\n",
      "Epoch 1933/2000\n",
      "3824/3824 [==============================] - 1s 301us/step - loss: 138.6492 - val_loss: 966.1995\n",
      "Epoch 1934/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 138.6286 - val_loss: 973.3020\n",
      "Epoch 1935/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 140.6613 - val_loss: 938.9749\n",
      "Epoch 1936/2000\n",
      "3824/3824 [==============================] - 1s 307us/step - loss: 143.5968 - val_loss: 973.8014\n",
      "Epoch 1937/2000\n",
      "3824/3824 [==============================] - 1s 301us/step - loss: 140.3251 - val_loss: 964.4521\n",
      "Epoch 1938/2000\n",
      "3824/3824 [==============================] - 1s 299us/step - loss: 156.5735 - val_loss: 973.1872\n",
      "Epoch 1939/2000\n",
      "3824/3824 [==============================] - 1s 298us/step - loss: 142.3920 - val_loss: 959.5483\n",
      "Epoch 1940/2000\n",
      "3824/3824 [==============================] - 1s 309us/step - loss: 137.9304 - val_loss: 948.1353\n",
      "Epoch 1941/2000\n",
      "3824/3824 [==============================] - 1s 313us/step - loss: 145.4326 - val_loss: 969.7426\n",
      "Epoch 1942/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 137.4156 - val_loss: 963.7788\n",
      "Epoch 1943/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 151.1780 - val_loss: 953.4524\n",
      "Epoch 1944/2000\n",
      "3824/3824 [==============================] - 1s 327us/step - loss: 136.1299 - val_loss: 946.6181\n",
      "Epoch 1945/2000\n",
      "3824/3824 [==============================] - 1s 328us/step - loss: 139.8774 - val_loss: 968.9432\n",
      "Epoch 1946/2000\n",
      "3824/3824 [==============================] - 1s 309us/step - loss: 134.0217 - val_loss: 975.1919\n",
      "Epoch 1947/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824/3824 [==============================] - 1s 293us/step - loss: 132.9816 - val_loss: 960.3968\n",
      "Epoch 1948/2000\n",
      "3824/3824 [==============================] - 1s 303us/step - loss: 135.2409 - val_loss: 937.1544\n",
      "Epoch 1949/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 153.4148 - val_loss: 975.0153\n",
      "Epoch 1950/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 150.2819 - val_loss: 932.5272\n",
      "Epoch 1951/2000\n",
      "3824/3824 [==============================] - 1s 292us/step - loss: 142.5368 - val_loss: 981.3204\n",
      "Epoch 1952/2000\n",
      "3824/3824 [==============================] - 1s 292us/step - loss: 155.6586 - val_loss: 997.4679\n",
      "Epoch 1953/2000\n",
      "3824/3824 [==============================] - 1s 292us/step - loss: 146.0608 - val_loss: 996.7542\n",
      "Epoch 1954/2000\n",
      "3824/3824 [==============================] - 1s 302us/step - loss: 151.8987 - val_loss: 978.8439\n",
      "Epoch 1955/2000\n",
      "3824/3824 [==============================] - 1s 318us/step - loss: 134.5064 - val_loss: 952.8342\n",
      "Epoch 1956/2000\n",
      "3824/3824 [==============================] - 1s 312us/step - loss: 138.5190 - val_loss: 988.3788\n",
      "Epoch 1957/2000\n",
      "3824/3824 [==============================] - 1s 320us/step - loss: 146.1278 - val_loss: 951.2596\n",
      "Epoch 1958/2000\n",
      "3824/3824 [==============================] - 1s 313us/step - loss: 144.0126 - val_loss: 992.7899\n",
      "Epoch 1959/2000\n",
      "3824/3824 [==============================] - 1s 315us/step - loss: 155.8197 - val_loss: 980.3889\n",
      "Epoch 1960/2000\n",
      "3824/3824 [==============================] - 1s 293us/step - loss: 157.9836 - val_loss: 1013.9851\n",
      "Epoch 1961/2000\n",
      "3824/3824 [==============================] - 1s 297us/step - loss: 168.5490 - val_loss: 994.5124\n",
      "Epoch 1962/2000\n",
      "3824/3824 [==============================] - ETA: 0s - loss: 199.462 - 1s 291us/step - loss: 193.1153 - val_loss: 1084.7613\n",
      "Epoch 1963/2000\n",
      "3824/3824 [==============================] - 1s 296us/step - loss: 305.6377 - val_loss: 1085.1316\n",
      "Epoch 1964/2000\n",
      "3824/3824 [==============================] - 1s 287us/step - loss: 288.9099 - val_loss: 1072.2131\n",
      "Epoch 1965/2000\n",
      "3824/3824 [==============================] - 1s 293us/step - loss: 163.0889 - val_loss: 973.2339\n",
      "Epoch 1966/2000\n",
      "3824/3824 [==============================] - 1s 304us/step - loss: 150.9947 - val_loss: 953.8074\n",
      "Epoch 1967/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 137.3413 - val_loss: 934.8608\n",
      "Epoch 1968/2000\n",
      "3824/3824 [==============================] - 1s 307us/step - loss: 142.4559 - val_loss: 923.3961\n",
      "Epoch 1969/2000\n",
      "3824/3824 [==============================] - 1s 321us/step - loss: 142.4730 - val_loss: 932.9219\n",
      "Epoch 1970/2000\n",
      "3824/3824 [==============================] - 1s 314us/step - loss: 138.9602 - val_loss: 930.7868\n",
      "Epoch 1971/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 141.4058 - val_loss: 935.4040\n",
      "Epoch 1972/2000\n",
      "3824/3824 [==============================] - 1s 320us/step - loss: 142.8020 - val_loss: 973.3303\n",
      "Epoch 1973/2000\n",
      "3824/3824 [==============================] - 1s 331us/step - loss: 145.5976 - val_loss: 931.9106\n",
      "Epoch 1974/2000\n",
      "3824/3824 [==============================] - 1s 319us/step - loss: 173.0887 - val_loss: 1009.5642\n",
      "Epoch 1975/2000\n",
      "3824/3824 [==============================] - 1s 315us/step - loss: 270.4237 - val_loss: 958.2109\n",
      "Epoch 1976/2000\n",
      "3824/3824 [==============================] - 1s 319us/step - loss: 168.0146 - val_loss: 942.7825\n",
      "Epoch 1977/2000\n",
      "3824/3824 [==============================] - 1s 317us/step - loss: 165.1682 - val_loss: 981.5679\n",
      "Epoch 1978/2000\n",
      "3824/3824 [==============================] - 1s 319us/step - loss: 158.6604 - val_loss: 961.3730\n",
      "Epoch 1979/2000\n",
      "3824/3824 [==============================] - 1s 323us/step - loss: 157.8075 - val_loss: 944.6450\n",
      "Epoch 1980/2000\n",
      "3824/3824 [==============================] - 1s 315us/step - loss: 145.3625 - val_loss: 950.0662\n",
      "Epoch 1981/2000\n",
      "3824/3824 [==============================] - 1s 316us/step - loss: 155.9170 - val_loss: 976.1329\n",
      "Epoch 1982/2000\n",
      "3824/3824 [==============================] - 1s 349us/step - loss: 151.9605 - val_loss: 968.0275\n",
      "Epoch 1983/2000\n",
      "3824/3824 [==============================] - 1s 344us/step - loss: 148.5380 - val_loss: 971.0502\n",
      "Epoch 1984/2000\n",
      "3824/3824 [==============================] - 1s 329us/step - loss: 139.8721 - val_loss: 961.4338\n",
      "Epoch 1985/2000\n",
      "3824/3824 [==============================] - 1s 337us/step - loss: 142.5104 - val_loss: 967.1733\n",
      "Epoch 1986/2000\n",
      "3824/3824 [==============================] - 1s 320us/step - loss: 141.7284 - val_loss: 980.3255\n",
      "Epoch 1987/2000\n",
      "3824/3824 [==============================] - 1s 305us/step - loss: 143.0475 - val_loss: 989.0854\n",
      "Epoch 1988/2000\n",
      "3824/3824 [==============================] - 1s 295us/step - loss: 148.7429 - val_loss: 966.6924\n",
      "Epoch 1989/2000\n",
      "3824/3824 [==============================] - 1s 299us/step - loss: 178.0054 - val_loss: 983.3322\n",
      "Epoch 1990/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 165.9437 - val_loss: 926.9165\n",
      "Epoch 1991/2000\n",
      "3824/3824 [==============================] - 1s 305us/step - loss: 143.9614 - val_loss: 985.4479\n",
      "Epoch 1992/2000\n",
      "3824/3824 [==============================] - 1s 296us/step - loss: 142.5715 - val_loss: 980.7475\n",
      "Epoch 1993/2000\n",
      "3824/3824 [==============================] - 1s 294us/step - loss: 145.1451 - val_loss: 975.9325\n",
      "Epoch 1994/2000\n",
      "3824/3824 [==============================] - 1s 306us/step - loss: 142.3769 - val_loss: 970.9876\n",
      "Epoch 1995/2000\n",
      "3824/3824 [==============================] - 1s 334us/step - loss: 136.8241 - val_loss: 978.3967\n",
      "Epoch 1996/2000\n",
      "3824/3824 [==============================] - 1s 338us/step - loss: 139.0387 - val_loss: 939.5169\n",
      "Epoch 1997/2000\n",
      "3824/3824 [==============================] - 1s 325us/step - loss: 137.2603 - val_loss: 922.4514\n",
      "Epoch 1998/2000\n",
      "3824/3824 [==============================] - 1s 324us/step - loss: 149.1927 - val_loss: 938.7521\n",
      "Epoch 1999/2000\n",
      "3824/3824 [==============================] - 1s 328us/step - loss: 144.0071 - val_loss: 933.3595\n",
      "Epoch 2000/2000\n",
      "3824/3824 [==============================] - 1s 305us/step - loss: 133.8714 - val_loss: 950.2786\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAETCAYAAAAs4pGmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xd0m9X5wPGvhi3b8YxHJtnJJUASMpghkKRsSimdlDIKFFoamh8tZe8CBcIOKxCggQYaWmahDTOBDDLI3jexs5f3tiVZ4/eHhiVbtiXbkpz4+ZyTc6T7rsey8z66473X4Ha7EUIIIZoyxjsAIYQQXZMkCCGEECFJghBCCBGSJAghhBAhSYIQQggRkiQIIYQQIUmCEKITKKU+VUr9po19JiulNoVbLkS8SYIQQggRkjneAQgRa0qpycCjwF5AAbXAY8B07/v3tdZ/8u57g7fcCRQCN2mttyul+gJvAn2BPUBewPlHAs8B2YAJmKm1fiPM2DKAF4ETATcwH7hLa+1QSj0IXArYgVLgN1rrQy2Vt+/TEaKR1CBEd3US8JjW+kSgCrgTuAgYB0xTSvVVSk0FbgOmaK3HAO8AHymlDHhu4su11sfjSSDHAiilzMB7wB1a6/HAWcBflFKnhhnXTDw3+VHABGCM9/hjgJuBk7TWE4AvgFNaKu/IByOEjyQI0V3t0lqv9b4uABZqre1a6xI8CaMncD7wrta6GEBrPQfoBwwCzgbmeMvzgQXec40AhgJvKKXWAd8CycDYMOO6AHhBa+3WWtuAWd6yA8B6YI1S6klgndb6o1bKhegwSRCiu7I1ed8QYh8TnmaeQAYgwVtuCCh3BBxTqbU+0fcPOBX4e5hxGZtc0wgkaK1deGojv8FTw3hGKTWjpfIwryVEqyRBCNGyz4DLlFK5AEqpa/DchPO9227wlg8ApniP0UC9UuoK77ZjgE3A+DCv+Tlwk1LKoJSyeK/xpVJqjPc8W7XWjwLPACe1VN6xH1sID0kQQrRAa/0lnhvuAqXUZuBq4Ifeb+3TgOOUUluB14F13mPswCXAb5VSG/D0CdyrtV4a5mWn4+nw3uj9p4FHtNbrgX8Bq5RSq4BrgT+3VN7xn14IMMh030IIIUKRGoQQQoiQJEEIIYQISRKEEEKIkCRBCCGECOmommqjuLi63T3uWVkplJfXdWY4nULiiozEFRmJKzJHa1y5uWmGUOVSg/Aym03xDiEkiSsyEldkJK7IdLe4JEEIIYQISRKEEEKIkCRBCCGECEkShBBCiJAkQQghhAhJEoQQQoiQJEEIIYQISRIEsPTACtYc3BjvMIQQokuRBAF8XDCfeRv/E+8whBACm83GJ5+Et2rs//73CUuWfBu1WCRBAImmRGrtXe/xeSFE91NWVhp2grjwwos544yzohbLUTUXU3ulJCRTbquIdxhCiC7mXwvy+X5bkf+9yWTA6ezYImsnHZvHL6YOa3H7W2+9we7du5g06SQmTDiZ+vp67rjjXj777L9s27aFuro6Bg0azF133c/rr79CdnY2o0cfx4svziIhwcyhQweZOvUcrr76ug7FCZIgAEgxJ3Og5hAutwujQSpVQoj4ueqqaykoyOeUU06jurqam2/+C7W1NaSlpfHssy/hcrm48spfUFxcFHRcYeEh5sz5Jw0NDfz4x+dLgugsyeZkAKwOKykJKXGORgjRVfxi6rCgb/u5uWkUF1fH7PoDBgwEwGJJory8nPvvv4uUlBTq6+txOBxB+w4ZMgyz2YzZbMZiSeqU60uCAJLNng+zThKEECLODAYjbrcLAKPRMwv38uVLKSoq5K9/fZTy8nIWLVqI2+1uclznxyIJAk8fBEC9oz7OkQghurusrCwaGhzYbDZ/2ciRxzNnzuvccMNvSExMpG/ffpSUFEc9FkkQePogAGobZCSTECK+LBYLc+a8E1SWnZ3Da6+91Wzf0aNPBDxNX0OGHOcv/89/Pu+UWKRHFkhLTAWgxl4T50iEEKLrkAQBpCV4EkRVgyQIIYTwkQQBpCWmAVAtNQghhPCTBAGkJfYAJEEIIUQgSRAE1iBiN75ZCCG6OkkQQJLJQoLRTLW9Nt6hCCFElyEJAjAYDGQkpVMlNQghRJxFMpurz/fff09+/o5Oj0UShFeGJY2ahppmTycKIUQsRTKbq8/7778flQfnovqgnFLqTuBHQCLwEvAtMAdwA5uAaVprl1LqeuB3gAN4WGv9qVIqGZgL5AHVwNVa66g9OpiRlEZDuQOr0+afekMI0b19kP8pa4saFxMzGQ04XR37Ejk2bxQ/GfbDFrf7ZnN9441X2bkzn8rKSgBuvvlWhg4dxiOPPMCBA/ux2+386ldX0K/fMSxevJgNGzYyaNAQevfu3aH4AkWtBqGUmgycDkwEzgKOAZ4G7tFaTwIMwCVKqd7AdO9+5wGPKqUswI3ARu++bwH3RCtWgPQk6agWQsTfVVddy6BBg7FarYwffzLPP/8Kt912N08++Sh1dbWsWbOKRx55giefnInT6eLYY0cyadIkbrxxeqcmB4huDeI8YCPwIZAO3Apcj6cWATAfOBdwAku11jbAppTKB0YDZwAzAva9t60LZmWlYDab2hVs5qF0AEwpbnJz09p1jmjpavH4SFyRkbgi0xXi+l3ur4BfxfSaNlsPEhJM7N+/mw0b1rB48QIA6utrGTiwNw88cD/PPfc4NTU1/OhHP/J/ThkZyZ3+mUUzQeQAA4EfAoOB/wBGrbWvflYNZOBJHpUBx4Uq95W1qry8/XMpZVg8H+zeokKyyWv3eTpbrKcXDpfEFRmJKzLdOa7y8nrs9gb69OnP5Mnncu6551NeXsYnn3zE1q27WLlyDQ888Bg2m42f/vQiTj99KgaDgYqK2nbH1lJiiWaCKAW2aa3tgFZKWfE0M/mkARVAlfd1a+W+sqjpmZIJQIWtso09hRAienyzudbV1bFw4Zf85z8fUFdXy7XX3kB2djZlZaVcc83lJCencNllV2A2mxkzZgyzZr1Anz79GDRocKfFEs0EsQT4P6XU00AfoAfwtVJqstb6G+ACYCGwEnhEKZUEWICReDqwlwIXerdfACyOYqxkJ2cByNKjQoi4CjWba6Bbb72rWdlll13GD35wUafHErUE4R2JdCaeG7wRmAbsAmYrpRKBrcB7WmunUmomngRgBO7WWluVUi8DbyqllgB24PJoxQqQneJJEBVWqUEIIQREeZir1vq2EMVnhdhvNjC7SVkd8PMohdZMZlI6BgzSxCSEEF7yoJyXyWgiw5IuCUIIIbwkQQTIsmRQYavC5V0PVgghujNJEAEyLRk43U6ZtE8IIZAEESQzyfOoRYWMZBJCCEkQgbIsnmchyqUfQgghJEEEyrR4axAy1FUIISRBBMryNzFJghBCCEkQAXw1CHmaWgghJEEEyUj0PCxXLk1MQgghCSKQyWgiPTFNmpiEEAJJEM1kJmVQaauUh+WEEN2eJIgmsiwZONxOahrkYTkhRPcmCaKJ3OQcAA7XFsU5EiGEiC9JEE30SskFoKS+NM6RCCFEfEmCaCIryfM0dZlVhroKIbo3SRBN9EzyLBxUZi2PcyRCCBFfkiCaaKxBSIIQQnRvkiCaSDCaSU9MkyYmIUS3JwkihJ5JWVTIsxBCiG5OEkQI2UlZON1OKm1V8Q5FCCHiRhJECNnJPQEZ6iqE6N4kQYSQ408QZXGORAgh4scczZMrpdYCvpnvdgGPAHMAN7AJmKa1dimlrgd+BziAh7XWnyqlkoG5QB5QDVyttS6OZrw+ucnZAJRYJUEIIbqvqNUglFJJAFrryd5/1wBPA/dorScBBuASpVRvYDowETgPeFQpZQFuBDZ6930LuCdasTaVneRNENLEJIToxqJZgxgDpCilvvBe5y5gPPCtd/t84FzACSzVWtsAm1IqHxgNnAHMCNj33ijGGiQrKQOTwSRNTEKIbi2aCaIOeBJ4DRiO5yZv0Fq7vdurgQwgncZmqJbKfWWtyspKwWw2tTvg3Nw0/+u8HtmU2cqCyuKlK8QQisQVGYkrMhJXZKIRVzQTxHYg35sQtiulSvHUIHzSgAqgyvu6tXJfWavKy+vaHWxubhrFxdX+95mJmRyqKWLfoWKSzEntPm9HNY2rq5C4IiNxRUbiikxH42opuURzFNO1wFMASqm+eGoEXyilJnu3XwAsBlYCk5RSSUqpDGAkng7spcCFTfaNmRxvR3WpTLkhhOimolmDeB2Yo5RagmfU0rVACTBbKZUIbAXe01o7lVIz8SQAI3C31tqqlHoZeNN7vB24PIqxNuMbyVRUV0K/1D6xvLQQQnQJUUsQWuuWbupnhdh3NjC7SVkd8PPoRNe2vBTPwkGFdTEZWSuEEF2OPCjXAt/CQUWSIIQQ3ZQkiBZkJ/XEZDBJDUII0W1JgmiByWgiJzmbwrpi3G532wcIIcRRRhJEK3ql5FLvqKemoTbeoQghRMxJgmiFrx9CmpmEEN2RJIhW5PkTRFGcIxFCiNiTBNEK31DXd7a9H+dIhBAi9iRBtKJfau94hyCEEHEjCaIVyeZk+vTohdlolpFMQohuRxJEG3omZeFwObA6rfEORQghYkoSRBt8y48W1ZXEORIhhIgtSRBt6J/aF4D91QfjHIkQQsSWJIg2+GZy3V9zKM6RCCFEbEmCaEOfHr0xGozsr5EahBCie5EE0YZEUwK9UnI5UHMQl9sV73CEECJmJEGEoV9qH2xOO6X1srqcEKL7kAQRBn9HtTQzCSG6EUkQYeifJglCCNH9SIIIgwx1FUJ0R5IgwpCWmEpGYhoHZKirEKIbkQQRpn5pfSm3VcjiQUKIbkMSRJh8zUwHqqUWIYToHiRBACUV9VTW2Frdp7/3ieoD0lEthOgmzNE8uVIqD1gNnAM4gDmAG9gETNNau5RS1wO/825/WGv9qVIqGZgL5AHVwNVa66it+/no22sY2Dud6T8d1eI+jUNdpQYhhOgeolaDUEolAK8A9d6ip4F7tNaTAANwiVKqNzAdmAicBzyqlLIANwIbvfu+BdwTrTgBGhwuSqvqW90nNyWHRGOCDHUVQnQb0WxiehKYBfjuqOOBb72v5wNnAycDS7XWNq11JZAPjAbOAD5rsm/UWBJMWO3OVvcxGoz0Te3DodpCGlyOaIYjhBBdQlSamJRSvwGKtdafK6Xu9BYbtNa+ZdmqgQwgHagMODRUua+sTVlZKZjNpojj7ZGSQHmVldzctFb3G547kN1Ve6kzVzIse1DE12mvtuKKF4krMhJXZCSuyEQjrmj1QVwLuJVSZwMn4mkmygvYngZUAFXe162V+8raVF5e165gTQYD9TYnxcXVre7XJ9HTD7F6zxYyXNntulakcnPT2owrHiSuyEhckZG4ItPRuFpKLlFpYtJan6m1PktrPRlYB1wFzFdKTfbucgGwGFgJTFJKJSmlMoCReDqwlwIXNtk3apISTTicLhzO1mdrHZo5CICdFbujGY4QQnQJsRzmegvwoFJqGZAIvKe1PgzMxJMAFgB3a62twMvA8UqpJcANwIPRDMyS4GmWsje03g+RndST9MQ0dlbuxu12t7qvEEIc6aI6zBXAW4vwOSvE9tnA7CZldcDPoxtZoySLJ0HU2RykJCW0uJ/BYGBIxiDWFW+k1FruX69aCCGORvKgHJCa7EkKtfVtj04amjEQgJ2Vu6MZkhBCxJ0kCCDNmyCq6+xt7js0czAABRW7ohqTEELEmyQIIC0lEYDq+oY29+2f2pdEYwKbSrdFOywhhIgrSRA0NjHV1LWdIExGE4mmRCpslawp2hDt0IQQIm4kQQBpKd4mpvq2m5gAzEZP3/7rm+ZGLSYhhIg3SRBAqreJKZwaBMCfx90IQKYlrAe8hRDiiCQJgsAaRHgJIju5J2mJqVTYKvlyzzdYHVYcMj+TEOIoIwkC6JFkxmCA6jBrEAAD0/oD8FHB/7hl0X08uPyJaIUnhBBxEdaDckqpk/HMsPoC8CkwFrhSa/1ZqwceIUxGI6nJCWENc/UZma2CRjKVWcujEZoQQsRNuDWImXjmSPoZUAeMAx6KVlDxkJmWRFVt+AniuJ4johiNEELEX7gJwqi1/gK4CHhfa72PGEzTEUs90y3UWh00OFqfj8knNzmnWVldQ/tmkxVCiK4o3ARRp5S6BZgKfKqUmo5nnYajRlZ6EgCVNeHVIgwGA3+beG9Q2b5qWW1OCHH0CDdB/BroAfxUa10O9AMuj1pUcZDtTRAVETQzZViC51C3u8I/VgghurpwE0Qx8JHW+jul1OXe46zRCyv2fDWIimpbRMcN887NBGB3hj8KSgghurpwE8Rc4Nfe0UwP4lnxbU60goqHnmneJqYIahAA151wBT0SUgCoc0gfhBDi6BFughistb4d+Cnwmtb6IaBX9MKKvZ4Z3hpETWQ1iPTENG4Z9wcA1hVt6vS4hBAiXsJNEGalVA5wKfBfpVRvIDl6YcVeVroFiDxBAPTqkceAtH5sK99Bpa2qs0MTQoi4CDdBPAGsAP6rtd4ELOIoew7C18RUEeYopqaO66kA+Hb/d50WkxBCxFNYCUJr/Q4wEnhdKXUicJzW+t2oRhZjSRYzKRYz5RF2Uvuc0mc8ANvLCzozLCGEiJuwEoRSagKwHXgT+DuwVyl1SjQDi4ecjCRKK6243e6Ij81LyWVE5lB2Ve2htL4sCtEJIURshdvE9BzwS631eK31WOAnwPPRCys+sjOSsDU4qQlzVtemTuo9FoBVhes6MywhhIiLcBNEqtZ6he+N1no5kBSdkOIn2zuSqbSqfY94nJg7CrPBxFd7v5Xpv4UQR7xwE0SZUuoS3xul1KVAaXRCip8c78NyJRXtSxApCcmMzFbUOeqls1oIccQLd8K9G4C5SqnXve93Ale0doBSygTMBhTgBK4BDHgesHPjmR12mtbapZS6Hvgd4AAe1lp/qpRKxvOAXh6eeZ+u1loXR/CzRcxXgyipbP9D4j8acj4bS7awvngTPxhwZmeFJoQQMddqDUIptVAptQB4Bc8037uAPUAtMKuNc18MoLWeCNwHPO39d4/WehKeZHGJ95mK6cBE4DzgUaWUBbgR2Ojd9y3gnnb9hBHIyfA82tHeJiaAvqm9GZY5mILK3VTZj6r5DIUQ3UxbNYgH2ntirfVHSqlPvW8HAoV4pgv/1ls2HzgXT+1iqdbaBtiUUvnAaDwLFM0I2Dd46tQQsrJSMJtN7Q2ZEUM8U3hX1zvIzU1rY++WnTxgDPkVu3h18xx+ccLFjO1zPEZD+xfv60gs0SRxRUbiiozEFZloxNVqgtBaf9va9rZorR1KqTfxPIH9M+CHWmvfGNJqIANIByoDDgtV7itrVXl5++dCys1Nw1prxZJo4mBxDcXF7f/2PyZ9DO/wEbvK9/H44pe4cNDZXDTk3HbH1ZFYokXiiozEFRmJKzIdjaul5BL1Nam11lcDI/D0RwROz5EGVOCZ+C+tjXJfWVQZDAZy0pMorarv0HlSE3uQkZjuf7+ycG1HQxNCiJiLWoJQSl2plLrT+7YOcAGrlFKTvWUXAIuBlcAkpVSSUioDzxPbm4ClwIVN9o267Iwk6m1O6qwdm7r7quN+6X9dUl/K3K3/7mhoQggRU9GsQXwAjFVKLQI+B24GpgEPKqWWAYnAe1rrw3jWvF4MLADu1lpbgZeB45VSS/CMonowirH65Xo7qosqOlaLUFnDuHTYRf73yw5936HzCSFErEVtXWmtdS3wixCbzgqx72w8TVCBZXXAz6MTXctys7wJoryeQb3T29i7ZQaDgR8ccyYf5v+3s0ITQoiYinofxJEmL9OTIIo7WIMAT5KwmBL979szx5MQQsSLJIgmcvzTbbRvVtembhk/zf/6q70dGhQmhBAxJQmiiZ7e6TbKOvCwXKB+qX04f9APAPio4H9U22vCOk5qG0KIeJME0URKkplki4nSDky30dQ5Ayb7X9/33aNt7r++eDM3Lbyd/IpdnRaDEEJEShJECNnpyZS0c12IUJLMFv9ru6uB59a8QkHF7hb3/3Tn5wAs2BeTkb1CCBGSJIgQ8rKSsTU4qarr2LMQgQak9fe/3l5RwNNrXmpxX9+0HC63s9OuL4QQkZIEEYJ/JFN5x0cy+dx+0vSw9zUaDAC4pB9CCBFHkiBC8D0LUdiBuZ1C+fO4PwS9X35oVcj9jAbPhIMut6tTry+EEJGQBBFCnjdBHC7r3AQxNHMQp/ae4H//j63/wu5s3ozVWIOQBCGEiB9JECEM7OWZI3BvYXhDUiNx5XG/4LoTGtdaqm2obbZPYx+EJAghRPxIggghNTmB1OSEDs/H1JJxeaP9r59ZM6tZLcLo/bU4JUEIIeJIEkQLemUlU1JRj9MVnZv0IxPvBqDUWsacze8EbfPVINySIIQQcSQJogV5WSk4Xe5Om3KjqUxLBpcMuQCA9SWbqbA1rplkNHp+LXWO6NRghBAiHJIgWtDLN6trJ3dUBzp30BT/62dWv+x/bfLWIArriqN2bSGEaIskiBbk9fQNdY3ut/g/jbsRgBJrGR/l/w+ABqfDv72gbE9Ury+EEC2RBNGCXlkpABRGsQYBMCxzMKf0Hg/Al3u/YVflHjIsjetQbC3eEdXrCyFESyRBtKB3T0+COFDSfBhqZ7tiZOO6SN/u/47EgDUkdMlOtpVJkhBCxJ4kiBYkW8zkZiaxv7jzn4VoymgwcvfJfwZgbdEGGgKGva7Yv5bn181u6VAhhIgaSRCt6J+bSnVdA5W19qhfq29qb3KSs3G4nZRYS5tt31u9H7fbzeIDyyipL4t6PEIIIQmiFX1zegDwxfd7Y3K9k3uNBQi5DsTOyj3sqNjJPP0hM76fGZN4hBDdmySIVjicngfV5i+PUYLoPR4DBv/7Hw4+z/96b9V+rA7PIka1juh2nAshBEiCaNX5pwyM6fVyU7I5b2DjsxHnDpzMsxfcT4LRzNay7f4nrIUQIhbM0TipUioBeAMYBFiAh4EtwBzADWwCpmmtXUqp64HfAQ7gYa31p0qpZGAukAdUA1drrWP+1FhGj0RyMpJocMZuyovzB/2A3VX7GJDeH5PRRO/03ozLG8OKw6t5ecPfYxaHEEJE6yvpFUCp1noScAHwAvA0cI+3zABcopTqDUwHJgLnAY8qpSzAjcBG775vAfdEKc425WUlU1ljp87aeavLtSbBlMAfx17PJUMv8JedG1CrEEKIWIlWgvg3cG/AewcwHvjW+34+cDZwMrBUa23TWlcC+cBo4Azgsyb7xsWAKE79Ha7ePfL4/ejfBJUFzt0khBDREJUmJq11DYBSKg14D08N4EmttW8NzWogA0gHAu90ocp9ZW3KykrBbDa1O+7c3LRmZaOG5/LZir2U1jaE3B4LublpTM09hV7ZWTy48BkA7l76CL844WJ+dvyFcYnJF1dXJHFFRuKKTHeKKyoJAkApdQzwIfCS1vodpdSMgM1pQAVQ5X3dWrmvrE3lHVgiNDc3jeLi6mblmcmej2hLQQkTj8tr9/nbKzCuPEMfrjn+cv7unR78X5s+QfVQ9ErJxWAwtHaaqMbVlUhckZG4InO0xtVScolKE5NSqhfwBXC71voNb/FapdRk7+sLgMXASmCSUipJKZUBjMTTgb0UuLDJvnHRq2cKlgQTe4u6xh/F+LwxQe8fWvEkNy28nRfXvU61PX7NYEKIo0+0+iDuArKAe5VS3yilvsHTzPSgUmoZkAi8p7U+DMzEkwAWAHdrra3Ay8DxSqklwA3Ag1GKs01Gg4Fj8lI5VFKHvcEZrzD8DAYDz5z1cLPyLWWaO5b81f+shBBCdFS0+iD+D/i/EJvOCrHvbGB2k7I64OdN942Xgb3SyD9Qyf7iWob0TW/7gChLNCUyPm8Mq4vWN9t2y6L7yE7qyf2n3orJGNwfs654E5/v/ppLhl7IsT2HxypcIcQRSp68CsOAXqkA7C3sGs1MAFeO/EWL20qtZawv2RxcVl/G7I1vsbf6gEz+J4QIiySIMPiGuu4+3HUSRIIpgRemPI7Z0FhLuPeUW/yvX980F5fbxcrDayipL6O2IbgD3+mKf3OZEKJri9oopqNJv9weJJiNbCgowe12x3zEUEsMBgPPTXmUSls1LreTrKRMZk5+lOnf3AnAHxfe4d/39gnTg461OW2kGFNiGq8Q4sgiNYgwmE1Gjh/Uk4oaO+XVtniH00yGJY2spEwATEYT9516a7N9tpRtD3pvdXa9n0MI0bVIggiTr3N6TxdqZmpJr5Rcbhx9TVDZJzs/C3q/rnhTLEMSQhyBJEGEyZcgduw/Mqa4OCFnJC9OncFDp98Zcvv7Oz7h+8NrqbJXN+ufEEIIkD6IsA3tl4HJaGDr3vJ4hxKRnklZLW6bs+Wf/tcvTvU86P7dwZVkWNI5PvvYqMfWWZ5b+yqJRjM3jrk23qEIcVSRGkSYLAkmhvZNZ29hdcxmdo2WUA/arSvaSEl9GW9ve4+X1r8R4qjwVNqqYj5Cant5PptKt8X0mkJ0B5IgIqAGZOF2g94X1tRQXUZeck7Q+0RTIi9OncGJuaP8ZbM3/YPXN83t0HXKrRXctfRhWbdCiKOEJIgIHDvQ01yzbc+RlSBunfBHkkxJAFx7/OX+8utHXcnTAbWJvdX7Qx7vdrvZWbkbl7tx4aTtJTuxOe1B+x2qLQRga5MRU0KII5MkiAgM65dOYoKRNduLcLvdbR/QRaQkJPPUWX/lxakzGN/rxKBtFlMiT53512bHNLgc/tf/1O/z1OqX+LhgPgA7ygu45+sneG3TP4KOCUwgQogjnySICCSYTYwemkNplY3C8vp4h9NpksxJPDHpAYZnDvGX3fzNXSzav4waey1LD64E4Ku9nvWeDtcVAbClVAedRxKEEEcXSRAROn6Qp5lpY0FpnCPpXCkJKdw87vfcc8otJJoSAXh3+4fcviR4It2NJVuwmCz+94E1KRfBtaptZTvYXbU3ilEHi2WCanA5mLbgNt7a8m7MrilErEmCiNDooZ4O3w07j64E4dOnR6+gOZ2amrVhDm9umed/bwt4IrvpDfr5dbN5YtULrDy8Jib9Eo6AZrFoq/Qu+bri8OqYXVOIWJMEEaGsNAsD8lJ9pOBAAAAgAElEQVTZvKuM4oqjp5kpUM+kLF6cOoNbJ9xEojEBlTWM+075S8h9Z6x6AavDyvxdX/HZ7q/95YFDXd/cMo8X1r3mTyDby/M5XFvU7vicLmfIPqCGGCYIq0OmKhFHP0kQ7ZCZ5mliuX3WsjhHEl2D0gfwzORHmD72Bnr1yOP5KY/xyxGXBu1TWFfELYvu49NdX3Cg5pC/vM7RPHnurzmIy+3iubWv8tCKJ3G6nDy9+mWWHlwBQE1DLXUNrSddu9POnUsfYp7+oNm2WNYgGlxH9rMwQoRDEkQ7/HLqsHiHEBdGg5Ez+5/Gi1Nn8K9fvsxl6tIW961tqG1W9vj3M9lZucf//kDNIQoqd/HOtvcBuH3xg9y6+P5WYyiuL6W2oY4l3qQSKJYJwhnnDnm7s4E9VfviGoM4+kmCaIc+2T0Y1j8Do8FAnTV2N6WuZlK/03h+ymOcM2Ay4/JGB217aMVTIY95Zs3L/tdvb3vP/3rm2lf9r1trKmr67EVgU1N7m5hq7G3XXJoKp0N8/q6veW/Hf9oVU1ve2Pw2M1Y9z47ygqicXwiQBNFuJwzuicvt5qZnF/HeNwV8tap7fpszGoz8eNiFXHfCFTw/5TGmHHNG2Mfurznof63L8/2vdwXUMpra3GRKDXfAyKk6R/smHbx9Sds1FwCrw8rfN7/DvuqDQQmivoV1wD/d9TkL9y1pV0yBXG4Xb299L+hn31iyBYB91Qc6fH4hWiIJop3Gj8j1v/7f8j2889UOGhzde5U2o8HIz4b/iBenzuAPY65jcv+JZFoyIj7P2qKNzN36b97Y9HazbYEd4YdrC4NqEFX2mvYFHqZFB5axqnAdz699NaiJ6b7vHo3qdQ/UHOa7QytDzpEV76YucXST2VzbqV9uarOy/ANVjBzY8uyp3cnx2YrjsxU/HX4xBgyU2yq4N8wb6aID3/lfjyseg8oaRrLZM1XI1GMmsWDfYgB2Ve0jO6mnf99qezVVthre2PQ2k/qdxvCsIXSmBqenY7rWUYfL3fhlIFSHfGdy03ISONoeTjxYc9j7pSIt6teyO+1YnTbSE6N/rSOV1CA64Mk/nE7P9MaHxg6WNO+Y7e6MBiMGg4GeSVmM9U4OOKnfafx0+MUAHJetSDYnMSJzKH8ZP42z+p8edPzsjW/xl0X38cK613hj09vsqNjp33aw5lBQE9M8/SG//ehWVhet59m1s3gnoI9jffFmbl10P1X24AWfAmsgJfVl/rJXN77FtAW38U9vB3pTTb+5rzy8hmfWvIw1RHPT7I1vdWhqFkMr/01dnTDly8aSLdyz9G9U2OK71km1vYZHVj7N498/F5PrPbvmFe5c8hD1UU7wLVlduJ6HVjwV0efetA8u2qQG0QE905P4+eRhvPKfzQDkH6jkB+P7xzmqruu3o64Mej/1mEnN9hmQ1p8KWxUbijcH3fxDPWi3pmgDJ/Ua2+L1lh5cydKDK/ntCVf65426c8lDvDDlcf+64o6AmsD9yx7jxakzqG2oY713xb0lB1dwRr/TyLSkBz0n3nRKc9/Dg8sPr2Zy/4lB29YVb2JPxX56kNlirK1pbQX0wJpMIKvDRqm1jH6pffxlJfWlVNqq6dOjF0lmC0aDJ/HM2jAHgCUHlvPDIee1eK3Hv5/J3ur9PHnmgySbk1vcr8ZeS37FTk7MG+WN0UW5tZLs5NZr15W2Kk+c1rIW9ympL6PCVsmwzMGtnisce6r3+a/b2s8TyNpg7bR16d/Z9j5Wp5UF+xZzyZALMBlNre6/vbyAmWtf5dJhF/GDAWd2+PrhiGqCUEqdAjyutZ6slBoGzAHcwCZgmtbapZS6Hvgd4AAe1lp/qpRKBuYCeUA1cLXWujiasbbXySPz+HDxTorK61mxpZDrLhqJ2SQVs/YyGU3cMOoqwHNjya/YxXNrXwm5b4WtksdXzQQ8yeaiwedywLGXupoG/00PaDap4LztH7LkwHKAoCnPAT7d+TlDMgYFlT32/bPNrv2/XV+GjOnf2z9mUt9Tm91AbvvibwxMP4bbJvwxqHxP1T721xxkYt9T2F99kBfWvcaNY65hYPox/n0Cm5FcbpenVoYBN27+t/srhmcNYUTWMGxOOyaDEbPRzMcF81l04DsuUz9hUr9TAbh/2eP+85w7cAqXDL2A3eWNM/jWNtS1ePOrtFX5Z/vNr9jFqJzj/NtsTjtf7/2WqcdMIsmcxIxVz1NqLeO6E65gXN5oFuxbzIf5/+X6UVcxJud4CuuK6N2jV7Nr1AQMjXY4g0ekVdgqsZgs3L/sMQCeOvMhkswW/3Hzd33FRYPPJSUh+Ea/q3IPDpeD4VlDm13PZ/amua3OHuBTbq1g2gd/A+CFKY9TULkbp8uJ6tm+Ye8WUyJWp5Wv9y7i672LeH7KY/6kHcrOyt24cfNB/qdHfoJQSt0GXAn4futPA/dorb9RSs0CLlFKLQOmAxOAJGCJUupL4EZgo9b6AaXUZcA9wP9FK9aOMBgM/O36U/ntjIUAbCgoZVxAB7ZoP6PByIisof7V7g7UHOJvK5/hkqEXsKdqP+uKN/r3Pa3PSSSZLZzaZxzFxdU8fPpdLDqwjC/2LGx2Xl9yAILOATA/oBO8Nb4JC0/pPb7ZdBvTvwm9zOueqn1MW3AbRoORUTnHccnQC5ix6nkA/7MgADNWPc8Tkx4gJSEFIKhZrKTeM8VLYO3qubWvcmLuKP/PctOY3/r7cebpD8hOyiLDkh4Uyxd7FjL1mEncseARf9miA8tYdGAZFw46mzP6nUpqQg8cbicWUyLF9Y1Ty8zaMIcnz/yrv1/ow/z/svjAMraXF3BW/4mUemsAr2+ay7ipM1h5eA3gaWo7tc8Elh9aRQ9zCjPOfIB6h5WtZdsZnXMc1QGDDHZX7CeDbMAzMOGTnZ8HxT9Pf8g5A8+iX2of5ukPWVu0gRWHV/PncX9Al+eTl5JD75RePLn6RQAm95+I3WlnVeE6/jTuxqA5wg7XFlJjryXJbMFsNPs/Z6PBSM+kLOzOBkwGI094f1cANy283f/6+SmPUddQjy7fQYIxgRNyRvpv9L6f/eTe4wBPzdPmtJOSkEylvSroZ7I57Xy15xtyUnI4rc+EoG37qg8EfQZvbpnHr9RPaHA5SAmz9tMehmhNW62U+imwAfiH1vpUpdQBoL/W2q2UugQ4F/gcuFBr/XvvMR8CfwPuBGZorZcrpTKA77TWx7d1TYfD6TabW6+mRcu2PWXcOnMxQ/pl8OyfzuqUKqhondVhw+FyYHPYyU5pufnC9w3c7mzgvq+fZHdF83Uv/u+0a3luWfAooZtO+Q0TB0xg+v/up7i2+dxbWUkZvHzx37js39M6+JOENv3UawADM5e3f4W/aOrVI4cEUwL7qw61uM8LFz3E62vmsfbQ5ojOnWBKIDs5k/L6ylbb3ef+bCZXvDc9onPHw3G5w9lSvCNq53/6gvvon96n7R1bFvKGFbUEAaCUGgTM8yaIg1rrvt7yqcC1wGfAKK317d7yt4C3gDuAP2qttyqljMBerXWbjfvFxdXt/mFyc9MoLq5ue8dWPPfv9awvKGXiqN5cd9FxbR8Qo7ii4WiNy+lysrpoPRmJ6QzPGuL/Jlhlr+bOJQ9hMpi4/aTpLD6wnJ8Mu4hEUyKvb5rLmqIN9EzKoszafM3yEVnD2F6ez/i8MawuWo/RYDzqRh+1xmJKjHnnajT5mve6mqfPehiLdybmSOXmpoVMELHspA78H5EGVABVBI9nC1XuK+vyLj1zCOsLSlm68TBD+qQzZZx0WB9pTEaTvzkgUHpiGjMmPYAbN6kJPYKmGblq5C85rc9JjOw5wtMha6sgLTGNgzWH6JWSR0pCsj9xXcuvAc9aGi+ufx2AJFMSfx5/I4dri3C4HBTVlwQ97xHKtDHXcWzP4eyu2ofL7WJY5mCq7TXUO6wsP7SKycdMJDWhB7o8ny92L2R7RQED0vrTu0ceKw+voX9qX/bXHOS43OH8bMgl/HXFk2QkpvPIxLsxGAy8teVdVhxezaD0AUHNMSfmjmJX5W4qm4wGs5gSOSF7JKuL1pNgTOCa4y/n1Y1vAo0jb0bnHM/28nx6JmXxS3Upz66ZxUWDz8VsNFFuq2BCr7F8f3gNp/U9ice/nxl0/ifP/CsFFbv4x9Z/BfVVhJKXnEOto45zB07hw/z/AnBGv1M5Z8Bkfx+Gz10n/4mCit28u/3DZuc5vc/JfHdoZVDZLRNvINfQmze3zKO4vpTc5GxG5xzPV3u/4UdDzmdEz2E8v3Y2B2sPtxhfojGB34++hjJrOQv2LW5132bHmhKxxzDZxrIG8QnwVEAfxELgW+BL4CTAAqwATgSmAWkBfRBnaa1vbOt68a5BADw1by2bd3u+Rb7yl8kkmDvWYX20flOPliMpLrfbjcPtJMHY/Huab9oQIwbKbZWkmJMxGoz8U7/PuLzRjMk9IWpxhcvqsGEyGEkwJYTcfqDmEPP0B+ys3MNJvcZy9XGXhd30akus4buCdWRY0hmbOyrouAZnA05vZ/3j3z/n7w8yGoxccezPOaXP+KBzBXa825123EBdQx2Zlgx/eV1DHQnGBBxuh39Ek8PlIL9iF8Mzh2AwGGhwOejfOzvsz6veYeVAzSH/9DJPnulZW8WA0d/BXm6tYE/1fvKSc+ib2tsfb6W9ijc3z+PaE35NakIPdlTsZFD6AOocdby5eR7nDpzCyOwRbCjezMaSLZx4zEiOT23/30RLNYhYJogRwGwgEdgKXK+1dnpHMd2A55mMv2mt31dKpQBvAn0AO3C51rrNNNsVEoTb7ea6xz0do3mZyfTN6cHFEwcxuE96G0dGN67OJnFFRuKKTCRxudwu3G53m8NEO8PR8Hm1cHzsE0SsdYUEAVBZY+NPLywNKvvDj09gwrF5YZ+j3uYg2WI+av8go0XiiozEFZmjNa6WEoQM2I+CjFQLV52ngspe+mgTTld4HZP5ByqZ9swiPl6yKxrhCSFEWCRBRMnksf149IZTg8r+s2R3WMduKCjx7L9UEoQQIn4kQURRr54pnHtS4xOxn3y3G7fbzcdLdrF9X8sDs4zejrOjqPVPCHEEkgQRZT+fMpTrLhrpf3/d4wv5eMkuHnt7TYvHGI3ykJ0QIv4kQUSZyWhk4qg+DO3bfBTTyq2FVNTYsNmdTY6RBCGEiD9JEDFy91UTmHZp8DjlWR9v5s8vLOXJeWuDyk3Gxl9Lvc3BzPc2tNokJYQQ0SAJIobGqzyevmkiPzkzeCGbgoNVvBwwyimwAvH+wh2syy9ptUlKCCGiQRJEjGWmWvjh6YN48U/B0/V+v62I62d8w9Y95UF9EO9+2bgOgtvt5tVPNrNgTfPJ5sJRWWvn8bfX+EdJCSFEayRBxEmyxcwrf5nMlU2el3jin2uptTpCHmO1O1m+uZC5XwQvnrN5VxlVtW3Pz/KvBfnofRV8sGhnm/u2V2WNjXpb6PiFEEcWSRBxlGA2MmVsP167bQrHDmhcbaylB+Te/Gyb/3WDw0VFjY1te8p56t113Pz8kjavt2yzZ7aS/UXRWxr1Ty8s5ZYXPU+Ru9xuHM7uM2upEEcbWXK0CzAaDdx2+Tg27Szl6X+tb3G/lVuL/K+/Wr2Pfy8saLaP0+WitNJKXpZnsZk3/ruV/nmpnDOhcWbZ1qYqrrc5cLrcpCaHnoCtNb5kYPWOynrkrVXsOlTNG3dMjfhcsbR1TznFFfWcOaZvp57X7XbjdLllhUFxxJIE0YUcP7gnZ0/oz1er9pPeI5H7f3MSmE2s2nSIf34dvNhIqOQw4501GAwGtu4p59ZfjWVYv3SWbPQs5jJ1XL/GHQPyQ1WdHbvdSU6mZwbLv7y0lHqbs103dWuT4bq7DnnmhmltDd+5X2iyM5K44JSBYV9n0fqD9MpKRg1ofY3jcD3xT88osjNG9/E/pNgZXv1kCyu2FPLyLWd1+FxbdpfRIymBgb3T2t75CFZrbZCE2oVIguhCDAYDl589gsvPHuEvy81NIyvZzEkj8/hq1X52Hqxk297QQ14Dy5/451ruvKJxXYM58xubpwLrDzfP9DRNvX77FKrqGqi3eW7yDqfL/x+1pVpF0xu/1R6678HldmMK2K/O2oAb6JGUwII1BwA4/+QBfLlqP+NG5JCT0fISig0Ol/9n6eyaicPhIjEhvBlBXW43T81bx5ih2Zx78oCQ+6zYUghAWZWV/h2snDw5bx3Q+T9zJNxuN6VV1lZ/Px31x2cXk2g28v7jF0ftGiJ8kqqPEJmpFn42eSi3/mosf/zpKKb/bHSbxzw6t3Fo7HebgmdLv/axBSwLKLM7XLzw/gb/+5r6Bv/rac8sYvpzi/1lDQ4XF9/yMdc9vtCfFFwud7MahM/uw9Xk768EPInnpmcX88dnF+MKmEtk5dYi5n29g9teXtbqz7Rtb/MV2/YX1bBxZ/MlQSPVEEF/SVWtna17ypm3IJ/PV+5tdV+Xq/PnTCmrsvL6p1sor7Z1+FxF5XU8+PfvOVBc0+p+Hy/ZxW0vL2PtjuIOX7M1dkds+q0Ky+r4aPHOqPx+wuF0ufj0u92d8juMFkkQRxiDwcDY4bmcOCyH12+fwvUXH8cvpgzjtdun8NrtU7i0yTMWrZn96Rb/66feXUfBwcZF1KvrPMmgLmBE1SdLdwOeG4rP4bI6dh+u4rczFjJ/eegb5SNvreZvc1dz56vL2VvYeBPaurvxZn+4rC7omKKKegrLg8sAVnq/lUNjn8d9b6zkmX+tb7VD3O12s3lXWav72BuCt9VZG6iuCz06zOlsvKm8uyC/xXNCZIknFFeISbne+lyzdNNh3l0Q/jrHDqcrKPEHnmtPYTX3vr6Sfy9s+WdZtP4gAGu3tz5MetehKv7y0lIOlUY2GCLWSw/M+Oda/rN0N4s3HGzX8W63u0MxL1p/iA8W7eSpd9eFfUxJRX1ME5okiCOYwWDgtON7c/4pAzAaDBgNBi4+fRBv3DGVZ26aCMClkwZzzoTGCQMDXwfyfcP3uf+NlVz72AJuffk7f9mXq/bx3jcFvB3wbMahkjr+OmcV0DhKCjy1jKYKy+r8zS5A0H+MpiO37pi1jDtfWd7s+LSUxjV3K5p88/Iltf3FNThdLiprbP44HnpzFU+9u45ZH28O9eMDsD7g+ZCXPtzITc8u5v9mhh4dVt+kOa21G0VVrZ2Pvi3g7SbDk8FTU1uti0Ic1cgZkGB8Cc43rLkugiHFc+ZvY/pziymuqA8q3xKQqOevaLk25Fsdsa2Raa//dytlVTY++Day4dTOdt74nC4XT/9rnT+Bhcv391NSaW3XdWd9vJnrHl8Y8m89HLXeZH2wJLxEeqC4httmLeON/20NmeijQfogjlIZqRZ/e7Xb7cbucLJuRwnnnNSfX509HIBV24r4dt0BMBjYvKsMgNsvH8vj7zRO/dH0mYb/Ld8T9D6wFhLopQ83hiz/ctW+NmMvDKhNzP1C8+tzRmAwGLjz1SYJo7ye7Iwk//uqWjtLNxzksTe/Z+TALLbuKefEYTlM/9lodh/2dJiv2e5pHtm2p5yUJDMDejV2+vpqEG63m1W6sRmlweHy3xxLK60kmI2s3R7czPLF9/sY0CuNfy3I5w+XnkBuZmM7/bP/bmy6u/yc4UH9Nk/NW8eewmr+/MsxnDA4O+Tn0eBovHHW1DeQmWrxf4sM7NuprLWTaDaSbDGzfMthXv3PFh689mSOyUsFGpsZdx2q8scXybfR4gqr//NojcO7vTrETcze4OTWl79DHZPJHy4d1eTnbDxvJN/Mi8rr2bSzjE07y5qNRFuzvZhte8r51dnDmw2U6JGcQE19Q4vPHbXl+22exF5dZ6dnelIbezcXaTrcX+xJJN9tOsx3mw5z449P4KQIFiFrD0kQ3YDBYODq84/l6vODyyccmxdylbu7rhjP3+auDiqbdukoXmzhph/K+oL29wkEJoIFaw74O7KbeurddfTN6eF//+Cc7/2vt+7xfCtel1/CzPc2BB1XXFHPDO/IpVf+MtlfPu/rHQzslYqjyU1z485Svl69nxSLmdXexGBJDO7MDmxmun3WsuBRYwGq6hrI6NFYC9pT6ElcT7+7nmenn8FDc1bx8ylDOXlkL/8+gd/Y8/dXMuHYPH+zk6+93u1288DfV+J2uXl2+iR/beXLVfu49sLG2YQBlm06zLEDskjvkUidNfJvoq01mRWW1VHkraGEmj+suNJKdV0Dq3QxVXV20gNqhIE/Z9MJLH1KKuqxNTjpl5vqL2utFvXCB56/2ZNG5jG8f2bQtozURGrqG5rVqFpib3CyZlsRdbXWoBF0NfUNvP9tAaWVVu64YnwrZwjmiLDm0XSA3bJNhyVBiNgb1j8j5GgZX1mDw4UlJZGqynq+Xr2fBoeLCcfmkmA2kWg2snJrIet2lFBT30BFrR2b3clfLjuRFVsKWbzhECkWM6OHZbN8cyHJFhO/v+QEVm0rYkCvNFbrohZHaYUSTvV8XX5wm/ntsxo7wn/35DdB2wJrTz6+m0yglm5gPi0ltT8FPNDYdDinb0TZrI838/GSXTQ4XKRYzOwtauy3eemjTUHHbN1TzrRnFjFpdB8qazzNTh8u2un/VrxkwyGSE82MOCbDf8z6glJufn4Jd/x6XMg5vtbll9AvpwepyQk4XW4sCaagmuSGglJmfbyJSaP7Um9zMGpoNoVldRyTl9qslldebSMxwci/FxZwxug+lFQ23owrqm2kJidgwPNtuqi8cVuop/EdThe3eX93JwzuyVXnK3IyknnkrdXN9m3q0blreOOOqewrqiHBbKSwrI4D3m/kxeVtJwiXy83vn/rW//6KcxtHGlbXN7Bss6fp9M8vLOG3PzyO4wb1BODWl76jtMrK8zdPokdS8ChA32dhNoU3tDqw38snf38lg/pEb+izrEntdbSuNRst0YzLandgtTtxOt0kmI0UHKhkX3ENfbN7sD6/hHEql8xUCw+96en7GHFMJpdOGsw7X+3A2uDkx2cMZn1+CYkJJvYV1vi/pZ9/ygC+WrUPR4j/aFPG9mPh2uCb+k0/GcWc+dtabO81Ggw8/ceJLN90mHltdFSLI0ui2Yjd4SI1OYEEs7FdI41SLOag2k1qcgLjRuQwvH8mTpc7aOh5UwlmI+efPID1BSVBAzta8sT0SWSnRP5wq09La1JLgvDqjjfijjiS4/K1dSeYjdRZG0i2mIPap10uNwYD/jKXy+2fQNH3/6Xe5sDW4CIrzeI/zt7g9PdVAEHnzMlJpbi4ml2Hqqmpt7NtTwUllfUM65fBsQOzsCSaWLu9hOMGZVFwsIp/fK79x+ZlJjN6aDYXTxzE7E+2sGlXGckWMz+fMpTDpXXsK6rxN6kFmjiqN0s3Ng4cmDy2H9+sbV6z+dMvxrBqWxGLNxwKKjcaDM1GUJ05pg+L1gfv19TJI/M4dmAWb32mW92vK0i2mPzP/hzpXv7zWc2aPsMlCaINR/INLx4krsgcSXH57gmBCc7XP9C0Wcxqd2BvcHn7MxwkW0z+42wNTswmA5U1dnokJVDkHaLZL7eH/zx11gZPTa+ohpr6Bo4dkElZtY11BWUkGCEr1YLV7qSy1s7hsjrGq1wG9krDZDKw+1A1W/eU0yc7hZNH9mJvYTWbd5exY18Fxw7MIiUpAbPRwPBjMlm7o5j1+aWMH5HLxl2lDOyVxgWnDCAlKYG9hZ7zrNpW5B/qnZuZRE29g3qbg15ZyQzum85xA3vyo8nD+Ob7vfRMt9AjKQG7w8l/v9tDakoCl04azIotRbz3bQFVtXaSLSYmntCHk0bm8fqnWymvsTFlbD+cTjdOlwuny80px/XiyXnryMlIYmCvNH8fV0uuPE+xaWcp40bksq+ohvJqGzsPVuIGHvvdae1+Cv2ISxBKKSPwEjAGsAG/1Vq3Wo+XBBE7EldkJK7ISFyR6WhcLSWIrvwcxI+BJK31acAdwFNxjkcIIbqVrpwgzgA+A9BaLwcmxDccIYToXrpyE9NrwPta6/ne93uBIVrrFgc9OxxOt9ncvk4aIYToxkI2MXXl5yCqgMABvsbWkgNAeYi5e8J1tLYtRovEFRmJKzISV2Q6oQ8iZHlXbmJaClwIoJQ6FQj/MV4hhBAd1pVrEB8C5yilvsNT/bkmzvEIIUS30mUThNbaBfw+3nEIIUR31ZWbmIQQQsRRlx3FJIQQIr6kBiGEECIkSRBCCCFCkgQhhBAiJEkQQgghQpIEIYQQIiRJEEIIIUKSBCGEECKkLvskdSy0Z1GiKMSQALwBDAIswMPAfuATYId3t5e11u8qpa4Hfgc4gIe11p9GOba1QKX37S7gEWAOnjXmNwHTtNauWMallPoN8Bvv2yTgROB04vh5KaVOAR7XWk9WSg0jzM9IKZUMzAXygGrgaq1160uKtT+uE4HnASeev/WrtNaFSqmZwETv9QEuAewxjGscYf7uYvx5zQN6ezcNApZrrS+L5efVwr1hCzH8++ruNYiusCjRFUCp1noScAHwAjAOeFprPdn7712lVG9gOp4/zvOAR5VSlhbP2kFKqSSAgBiuAZ4G7vHGagAuiXVcWus5vpiA1d5rx+3zUkrdBryGJ1lBZJ/RjcBG775vAfdEMa7ngD96P7cPgNu95eOA8wI+u8oYxxXJ7y5mcWmtL/N+VpcCFcCfAuKN1ecV6t4Q07+v7p4gusKiRP8G7g147wDGAxcppRYppV5XSqUBJwNLtdY27x9lPjA6inGNAVKUUl8opRZ4Z9QdD3zr3T4fODsOcQGglJoAHK+1fpX4fl4FwE8C3kfyGfn//gL2jVZcl2mt13lfmwGrtwY9HHhVKbVUKXWtd3ss44rkdxfLuHweBJ7XWh+Kw+fV0r0hZn9f3T1BpNPYhALgVErFtNlNa12jta72/sd4D0+WXwncqrU+E9gJ3B8i1mogI4qh1QFP4vlG8nvgbcCgtQg/gnEAAAQgSURBVPbNzeK7fqzj8rkLz39eiOPnpbV+H2gIKIrkMwos79T4msaltT4EoJQ6HbgJeAbogafZ6QrgfOAPSqnRsYyLyH53sYwLpVQe8AM8TToQ48+rhXtDTP++unuCiHhRomhQSh0DLAT+obV+B/hQa73au/lDYCzNY03DU/WNlu3AXK21W2u9HSgFeoW4fqzjQimVCRyrtV7oLeoKn5ePK8Q1W4olsDwWn9svgVnARd626DrgOa11nda6GliAp+YYy7gi+d3F9PMCfga8o7V2et/H/PMKcW+I6d9Xd08QcV+USCnVC/gCuF1r/Ya3+HOl1Mne1z/A09a+EpiklEpSSmUAI/F0UkXLtXj7ZJRSffF8G/lCKTXZu/0CYHEc4gI4E/gq4H1X+Lx81kbwGfn//gL2jQql1BV4ag6TtdY7vcUjgCVKKZO3Q/QMYE0s4yKy310s4wJPk8z8gPcx/bxauDfE9O+rW49iomssSnQXkAXcq5TytTf+GXhWKWUHDgM3aK2rvCMoFuNJ7Hdrra1RjOt1YI5SagmeERPXAiXAbKVUIrAVeE9r7YxxXAAKT3OEz43AC3H+vHxu4f/bu3vXKIIwjuNfk0oQbMRGCekeTIidaGmRQsHCyiJGtPCtsxNTqWBhJRLEoJUJRLDwDQuxUKKVWokY9BHEF7CwyR+ghRYzxjOsckGzCeT76W7Yu5ud29vnZrn9TZdjFBETwGQd46/AyFJ0KCJ6gXHgE3ArIgAeZ+bpiJgGnlIur0xl5mxEvG+jX1XXn11b49Xht+MsM1+3PF5N54YTwHhbx5dx35KkRqv9EpMk6Q8sEJKkRhYISVIjC4QkqZEFQpLUyAIhrQARcSgiri13P6ROFghJUiPvg5AWISJOAfuAXuABMAHcBd4Ag8BHYDQz5yJiDyWiuYdyw9WxGrE9TLlLvaduP0IJijtMCWTrAx5m5pE2901ayBmE1KWI2EVJ09xGyQzaBOwHhoDLmTlIubv1TA16uwLszcytlNiDSzWGeZqSzT9EiXc5WN+ij1IotgC7I2KwtZ2TGqz2qA1pMYaB7ZS8IIC1lB9ZbzNzprZNAtcpGTrPM/NDbb8KjFGKyeef0duZOQbzCyE9ycy5+vgdsGFpd0f6OwuE1L1e4GJmXoD5VNnNwI2ObXool4kWzs7XUL5v3yjZVtTXWM+vxM3OJOHv9TnSsvESk9S9R8CBiFhX1w25Q1lkKqIs6Qkl8PE+8AzYERH9tf0oJbY5gY0RMVDbT1LW25BWHAuE1KXMvAfcpJz8XwEvKKt7zQFnI2KWsv7vucz8QikKt2v7TuB4TZQdBaYi4iUwAJxve1+kbvgvJukf1BnCTGb2L3NXpP/OGYQkqZEzCElSI2cQkqRGFghJUiMLhCSpkQVCktTIAiFJavQDcLcLssP6x5kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#stacked LSTMs\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(t_period))\n",
    "model.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adam(lr=0.003))\n",
    "# fit model\n",
    "es = keras.callbacks.EarlyStopping(monitor='mean_squared_error', mode='min', verbose=1, patience=5)\n",
    "\n",
    "history = model.fit(train_X, train_y, epochs=2000, batch_size=100, validation_data=(test_X, test_y), callbacks=[es], verbose=1, shuffle=False)\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 3.145\n",
      "Test Standardized RMSE: 30.827\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.median(np.abs((y_true - y_pred) / np.mean(y_true))) * 100\n",
    "\n",
    "def standardized_rmse(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    " \n",
    "yhat = model.predict(test_X, verbose=0)\n",
    "mape = mean_absolute_percentage_error(test_y, yhat)\n",
    "rmse = standardized_rmse(test_y, yhat)\n",
    "print('MAPE: %.3f' % mape)\n",
    "print('Test Standardized RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark:\n",
    "\n",
    "MAPE: 33.17\n",
    "\n",
    "RMSE: 1907.62\n",
    "\n",
    "Univariate:\n",
    "\n",
    "MAPE 45.26\n",
    "\n",
    "RMSE 1214.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2470, 7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aa=[x for x in range(len(test_y))]\n",
    "plt.plot(aa, test_y[:,-1], marker='.', label=\"actual\")\n",
    "plt.plot(aa, yhat[:,-1], 'r', label=\"prediction\")\n",
    "plt.ylabel('Unblendedcost', size=15)\n",
    "plt.xlabel('Time', size=15)\n",
    "plt.legend(fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# big picture question:Need to understand one model for per product per perspective, how to use it to predict each product/per?\n",
    "include the product & perspective as features. panel data regression - > investigate time series deep learning\n",
    "\n",
    "# categorical data aggregation: not found online, use the mode for each feature now\n",
    "use lambda function to produce a list, and then do one-hot encoding.\n",
    "can try different statistics to see which one is more important.\n",
    "\n",
    "\n",
    "# high cardinality categorical features: one-hot encoding is not ideal-too sparse, dimension reduction techniques: entity embedding, AutoEncoder...\n",
    "needs a lot of data.simple PCA.take the top # that make up the #% cost \n",
    "resourceid, clustering analysis for cat vars."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
